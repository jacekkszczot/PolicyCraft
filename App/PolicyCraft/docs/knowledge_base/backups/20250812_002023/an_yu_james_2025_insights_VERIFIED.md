# An, Yu & James (2025) - US Universities GenAI Guidelines Analysis

## Research Methodology:
- **Sample**: Top 50 US universities (Times Higher Education ranking)
- **Data Collection**: 214 documents, 235,118 words total
- **Mixed Methods**: Topic modeling (LDA), sentiment analysis (VADER), qualitative thematic analysis
- **Text Analysis**: TF-IDF analysis for term significance and frequency patterns

## Four Core Topics Identified (LDA Analysis):

### Topic 1: Integration of GenAI in Learning and Assessment (30% of tokens)
- **Key Terms**: assignment, prompt, learning, assessment, class, design
- **Focus**: Academic task integration, course design, learning enhancement
- **Applications**: GenAI in assignments, classroom settings, educational content

### Topic 2: GenAI in Visual, Interactive, and Multimodal Media (28.8% of tokens)
- **Key Terms**: image, prompt, search, google, microsoft, chatbot, code
- **Focus**: Beyond text applications, visual media, platform integration
- **Applications**: Image generation, interactive experiences, multimodal content

### Topic 3: Security and Ethical Considerations (21.3% of tokens)
- **Key Terms**: service, security, guidance, privacy, human, risk, copyright
- **Focus**: Safe deployment, ethical guidelines, risk management
- **Applications**: Data protection, intellectual property, responsible use

### Topic 4: GenAI in Academic Integrity (20% of tokens)
- **Key Terms**: assignment, syllabus, statement, writing, violation, expectation
- **Focus**: Academic honesty, policy enforcement, educational standards
- **Applications**: Assignment submission, policy compliance, academic conduct

## Sentiment Analysis Results:

### Overall Sentiment: Highly Positive Across All Institution Types
- **Private Institutions**: Mean = 0.945 (very positive)
- **Public Institutions**: Mean = 0.963 (very positive)
- **Small Institutions**: Mean = 0.975 (most optimistic - more agile adoption)
- **Medium Institutions**: Mean = 0.961
- **Large Institutions**: Mean = 0.952

### Target Audience Sentiment Differences:
- **Administrators**: Mean = 0.997 (highest - very supportive)
- **Faculty**: Mean = 0.976 (strong positive - encouraging integration)
- **Students**: Mean = 0.910 (lower - more cautious/regulatory approach)
- **Researchers**: Mean = 0.899 (lowest - most cautious due to integrity concerns)

### Technology Terminology Impact:
- **"GenAI" References**: Mean = 0.956 (more positive perception)
- **"ChatGPT" References**: Mean = 0.939 (slightly less positive)

## Stakeholder-Specific Guidelines Analysis:

### Faculty Guidelines (94% of universities - 47/50):
1. **Syllabus Statements** (56%): Template statements for different AI usage levels
2. **Course Policy Setting** (54%): Faculty autonomy in establishing AI rules
3. **Assessment Redesign** (48%): Authentic, process-oriented evaluations
4. **GenAI Education** (42%): Basic AI literacy and capability information
5. **AI Detection Tools** (38%): Discouragement due to unreliability and bias
6. **Teaching Integration** (32%): Practical incorporation strategies
7. **Ethical Use Guidelines** (24%): Data privacy and responsible use principles

### Student Guidelines (42% of universities - 21/50):
1. **Academic Integrity** (67% of student guidelines): Policy compliance and consequences
2. **Instructor Consultation** (57%): Check course-specific policies before use
3. **AI Limitations** (43%): Understanding risks, biases, and accuracy issues

### All Stakeholders Guidelines (68% of universities - 34/50):
1. **Privacy & Security** (50%): Data protection and FERPA compliance
2. **AI Limitations** (38%): Hallucinations, biases, accuracy concerns
3. **Basic AI Education** (28%): Definitions, tools, capabilities overview
4. **Academic Integrity** (26%): General policy information
5. **Disclosure Requirements** (22%): Transparency and proper citation

### Researcher Guidelines (18% of universities - 9/50):
1. **AI Limitations** (56%): Critical evaluation, fact-checking requirements
2. **Ethical Research Use** (56%): Responsible integration across research stages
3. **Staying Updated** (33%): Following evolving tools and journal policies

### Staff/Administrator Guidelines (14% of universities - 7/50):
1. **Privacy Protection** (100%): Confidential data security protocols
2. **AI Capabilities** (86%): Administrative task applications and benefits
3. **AI Limitations** (71%): Accuracy and bias awareness

## Key Implementation Findings:

### Policy Evolution:
- **Rapid Development**: Most universities moved from reactive bans to strategic integration
- **Faculty Autonomy**: Emphasis on course-specific policy development
- **Detection Tool Skepticism**: 38% explicitly discourage AI detection tools
- **Continuous Updates**: Policies noted as constantly evolving with technology

### Assessment Strategy Changes:
- **Authentic Assessment**: Real-world, practical task focus
- **Process-Oriented**: Emphasis on drafts, presentations, portfolios over final products
- **Higher-Order Thinking**: Requirements for analysis, synthesis, evaluation
- **Alternative Formats**: Oral exams, collaborative projects, in-person assessments

## PolicyCraft Application Keywords:
- **Stakeholder differentiation**: Different approaches for faculty, students, researchers, admin
- **Policy flexibility**: Course-specific and discipline-specific adaptation
- **Sentiment-informed communication**: Tailored messaging based on audience receptivity
- **Multi-modal AI applications**: Beyond text to visual and interactive content
- **Evolution readiness**: Framework for continuous policy updates
