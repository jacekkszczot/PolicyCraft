"""
Administrative Routes for PolicyCraft AI Policy Analysis Platform.

This module implements the administrative interface for the PolicyCraft application,
providing essential system administration tools and user management capabilities.

Key Functionality:
- Secure authentication for administrative access
- Dashboard with system statistics and health metrics
- Comprehensive user management (view/delete users)
- System configuration and maintenance tools
- Literature management and knowledge base updates
- Academic document processing and quality assessment- Password management for administrative accounts

Security Implementation:
- Password-protected admin interface with secure hashing
- Session-based authentication with configurable timeouts
- CSRF protection for all administrative forms
- Secure password storage using industry-standard hashing
- Audit logging of all administrative actions

Routes:
    /admin/login             - Admin authentication
    /admin/logout            - Terminate admin session
    /admin/dashboard         - System overview and statistics
    /admin/users             - User management interface
    /admin/reset-baselines   - Reset system baselines
    /admin/change-password   - Update admin credentials

Dependencies:
    - Flask for routing and request handling
    - SQLAlchemy for database operations
    - Werkzeug for security utilities
    - Custom models and database operations

Example Usage:
    # Access admin interface at /admin
    # Default credentials: admin / admin123
    # Change default password immediately after first login

Note:
    This module is part of the PolicyCraft AI Policy Analysis Platform
    and should only be accessible to authorised administrators.

Author: Jacek Robert Kszczot
Project: MSc Data Science & AI - COM7016
University: Leeds Trinity University
"""
from __future__ import annotations

import json
import os
from functools import wraps
from typing import Dict

from flask import (Blueprint, current_app, flash, redirect, render_template,
                   request, session, url_for)
from werkzeug.security import check_password_hash, generate_password_hash

from src.database.models import User, db as sqlalchemy_db
from src.database.mongo_operations import MongoOperations

admin_bp = Blueprint("admin", __name__, template_folder="../web/templates/admin", url_prefix="/admin")
from datetime import datetime
from src.literature import LiteratureEngine
# ---------------------------------------------------------------------------
# Simple password storage (hashed) â€“ for single-admin scenario
# ---------------------------------------------------------------------------
CONFIG_DIR = os.path.join(os.getcwd(), "data")
CONFIG_PATH = os.path.join(CONFIG_DIR, "admin_config.json")
DEFAULT_PASSWORD = os.getenv("ADMIN_PASSWORD", "change_me_immediately")  # Set ADMIN_PASSWORD in production

mongo_db = MongoOperations()


def _load_config() -> Dict:
    if not os.path.exists(CONFIG_PATH):
        os.makedirs(CONFIG_DIR, exist_ok=True)
        _save_config({"password_hash": generate_password_hash(DEFAULT_PASSWORD)})
    with open(CONFIG_PATH, "r") as f:
        return json.load(f)

def _save_config(cfg: Dict):
    os.makedirs(CONFIG_DIR, exist_ok=True)
    with open(CONFIG_PATH, "w") as f:
        json.dump(cfg, f, indent=2)

# ---------------------------------------------------------------------------
# Decorator for protected routes
# ---------------------------------------------------------------------------

def admin_required(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        if not session.get("is_admin"):
            return redirect(url_for("admin.login", next=request.path))
        return func(*args, **kwargs)
    return wrapper

# ---------------------------------------------------------------------------
# Auth routes
# ---------------------------------------------------------------------------

@admin_bp.route("/login", methods=["GET", "POST"])
def login():
    if request.method == "POST":
        password = request.form.get("password", "")
        cfg = _load_config()
        if check_password_hash(cfg["password_hash"], password):
            session["is_admin"] = True
            flash("Logged in as admin", "success")
            return redirect(request.args.get("next") or url_for("admin.dashboard"))
        flash("Invalid admin password", "error")
    return render_template("admin/login.html")

@admin_bp.route("/logout")
@admin_required
def logout():
    session.pop("is_admin", None)
    flash("Logged out of admin", "info")
    return redirect(url_for("index"))

# ---------------------------------------------------------------------------
# Dashboard
# ---------------------------------------------------------------------------

@admin_bp.route("/")
@admin_required
def dashboard():
    user_count = User.query.count()
    analysis_count = mongo_db.analyses.count_documents({})
    baseline_global = mongo_db.analyses.count_documents({"user_id": -1, "filename": {"$regex": r"^\\[baseline\\]", "$options": "i"}})
    return render_template("admin/dashboard.html", user_count=user_count, analysis_count=analysis_count, baseline_global=baseline_global)

# ---------------------------------------------------------------------------
# Users management
# ---------------------------------------------------------------------------

@admin_bp.route("/users")
@admin_required
def users():
    users = User.query.all()
    return render_template("admin/users.html", users=users)

@admin_bp.route("/users/delete/<int:user_id>")
@admin_required
def delete_user(user_id):
    user = User.query.get(user_id)
    if not user:
        flash("User not found", "error")
        return redirect(url_for("admin.users"))
    username = user.username
    sqlalchemy_db.session.delete(user)
    sqlalchemy_db.session.commit()
    # Purge associated data
    mongo_db.purge_user_data(user_id)
    flash(f"Deleted user {username}", "success")
    return redirect(url_for("admin.users"))

# ---------------------------------------------------------------------------
# Baselines reset
# ---------------------------------------------------------------------------

@admin_bp.route("/reset_baselines", methods=["GET", "POST"])
@admin_required
def reset_baselines():
    if request.method == "POST":
        # Remove existing baseline docs
        mongo_db.analyses.delete_many({"filename": {"$regex": r"^\\[baseline\\]", "$options": "i"}})
        # Recreate global baselines from dataset
        mongo_db.load_sample_policies_for_user(-1)
        flash("Baselines reset completed", "success")
        return redirect(url_for("admin.dashboard"))
    return render_template("admin/reset_baselines.html")

# ---------------------------------------------------------------------------
# Change admin password
# ---------------------------------------------------------------------------

@admin_bp.route("/change_password", methods=["GET", "POST"])
@admin_required
def change_password():
    if request.method == "POST":
        current_pw = request.form.get("current_password", "")
        new_pw = request.form.get("new_password", "")
        confirm_pw = request.form.get("confirm_password", "")
        cfg = _load_config()
        if not check_password_hash(cfg["password_hash"], current_pw):
            flash("Current password incorrect", "error")
            return redirect(url_for("admin.change_password"))
        if new_pw != confirm_pw or len(new_pw) < 6:
            flash("New passwords do not match or too short", "error")
            return redirect(url_for("admin.change_password"))
        cfg["password_hash"] = generate_password_hash(new_pw)
        _save_config(cfg)
        flash("Password updated", "success")
        return redirect(url_for("admin.dashboard"))
    return render_template("admin/change_password.html")


# Literature Management Routes
@admin_bp.route("/literature")
@admin_required
def literature_dashboard():
    """Literature management dashboard displaying system status and recent processing history."""
    try:
        literature_engine = LiteratureEngine()
        system_status = literature_engine.get_processing_status()
        recent_history = literature_engine.get_recent_processing_history(limit=10)
        
        return render_template(
            "admin/literature_dashboard.html",
            system_status=system_status,
            recent_history=recent_history,
            page_title="Literature Management"
        )
        
    except Exception as e:
        flash(f"Error loading literature dashboard: {str(e)}", "error")
        return redirect(url_for("admin.dashboard"))


@admin_bp.route("/literature/upload", methods=["GET", "POST"])
@admin_required
def literature_upload():
    """Handle academic literature document uploads and processing."""
    if request.method == "GET":
        return render_template("admin/literature_upload.html", page_title="Upload Literature")
    
    try:
        literature_engine = LiteratureEngine()
        
        if 'file' not in request.files:
            flash("No file provided", "error")
            return redirect(request.url)
        
        file = request.files['file']
        if file.filename == '':
            flash("No file selected", "error")
            return redirect(request.url)
        
        # Extract metadata from form
        metadata = {
            'title': request.form.get('title', ''),
            'author': request.form.get('author', ''),
            'publication_year': request.form.get('publication_year', ''),
            'source_url': request.form.get('source_url', ''),
            'description': request.form.get('description', ''),
            'uploaded_by': 'admin',
            'upload_date': datetime.now().isoformat()
        }
        
        # Process the uploaded file
        # Save file first
        saved_path = os.path.join(literature_engine.upload_path, file.filename)
        file.save(saved_path)
        
        # Process using batch_process_directory
        processing_results = literature_engine.batch_process_directory(literature_engine.upload_path)
        
        # Flash message based on results
        # Handle list result
        if processing_results and len(processing_results) > 0:
            first_result = processing_results[0]
            status = first_result.get('status')
        else:
            status = 'error'
        if status == 'integrated_successfully':
            flash("Document processed and integrated successfully!", "success")
        elif status == 'requires_review':
            flash("Document processed but requires manual review", "warning")
        else:
            flash(f"Processing failed: {processing_results.get('message', 'Unknown error')}", "error")
        
        return render_template(
            "admin/literature_results.html",
            results=processing_results,
            page_title="Processing Results"
        )
        
    except Exception as e:
        flash(f"Error processing upload: {str(e)}", "error")
        return redirect(request.url)


@admin_bp.route("/literature/knowledge-base")
@admin_required
def literature_knowledge_base():
    """Knowledge base management interface for reviewing and managing academic insights."""
    try:
        literature_engine = LiteratureEngine()
        kb_status = literature_engine.get_processing_status()
        recent_updates = literature_engine.get_recent_processing_history(limit=20)
        
        return render_template(
            "admin/literature_knowledge_base.html",
            kb_status=kb_status,
            recent_updates=recent_updates,
            page_title="Knowledge Base Management"
        )
        
    except Exception as e:
        flash(f"Error loading knowledge base: {str(e)}", "error")
        return redirect(url_for("admin.dashboard"))


@admin_bp.route("/literature/delete/<document_id>", methods=["POST"])
@admin_required
def delete_literature_document():
    """Delete a document from the knowledge base with backup creation."""
    try:
        document_id = request.view_args.get('document_id')
        literature_engine = LiteratureEngine()
        
        # Implement actual document deletion
        literature_engine = LiteratureEngine()
        kb_manager = literature_engine.knowledge_manager
        
        # Create backup before deletion
        backup_id = kb_manager._create_backup()
        
        # Find and delete the document file
        kb_path = kb_manager.knowledge_base_path
        deleted_files = []
        
        for filename in os.listdir(kb_path):
            if filename.endswith(".md") and document_id in filename:
                file_path = os.path.join(kb_path, filename)
                os.remove(file_path)
                
                # Remove from version history
                kb_manager.remove_document_from_history(doc_id)
                deleted_files.append(filename)
        
        if deleted_files:
            flash(f"Successfully deleted {len(deleted_files)} file(s). Backup created: {backup_id}", "success")
        else:
            flash(f"Document {document_id} not found in knowledge base", "warning")        
        return redirect(url_for('admin.literature_knowledge_base'))
        
    except Exception as e:
        flash(f"Error deleting document: {str(e)}", "error")
        return redirect(url_for('admin.literature_knowledge_base'))


@admin_bp.route("/literature/cleanup", methods=["GET", "POST"])
@admin_required  
def literature_cleanup():
    """Cleanup interface for removing outdated documents."""
    if request.method == "GET":
        # Show cleanup interface
        try:
            literature_engine = LiteratureEngine()
            kb_status = literature_engine.get_processing_status()
            recent_updates = literature_engine.get_recent_processing_history(limit=50)
            
            return render_template(
                "admin/literature_cleanup.html",
                kb_status=kb_status,
                recent_updates=recent_updates,
                page_title="Literature Cleanup"
            )
            
        except Exception as e:
            flash(f"Error loading cleanup interface: {str(e)}", "error")
            return redirect(url_for('admin.literature_dashboard'))
    
    else:
        # Handle cleanup actions
        selected_docs = request.form.getlist('selected_documents')
        action = request.form.get('action')
        
        if not selected_docs:
            flash("No documents selected", "error")
            return redirect(request.url)
        
        try:
            if action == 'delete':
                literature_engine = LiteratureEngine()
                kb_manager = literature_engine.knowledge_manager
                
                # Create backup before bulk operations
                backup_id = kb_manager._create_backup()
                
                deleted_count = 0
                kb_path = kb_manager.knowledge_base_path
                
                for doc_id in selected_docs:
                    for filename in os.listdir(kb_path):
                            file_path = os.path.join(kb_path, filename)
                            os.remove(file_path)
                
                # Remove from version history
                            kb_manager.remove_document_from_history(doc_id)
                            deleted_count += 1
                            break
                
                flash(f"Successfully deleted {deleted_count} document(s). Backup created: {backup_id}", "success")
            elif action == 'archive':
                flash(f"Successfully archived {len(selected_docs)} documents", "success")            
            return redirect(url_for('admin.literature_knowledge_base'))
            
        except Exception as e:
            flash(f"Error during cleanup: {str(e)}", "error")
            return redirect(request.url)


import logging
from flask import jsonify

def _get_document_details_data(document_id):
    """Helper function to get document details data."""
    logger = logging.getLogger(__name__)
    logger.info("Fetching details for document: %s", document_id)
    literature_engine = LiteratureEngine()
    
    # Initialise default values
    document_info = {'document_id': document_id, 'filename': 'Unknown', 'timestamp': ''}
    insights = []
    
    # Safely get version history with error handling
    try:
        version_history = getattr(literature_engine.knowledge_manager, 'version_history', None)
        if version_history is not None:
            for entry in version_history:
                if isinstance(entry, dict) and entry.get('document_id') == document_id:
                    document_info.update(entry)  # Update with any additional fields from the entry
                    break
        else:
            logger.warning("version_history is None in knowledge_manager")
    except Exception as e:
        logger.warning("Error accessing version history: %s", str(e))
    
    # Try to load additional details from the actual files
    document_content = ''
    word_count = 0
    original_filename = ''
    generated_filename = ''
    kb_path = ''
    uploads_path = ''
    
    try:
        logger.info("Starting file detection for document_id: %s", document_id)
        if hasattr(literature_engine.knowledge_manager, 'knowledge_base_path'):
            # Get the base directory of the application
            base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
            
            # Get and normalize the knowledge base path
            kb_relative_path = getattr(literature_engine.knowledge_manager, 'knowledge_base_path', '')
            kb_path = os.path.abspath(os.path.join(base_dir, kb_relative_path))
            
            # Get the uploads path from LiteratureEngine's configuration
            uploads_path = os.path.abspath(os.path.join(
                base_dir, 
                'data', 
                'literature'  # This matches the default upload_path in LiteratureEngine
            ))
            
            # Create the uploads directory if it doesn't exist
            os.makedirs(uploads_path, exist_ok=True)
            
            logger.info("Base directory: %s", base_dir)
            logger.info("Knowledge base path: %s", kb_path)
            logger.info("Uploads path: %s", uploads_path)
            logger.info("Knowledge base path exists: %s", "Yes" if kb_path and os.path.exists(kb_path) else "No (empty path)")
            logger.info("Uploads path exists: %s", "Yes" if uploads_path and os.path.exists(uploads_path) else "No (empty path)")
            logger.info("Knowledge base directory contents: %s", str(os.listdir(kb_path)) if kb_path and os.path.exists(kb_path) else 'Not found')
            logger.info("Uploads directory contents: %s", str(os.listdir(uploads_path)) if uploads_path and os.path.exists(uploads_path) else 'Not found')
            
            # Find the generated markdown file
            if kb_path and os.path.exists(kb_path):
                try:
                    kb_files = os.listdir(kb_path)
                except Exception as e:
                    logger.warning("Could not list files in knowledge base path: %s", str(e))
                    kb_files = []
                
                logger.info("Searching through %d files in knowledge base", len(kb_files))
                for filename in kb_files:
                    logger.info("Checking file: %s", filename)
                    if document_id in filename and filename.endswith('.md'):
                        logger.info("Found matching markdown file: %s", filename)
                        generated_filename = filename
                        file_path = os.path.join(kb_path, filename)
                        logger.info("Full path to markdown: %s", file_path)
                        logger.info("File exists: %s", "Yes" if os.path.exists(file_path) else "No")
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                                document_content = content
                                # Calculate word count (simple whitespace-based count)
                                word_count = len(content.split())
                                
                                # Parse insights from markdown if method exists
                                if hasattr(literature_engine.knowledge_manager, '_parse_insights_from_markdown'):
                                    try:
                                        parsed_insights = literature_engine.knowledge_manager._parse_insights_from_markdown(content)
                                        if isinstance(parsed_insights, list):
                                            # Ensure all insights are dictionaries with the expected structure
                                            insights = []
                                            for insight in parsed_insights:
                                                if isinstance(insight, dict):
                                                    insights.append(insight)
                                                elif isinstance(insight, str):
                                                    # Convert string insights to proper format
                                                    insights.append({
                                                        'content': insight,
                                                        'type': 'insight',
                                                        'relevance': 0.8  # Default relevance score
                                                    })
                                    except Exception as parse_error:
                                        logger.warning("Error parsing insights from %s: %s", filename, str(parse_error))
                                        insights = []
                        except Exception as file_error:
                            logger.warning("Could not read generated file %s: %s", filename, str(file_error))
            
            # Try to find the original uploaded file
            original_found = False
            if os.path.exists(uploads_path):
                try:
                    logger.info("Searching for original file in uploads directory: %s", uploads_path)
                    upload_files = os.listdir(uploads_path)
                    logger.info("Found %d files in uploads directory", len(upload_files))
                    
                    # Look for files that match the document_id but aren't markdown files
                    for filename in upload_files:
                        logger.info("Checking upload file: %s", filename)
                        # Match the document_id in the filename (handles both with and without timestamps)
                        base_name = os.path.splitext(filename)[0]
                        if document_id in base_name and not filename.endswith('.md'):
                            logger.info("Found matching original file: %s", filename)
                            original_filename = filename
                            original_file_exists = False
                            if original_filename:
                                # First check in the standard location
                                original_file_path = os.path.join(uploads_path, original_filename)
                                original_file_exists = os.path.exists(original_file_path)
                                
                                # If not found, check if it's a pre-loaded document
                                if not original_file_exists and document_info.get('is_preloaded', False):
                                    preloaded_path = os.path.join('docs', 'knowledge_base', 'preloaded', original_filename)
                                    if os.path.exists(preloaded_path):
                                        original_file_path = preloaded_path
                                        original_file_exists = True
                                        
                                logger.info("Original file %s at %s", "found" if original_file_exists else "not found", original_file_path)
                            
                            if original_file_exists:
                                logger.info("Original file exists")
                                # Try to get file stats
                                try:
                                    file_stats = os.stat(original_file_path)
                                    logger.info("File size: %d bytes", file_stats.st_size)
                                    logger.info("Last modified: %s", datetime.fromtimestamp(file_stats.st_mtime))
                                    original_found = True
                                except Exception as e:
                                    logger.warning("Could not get file stats: %s", str(e))
                            else:
                                logger.warning("Original file not found at: %s", original_path)
                            
                            break
                except Exception as e:
                    logger.warning("Could not list files in uploads directory: %s", str(e))
            
            # If original file not found, check if it's a pre-loaded document
            if not original_found:
                logger.info("Original file not found in uploads directory")
                # Check if this is a pre-loaded document (like unesco_2021_insights.md)
                if any(doc in document_id for doc in [
                    'unesco_2021', 'an_yu_james_2025', 'selwyn_etal_2020', 
                    'bond_etal_2024', 'bobula_2024', 'webb_jisc_2023',
                    'barnes_hutson_2024', 'dabis_csaki_2024', 'unesco_2023',
                    'chan_hu_2023', 'bera_2018', 'eu_ai_act_2024', 'li_etal_2024'
                ]):
                    logger.info("This appears to be a pre-loaded document")
                    document_info['is_preloaded'] = True
    except Exception as e:
        logger.error("Error processing document files: %s", str(e), exc_info=True)
        return {
            'success': False,
            'error': 'Error processing document files',
            'details': str(e)
        }
    
    # Set quality dimensions with safe defaults
    quality_score = float(document_info.get('quality_score', 0.5))  # Default to 0.5 if not available
    
    # Ensure quality score is within valid range
    quality_score = max(0.0, min(1.0, quality_score))
    
    # Set quality dimensions
    quality_dimensions = [
        {'name': 'Source Credibility', 'score': min(1.0, quality_score + 0.1)},
        {'name': 'Content Quality', 'score': quality_score},
        {'name': 'Methodology Quality', 'score': max(0.0, quality_score - 0.1)},
        {'name': 'Policy Relevance', 'score': quality_score * 0.9}
    ]
    
    # Add file information to document info
    document_info['word_count'] = word_count
    document_info['character_count'] = len(document_content) if document_content else 0
    document_info['original_filename'] = original_filename
    document_info['generated_filename'] = generated_filename
    document_info['original_file_exists'] = os.path.exists(os.path.join(uploads_path, original_filename)) if original_filename else False
    document_info['generated_file_exists'] = os.path.exists(os.path.join(kb_path, generated_filename)) if generated_filename else False
    document_info['is_preloaded'] = document_info.get('is_preloaded', False)
    
    # Load saved analysis data if available
    analysis_data = {}
    analysis_dir = os.path.join('data', 'analysis')
    analysis_file = os.path.join(analysis_dir, f"{document_id}.json")
    
    if os.path.exists(analysis_file):
        try:
            with open(analysis_file, 'r', encoding='utf-8') as f:
                analysis_data = json.load(f)
                logger.info("Loaded analysis data from %s", analysis_file)
        except Exception as e:
            logger.warning("Could not load analysis data: %s", str(e))
    else:
        logger.info("No analysis data found at %s", analysis_file)
    
    # Ensure metadata is properly structured
    metadata = document_info.get('metadata', {})
    if not isinstance(metadata, dict):
        metadata = {}
    
    # Ensure we have the required metadata fields with proper fallbacks
    if 'author' not in metadata and 'author' in document_info:
        metadata['author'] = document_info['author']
    
    # Try to extract university/institution from filename if not in metadata
    if 'university' not in metadata and 'institution' not in metadata and 'affiliation' not in metadata:
        # Extract the base filename without extension
        filename = document_info.get('filename', '')
        base_name = os.path.splitext(filename)[0]  # Remove file extension
        
        # Common patterns in filenames
        patterns = ['university', 'college', 'institute', 'school', 'polytechnic']
        
        # Try to find the university name before these patterns
        for pattern in patterns:
            if f'-{pattern}' in base_name.lower():
                # Extract everything before the pattern
                parts = base_name.split(f'-{pattern}')[0].split('-')
                # Join the parts and clean up
                university = ' '.join(part.capitalize() for part in parts if part)
                if university:
                    metadata['university'] = f"{university} {pattern.capitalize()}"
                    break
    
    # Handle university/affiliation fields
    if 'university' not in metadata and 'institution' in metadata:
        metadata['university'] = metadata['institution']
    elif 'institution' not in metadata and 'university' in metadata:
        metadata['institution'] = metadata['university']
    
    if 'affiliation' not in metadata and ('university' in metadata or 'institution' in metadata):
        metadata['affiliation'] = metadata.get('university') or metadata.get('institution')
    
    # Handle publication date/year
    if 'publication_date' not in metadata and 'publication_year' in metadata:
        metadata['publication_date'] = f"{metadata['publication_year']}-01-01"  # Default to Jan 1st if only year is known
    
    # Prepare comprehensive response
    details = {
        'success': True,
        'filename': document_info.get('filename', 'Unknown'),
        'document_id': document_id,
        'timestamp': document_info.get('timestamp', datetime.now().isoformat()),
        'quality_score': (quality_score * 10) if quality_score is not None else None,  # Scale to 1-10 for display
        'quality_dimensions': quality_dimensions or {},
        'insights': insights or [],
        'analysis_data': analysis_data or {},
        'word_count': word_count or 0,
        'character_count': len(document_content) if document_content else 0,
        'original_filename': original_filename or '',
        'generated_filename': generated_filename or '',
        'original_file_exists': os.path.exists(os.path.join(uploads_path, original_filename)) if original_filename else False,
        'generated_file_exists': os.path.exists(os.path.join(kb_path, generated_filename)) if generated_filename else False,
        'is_preloaded': document_info.get('is_preloaded', False),
        'insights_extracted': len(insights) if insights else 0,
        'metadata': metadata,  # Use the processed metadata
        # Add direct access to common metadata fields for easier template access
        'author': metadata.get('author', ''),
        'university': metadata.get('university', metadata.get('institution', metadata.get('affiliation', ''))),
        'publication_year': metadata.get('publication_year', 
                                       metadata.get('publication_date', '')[:4] if 'publication_date' in metadata else '')
    }
    
    # Add any missing fields that might be expected by the template
    if 'analysis_summary' not in details['analysis_data']:
        details['analysis_data']['analysis_summary'] = ''
    
    return details

@admin_bp.route("/literature/document/<document_id>", methods=["GET"])
@admin_required
def view_document_details(document_id):
    """Display document details in a dedicated template."""
    details = _get_document_details_data(document_id)
    if 'success' in details and not details['success']:
        flash(f"Error loading document details: {details.get('error', 'Unknown error')}", "danger")
        return redirect(url_for('admin.literature_knowledge_base'))
    
    return render_template('admin/document_details.html', document=details)

@admin_bp.route("/literature/document-details/<document_id>", methods=["GET"])
@admin_required
def get_document_details(document_id):
    """API endpoint to get document details as JSON."""
    details = _get_document_details_data(document_id)
    if 'success' in details and not details['success']:
        return jsonify({
            'success': False,
            'error': details.get('error', 'Unknown error'),
            'details': details.get('details', '')
        }), 500
    
    return jsonify(details)
