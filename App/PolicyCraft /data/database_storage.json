{
  "documents": [],
  "analyses": [
    {
      "_id": "analysis_1",
      "user_id": 5,
      "document_id": null,
      "filename": "5_20250618_160913_sample_stanford_stanford-ai-policy.pdf",
      "analysis_date": "2025-06-18T15:09:14.437807",
      "text_data": {
        "original_text": "Generative AI Policy Guidance Main content start Guidance adopted on February 16, 2023    Honor Code Implications of Generative AI Tools The Board on Conduct Affairs (BCA) has been asked to address the Honor Code implications of generative AI tools such as ChatGPT, Bard, DALL-E, and Stable Diffusion. These are novel tools, and both students and instructors have been experimenting with their use in academic settings.   While these tools have applications that foster student learning and understanding, these tools can also be used in ways that bypass key learning objectives. To give sufficient space for instructors to explore uses of generative AI tools in their courses, and to set clear guidelines to students about what uses are and are not consistent with the Stanford Honor Code, the BCA has set forth the following policy guidance regarding generative AI in the context of coursework: Absent a clear statement from a course instructor, use of or consultation with generative AI shall be treated analogously to assistance from another person. In particular, using generative AI tools to substantially complete an assignment or exam (e.g. by entering exam or assignment questions) is not permitted. Students should acknowledge the use of generative AI (other than incidental use) and default to disclosing such assistance when in doubt. \n\nIndividual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools. Course instructors should set such policies in their course syllabi and clearly communicate such policies to students. Students who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification. The BCA will continue to monitor developments in these tools and their use in academic settings and may update this guidance. Members of the community are encouraged to contact the BCA to provide input, suggestions, and comments on this policy. NOTE: As part of the BCA\u2019s guidance on clear communication of a course\u2019s generative AI policy, OCS recommends course instructors provide clear advance notice that they may use detection software to review work submitted for use of generative AI.   \n",
        "cleaned_text": "Generative AI Policy Guidance Main content start Guidance adopted on February 16, 2023 Honor Code Implications of Generative AI Tools The Board on Conduct Affairs (BCA) has been asked to address the Honor Code implications of generative AI tools such as ChatGPT, Bard, DALL-E, and Stable Diffusion. These are novel tools, and both students and instructors have been experimenting with their use in academic settings. While these tools have applications that foster student learning and understanding, these tools can also be used in ways that bypass key learning objectives. To give sufficient space for instructors to explore uses of generative AI tools in their courses, and to set clear guidelines to students about what uses are and are not consistent with the Stanford Honor Code, the BCA has set forth the following policy guidance regarding generative AI in the context of coursework: Absent a clear statement from a course instructor, use of or consultation with generative AI shall be treated analogously to assistance from another person. In particular, using generative AI tools to substantially complete an assignment or exam (e. g. by entering exam or assignment questions) is not permitted. Students should acknowledge the use of generative AI (other than incidental use) and default to disclosing such assistance when in doubt. Individual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools. Course instructors should set such policies in their course syllabi and clearly communicate such policies to students. Students who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification. The BCA will continue to monitor developments in these tools and their use in academic settings and may update this guidance. Members of the community are encouraged to contact the BCA to provide input, suggestions, and comments on this policy. NOTE: As part of the BCA s guidance on clear communication of a course s generative AI policy, OCS recommends course instructors provide clear advance notice that they may use detection software to review work submitted for use of generative AI.",
        "text_length": 2268
      },
      "themes": [
        {
          "name": "Student Guidelines",
          "score": 4.5,
          "frequency": 4,
          "keywords": [
            "student",
            "students",
            "coursework",
            "assignment"
          ],
          "matches": [],
          "confidence": 45
        },
        {
          "name": "Faculty Guidelines",
          "score": 3.5,
          "frequency": 3,
          "keywords": [
            "instructor",
            "course"
          ],
          "matches": [],
          "confidence": 35
        },
        {
          "name": "Academic Integrity",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "academic"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "AI Technology",
          "score": 1.0,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 10,
          "entities": [
            "Guidance Main",
            "AI"
          ]
        },
        {
          "name": "Assessment and Evaluation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 45,
        "method": "hybrid",
        "scores": {
          "Restrictive": 1.3154834942672964,
          "Moderate": 4.994435190664014,
          "Permissive": 8.970081315068684
        },
        "reasoning": "Classified as Permissive based on: - 'guidance' appears 5 time(s) - 'encouraged' appears 2 time(s) - 'assistance' appears 2 time(s) - Encouraging language detected",
        "details": {
          "rule_based": {
            "classification": "Permissive",
            "confidence": 71,
            "scores": {
              "Restrictive": 0.0,
              "Moderate": 5.3999999999999995,
              "Permissive": 13.4
            },
            "keyword_matches": {
              "Restrictive": [],
              "Moderate": [
                [
                  "guidelines",
                  1,
                  1.2
                ],
                [
                  "context",
                  1,
                  1.4
                ],
                [
                  "individual",
                  1,
                  1.4
                ],
                [
                  "particular",
                  1,
                  1.4
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  2,
                  3.6
                ],
                [
                  "foster",
                  1,
                  1.8
                ],
                [
                  "guidance",
                  5,
                  5.0
                ],
                [
                  "suggestions",
                  1,
                  1.0
                ],
                [
                  "assistance",
                  2,
                  2.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 43,
            "scores": {
              "Moderate": 0.43860879766600336,
              "Permissive": 0.23252032876717124,
              "Restrictive": 0.3288708735668241
            },
            "method": "ml_based"
          },
          "agreement": false
        }
      },
      "summary": {
        "total_themes": 5,
        "top_theme": "Student Guidelines",
        "classification_type": "Permissive",
        "confidence": 45
      },
      "analysis_id": "analysis_1"
    },
    {
      "_id": "analysis_2",
      "user_id": 5,
      "document_id": null,
      "filename": "5_20250618_160914_sample_edinburgh_edinburgh university-ai-policy.pdf",
      "analysis_date": "2025-06-18T15:09:15.615174",
      "text_data": {
        "original_text": "Guidance\tfor\tworking\twith\tGenerative\tAI\t(\u201cGenAI\u201d)\tin\tyour\tstudies\tGenAI guidance for students. The technology, ethics and use of AI is a fast-moving area. This guidance is current as of October 2024 and will be reviewed by 30th June 2025. University\tposition\ton\tGenAI\tThere is currently a lot of interest in Generative AI (GenAI) systems (e.g. ChatGPT, DALL-E, Microsoft Copilot, Claude, LLama, Grammarly Pro and Google Gemini). We recognise that developing skills in the responsible use of AI is important for you and will be an important part of your future life and work. We want to help you understand how GenAI may be used to support and aid your learning, research and assessments, while making you aware of the limitations and risks. All University of Edinburgh students have free access to ELM (Edinburgh (access to) Language Models), the University\u2019s AI access and innovation platform, offering you a gateway to safer access to GenAI. ELM provides some key benefits over accessing AI through other methods. We would encourage you to use ELM over other similar tools because: 1. You can access a wider range of AI Large Language models through ELM including the very latest and most powerful versions of ChatGPT as well as, coming soon, Open Source LLMs. 2. Your data is secure and will not be retained by third party services to train their models or for any other purpose. The University has a Zero Data Retention agreement with OpenAI which assures that your data is secure and private. All your chat histories and your document downloads are kept private to you on your instance of ELM. 3. It is free to use for all staff and students, providing the same access for all. 4. You can innovate on top of ELM by writing your own AI applications through our API. \n5. ELM is fully supported by the University through your local IT teams, EdHelp and the IS Helpline. How to get started with ELM Golden\trules\tfor\tGenAI\tuse\tWhen using any GenAI tool there are a few golden rules. By following these points, you will be able to benefit from using GenAI while also reducing the likelihood of engaging in academic misconduct. 1. Learn, don\u2019t copy: Use GenAI to aid your learning, but never copy-paste any GenAI outputs into your own assessed work. Doing so constitutes academic misconduct. 2. Ask if uncertain: Always consult your Course Organiser if you are unclear about the use of GenAI in your assessed work. Some assessed work may encourage GenAI use, while others may impose restrictions. 3. Credit use of tools: Before handing in your assessed work, make sure you acknowledge the use of GenAI, where used. 4. Protect personal data: Avoid uploading personal data - yours or anyone else\u2019s - to a GenAI platform, unless you are using the University\u2019s secure platform, ELM, and complying with the University's data protection policy.  5. Respect copyrights: Never upload copyrighted materials to a GenAI platform without authorization from the copyright owner. If you are using the University\u2019s secure platform, ELM, ensure you have the right to use the material for that purpose.   6. Verify facts: Always check GenAI output for factual accuracy, including references and citations. 7. Diversify sources: Never rely solely on GenAI; it should supplement, but not replace, traditional sources. University\u2019s data protection policy. Acceptable\tuses\tof\tGenAI\tGenAI\tand\treasonable\tadjustments\tPlease note, guidance on acceptable uses of GenAI does not preclude the use of AI tools where they are being used in the context of a reasonable adjustment. Disabled students should register with the Disability and Learning Support \nService in order to put in place a Schedule of Adjustments (a list of modifications to how you experience your teaching, learning and research). Disability and Learning Support Service Before using GenAI in any assessed work, please check whether there are any restrictions. This should be mentioned in the assessment task. If not, then ask your Course Organiser. Some assessments may explicitly ask you to work with AI tools and to analyse and critique the content it generates. Other assessments may specify, for good reason, that AI tools should not be used in particular ways. Please also make sure that you follow the University\u2019s Guidance on Proofreading of Students Assessments when using GenAI for proofreading, editing and/or translation. You will never be asked to pay to use external GenAI tools. Some of the positive, and generally acceptable, ways GenAI might be used include: \u2022 Brainstorming ideas through prompts \u2022 Getting explanations of difficult ideas, questions and concepts \u2022 Self-tutoring through conversation with the GenAI tool \u2022 Creating practice questions and self-tests \u2022 Organising and summarising your notes \u2022 Planning and structuring your writing \u2022 Summarising a text, article or book (Check first that the copyright owner permits use of GenAI for this purpose) \u2022 Helping to improve your grammar, spelling, and writing (Check for restrictions whe",
        "cleaned_text": "Guidance for working with Generative AI ( GenAI ) in your studies GenAI guidance for students. The technology, ethics and use of AI is a fast-moving area. This guidance is current as of October 2024 and will be reviewed by 30th June 2025. University position on GenAI There is currently a lot of interest in Generative AI (GenAI) systems (e. g. ChatGPT, DALL-E, Microsoft Copilot, Claude, LLama, Grammarly Pro and Google Gemini). We recognise that developing skills in the responsible use of AI is important for you and will be an important part of your future life and work. We want to help you understand how GenAI may be used to support and aid your learning, research and assessments, while making you aware of the limitations and risks. All University of Edinburgh students have free access to ELM (Edinburgh (access to) Language Models), the University s AI access and innovation platform, offering you a gateway to safer access to GenAI. ELM provides some key benefits over accessing AI through other methods. We would encourage you to use ELM over other similar tools because: 1. You can access a wider range of AI Large Language models through ELM including the very latest and most powerful versions of ChatGPT as well as, coming soon, Open Source LLMs. 2. Your data is secure and will not be retained by third party services to train their models or for any other purpose. The University has a Zero Data Retention agreement with OpenAI which assures that your data is secure and private. All your chat histories and your document downloads are kept private to you on your instance of ELM. 3. It is free to use for all staff and students, providing the same access for all. 4. You can innovate on top of ELM by writing your own AI applications through our API. 5. ELM is fully supported by the University through your local IT teams, EdHelp and the IS Helpline. How to get started with ELM Golden rules for GenAI use When using any GenAI tool there are a few golden rules. By following these points, you will be able to benefit from using GenAI while also reducing the likelihood of engaging in academic misconduct. 1. Learn, do not copy: Use GenAI to aid your learning, but never copy-paste any GenAI outputs into your own assessed work. Doing so constitutes academic misconduct. 2. Ask if uncertain: Always consult your Course Organiser if you are unclear about the use of GenAI in your assessed work. Some assessed work may encourage GenAI use, while others may impose restrictions. 3. Credit use of tools: Before handing in your assessed work, make sure you acknowledge the use of GenAI, where used. 4. Protect personal data: Avoid uploading personal data - yours or anyone else s - to a GenAI platform, unless you are using the University s secure platform, ELM, and complying with the University s data protection policy. 5. Respect copyrights: Never upload copyrighted materials to a GenAI platform without authorization from the copyright owner. If you are using the University s secure platform, ELM, ensure you have the right to use the material for that purpose. 6. Verify facts: Always check GenAI output for factual accuracy, including references and citations. 7. Diversify sources: Never rely solely on GenAI; it should supplement, but not replace, traditional sources. University s data protection policy. Acceptable uses of GenAI GenAI and reasonable adjustments Please note, guidance on acceptable uses of GenAI does not preclude the use of AI tools where they are being used in the context of a reasonable adjustment. Disabled students should register with the Disability and Learning Support Service in order to put in place a Schedule of Adjustments (a list of modifications to how you experience your teaching, learning and research). Disability and Learning Support Service Before using GenAI in any assessed work, please check whether there are any restrictions. This should be mentioned in the assessment task. If not, then ask your Course Organiser. Some assessments may explicitly ask you to work with AI tools and to analyse and critique the content it generates. Other assessments may specify, for good reason, that AI tools should not be used in particular ways. Please also make sure that you follow the University s Guidance on Proofreading of Students Assessments when using GenAI for proofreading, editing and or translation. You will never be asked to pay to use external GenAI tools. Some of the positive, and generally acceptable, ways GenAI might be used include: Brainstorming ideas through prompts Getting explanations of difficult ideas, questions and concepts Self-tutoring through conversation with the GenAI tool Creating practice questions and self-tests Organising and summarising your notes Planning and structuring your writing Summarising a text, article or book (Check first that the copyright owner permits use of GenAI for this purpose) Helping to improve your grammar, spelling, and writing (Check for restrictions where use of language ",
        "text_length": 11710
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 24.0,
          "frequency": 24,
          "keywords": [
            "plagiarism",
            "cheating",
            "academic",
            "misconduct",
            "citation"
          ],
          "matches": [
            "academic misconduct",
            "academic misconduct",
            "academic misconduct"
          ],
          "confidence": 100
        },
        {
          "name": "Privacy and Data",
          "score": 18.0,
          "frequency": 18,
          "keywords": [
            "private",
            "data",
            "personal",
            "protection",
            "security"
          ],
          "matches": [
            "personal data",
            "personal data",
            "data protection"
          ],
          "confidence": 100
        },
        {
          "name": "AI Technology",
          "score": 9.0,
          "frequency": 9,
          "keywords": [],
          "matches": [],
          "confidence": 90,
          "entities": [
            "Generative AI",
            "the University s AI",
            "GenAI"
          ]
        },
        {
          "name": "Assessment and Evaluation",
          "score": 6.0,
          "frequency": 6,
          "keywords": [
            "assessment",
            "evaluation"
          ],
          "matches": [],
          "confidence": 60
        },
        {
          "name": "Student Guidelines",
          "score": 6.0,
          "frequency": 6,
          "keywords": [
            "student",
            "students",
            "assignment",
            "report"
          ],
          "matches": [],
          "confidence": 60
        },
        {
          "name": "Faculty Guidelines",
          "score": 3.5,
          "frequency": 3,
          "keywords": [
            "staff",
            "teaching",
            "course"
          ],
          "matches": [],
          "confidence": 35
        },
        {
          "name": "Research and Innovation",
          "score": 2.5,
          "frequency": 2,
          "keywords": [
            "research",
            "innovation",
            "analysis"
          ],
          "matches": [],
          "confidence": 25
        },
        {
          "name": "AI Ethics",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "ethics",
            "responsible",
            "benefit",
            "respect"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Transparency",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "transparent",
            "open"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Restrictive",
        "confidence": 54,
        "method": "hybrid",
        "scores": {
          "Restrictive": 14.218749738418419,
          "Moderate": 10.40001895964998,
          "Permissive": 10.701231301931605
        },
        "reasoning": "Classified as Restrictive based on: - 'academic misconduct' appears 6 time(s) - 'limited' appears 2 time(s) - 'required' appears 2 time(s) - Penalty/consequence language present",
        "details": {
          "rule_based": {
            "classification": "Restrictive",
            "confidence": 38,
            "scores": {
              "Restrictive": 20.200000000000003,
              "Moderate": 15.0,
              "Permissive": 17.0
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "limited",
                  2,
                  3.0
                ],
                [
                  "academic misconduct",
                  6,
                  12.0
                ],
                [
                  "required",
                  2,
                  2.6
                ],
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "essential",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "reasonable",
                  2,
                  3.0
                ],
                [
                  "guidelines",
                  1,
                  1.2
                ],
                [
                  "requirements",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  1,
                  1.3
                ],
                [
                  "respect",
                  1,
                  1.3
                ],
                [
                  "context",
                  2,
                  2.8
                ],
                [
                  "particular",
                  3,
                  4.199999999999999
                ]
              ],
              "Permissive": [
                [
                  "support",
                  3,
                  5.4
                ],
                [
                  "benefits",
                  1,
                  1.2
                ],
                [
                  "improve",
                  1,
                  1.2
                ],
                [
                  "innovation",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  5,
                  5.0
                ],
                [
                  "support",
                  3,
                  3.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Restrictive",
            "confidence": 52,
            "scores": {
              "Moderate": 0.3500047399124949,
              "Permissive": 0.12530782548290156,
              "Restrictive": 0.5246874346046043
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 10,
        "top_theme": "Academic Integrity",
        "classification_type": "Restrictive",
        "confidence": 54
      },
      "analysis_id": "analysis_2"
    },
    {
      "_id": "analysis_3",
      "user_id": 5,
      "document_id": null,
      "filename": "5_20250618_160914_sample_edinburgh_edinburgh university-ai-policy.pdf",
      "analysis_date": "2025-06-18T19:31:50.311168",
      "text_data": {
        "original_text": "Guidance\tfor\tworking\twith\tGenerative\tAI\t(\u201cGenAI\u201d)\tin\tyour\tstudies\tGenAI guidance for students. The technology, ethics and use of AI is a fast-moving area. This guidance is current as of October 2024 and will be reviewed by 30th June 2025. University\tposition\ton\tGenAI\tThere is currently a lot of interest in Generative AI (GenAI) systems (e.g. ChatGPT, DALL-E, Microsoft Copilot, Claude, LLama, Grammarly Pro and Google Gemini). We recognise that developing skills in the responsible use of AI is important for you and will be an important part of your future life and work. We want to help you understand how GenAI may be used to support and aid your learning, research and assessments, while making you aware of the limitations and risks. All University of Edinburgh students have free access to ELM (Edinburgh (access to) Language Models), the University\u2019s AI access and innovation platform, offering you a gateway to safer access to GenAI. ELM provides some key benefits over accessing AI through other methods. We would encourage you to use ELM over other similar tools because: 1. You can access a wider range of AI Large Language models through ELM including the very latest and most powerful versions of ChatGPT as well as, coming soon, Open Source LLMs. 2. Your data is secure and will not be retained by third party services to train their models or for any other purpose. The University has a Zero Data Retention agreement with OpenAI which assures that your data is secure and private. All your chat histories and your document downloads are kept private to you on your instance of ELM. 3. It is free to use for all staff and students, providing the same access for all. 4. You can innovate on top of ELM by writing your own AI applications through our API. \n5. ELM is fully supported by the University through your local IT teams, EdHelp and the IS Helpline. How to get started with ELM Golden\trules\tfor\tGenAI\tuse\tWhen using any GenAI tool there are a few golden rules. By following these points, you will be able to benefit from using GenAI while also reducing the likelihood of engaging in academic misconduct. 1. Learn, don\u2019t copy: Use GenAI to aid your learning, but never copy-paste any GenAI outputs into your own assessed work. Doing so constitutes academic misconduct. 2. Ask if uncertain: Always consult your Course Organiser if you are unclear about the use of GenAI in your assessed work. Some assessed work may encourage GenAI use, while others may impose restrictions. 3. Credit use of tools: Before handing in your assessed work, make sure you acknowledge the use of GenAI, where used. 4. Protect personal data: Avoid uploading personal data - yours or anyone else\u2019s - to a GenAI platform, unless you are using the University\u2019s secure platform, ELM, and complying with the University's data protection policy.  5. Respect copyrights: Never upload copyrighted materials to a GenAI platform without authorization from the copyright owner. If you are using the University\u2019s secure platform, ELM, ensure you have the right to use the material for that purpose.   6. Verify facts: Always check GenAI output for factual accuracy, including references and citations. 7. Diversify sources: Never rely solely on GenAI; it should supplement, but not replace, traditional sources. University\u2019s data protection policy. Acceptable\tuses\tof\tGenAI\tGenAI\tand\treasonable\tadjustments\tPlease note, guidance on acceptable uses of GenAI does not preclude the use of AI tools where they are being used in the context of a reasonable adjustment. Disabled students should register with the Disability and Learning Support \nService in order to put in place a Schedule of Adjustments (a list of modifications to how you experience your teaching, learning and research). Disability and Learning Support Service Before using GenAI in any assessed work, please check whether there are any restrictions. This should be mentioned in the assessment task. If not, then ask your Course Organiser. Some assessments may explicitly ask you to work with AI tools and to analyse and critique the content it generates. Other assessments may specify, for good reason, that AI tools should not be used in particular ways. Please also make sure that you follow the University\u2019s Guidance on Proofreading of Students Assessments when using GenAI for proofreading, editing and/or translation. You will never be asked to pay to use external GenAI tools. Some of the positive, and generally acceptable, ways GenAI might be used include: \u2022 Brainstorming ideas through prompts \u2022 Getting explanations of difficult ideas, questions and concepts \u2022 Self-tutoring through conversation with the GenAI tool \u2022 Creating practice questions and self-tests \u2022 Organising and summarising your notes \u2022 Planning and structuring your writing \u2022 Summarising a text, article or book (Check first that the copyright owner permits use of GenAI for this purpose) \u2022 Helping to improve your grammar, spelling, and writing (Check for restrictions whe",
        "cleaned_text": "Guidance for working with Generative AI ( GenAI ) in your studies GenAI guidance for students. The technology, ethics and use of AI is a fast-moving area. This guidance is current as of October 2024 and will be reviewed by 30th June 2025. University position on GenAI There is currently a lot of interest in Generative AI (GenAI) systems (e. g. ChatGPT, DALL-E, Microsoft Copilot, Claude, LLama, Grammarly Pro and Google Gemini). We recognise that developing skills in the responsible use of AI is important for you and will be an important part of your future life and work. We want to help you understand how GenAI may be used to support and aid your learning, research and assessments, while making you aware of the limitations and risks. All University of Edinburgh students have free access to ELM (Edinburgh (access to) Language Models), the University s AI access and innovation platform, offering you a gateway to safer access to GenAI. ELM provides some key benefits over accessing AI through other methods. We would encourage you to use ELM over other similar tools because: 1. You can access a wider range of AI Large Language models through ELM including the very latest and most powerful versions of ChatGPT as well as, coming soon, Open Source LLMs. 2. Your data is secure and will not be retained by third party services to train their models or for any other purpose. The University has a Zero Data Retention agreement with OpenAI which assures that your data is secure and private. All your chat histories and your document downloads are kept private to you on your instance of ELM. 3. It is free to use for all staff and students, providing the same access for all. 4. You can innovate on top of ELM by writing your own AI applications through our API. 5. ELM is fully supported by the University through your local IT teams, EdHelp and the IS Helpline. How to get started with ELM Golden rules for GenAI use When using any GenAI tool there are a few golden rules. By following these points, you will be able to benefit from using GenAI while also reducing the likelihood of engaging in academic misconduct. 1. Learn, do not copy: Use GenAI to aid your learning, but never copy-paste any GenAI outputs into your own assessed work. Doing so constitutes academic misconduct. 2. Ask if uncertain: Always consult your Course Organiser if you are unclear about the use of GenAI in your assessed work. Some assessed work may encourage GenAI use, while others may impose restrictions. 3. Credit use of tools: Before handing in your assessed work, make sure you acknowledge the use of GenAI, where used. 4. Protect personal data: Avoid uploading personal data - yours or anyone else s - to a GenAI platform, unless you are using the University s secure platform, ELM, and complying with the University s data protection policy. 5. Respect copyrights: Never upload copyrighted materials to a GenAI platform without authorization from the copyright owner. If you are using the University s secure platform, ELM, ensure you have the right to use the material for that purpose. 6. Verify facts: Always check GenAI output for factual accuracy, including references and citations. 7. Diversify sources: Never rely solely on GenAI; it should supplement, but not replace, traditional sources. University s data protection policy. Acceptable uses of GenAI GenAI and reasonable adjustments Please note, guidance on acceptable uses of GenAI does not preclude the use of AI tools where they are being used in the context of a reasonable adjustment. Disabled students should register with the Disability and Learning Support Service in order to put in place a Schedule of Adjustments (a list of modifications to how you experience your teaching, learning and research). Disability and Learning Support Service Before using GenAI in any assessed work, please check whether there are any restrictions. This should be mentioned in the assessment task. If not, then ask your Course Organiser. Some assessments may explicitly ask you to work with AI tools and to analyse and critique the content it generates. Other assessments may specify, for good reason, that AI tools should not be used in particular ways. Please also make sure that you follow the University s Guidance on Proofreading of Students Assessments when using GenAI for proofreading, editing and or translation. You will never be asked to pay to use external GenAI tools. Some of the positive, and generally acceptable, ways GenAI might be used include: Brainstorming ideas through prompts Getting explanations of difficult ideas, questions and concepts Self-tutoring through conversation with the GenAI tool Creating practice questions and self-tests Organising and summarising your notes Planning and structuring your writing Summarising a text, article or book (Check first that the copyright owner permits use of GenAI for this purpose) Helping to improve your grammar, spelling, and writing (Check for restrictions where use of language ",
        "text_length": 11710
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 24.0,
          "frequency": 24,
          "keywords": [
            "plagiarism",
            "cheating",
            "academic",
            "misconduct",
            "citation"
          ],
          "matches": [
            "academic misconduct",
            "academic misconduct",
            "academic misconduct"
          ],
          "confidence": 100
        },
        {
          "name": "Privacy and Data",
          "score": 18.0,
          "frequency": 18,
          "keywords": [
            "private",
            "data",
            "personal",
            "protection",
            "security"
          ],
          "matches": [
            "personal data",
            "personal data",
            "data protection"
          ],
          "confidence": 100
        },
        {
          "name": "AI Technology",
          "score": 9.0,
          "frequency": 9,
          "keywords": [],
          "matches": [],
          "confidence": 90,
          "entities": [
            "Generative AI",
            "the University s AI",
            "GenAI"
          ]
        },
        {
          "name": "Assessment and Evaluation",
          "score": 6.0,
          "frequency": 6,
          "keywords": [
            "assessment",
            "evaluation"
          ],
          "matches": [],
          "confidence": 60
        },
        {
          "name": "Student Guidelines",
          "score": 6.0,
          "frequency": 6,
          "keywords": [
            "student",
            "students",
            "assignment",
            "report"
          ],
          "matches": [],
          "confidence": 60
        },
        {
          "name": "Faculty Guidelines",
          "score": 3.5,
          "frequency": 3,
          "keywords": [
            "staff",
            "teaching",
            "course"
          ],
          "matches": [],
          "confidence": 35
        },
        {
          "name": "Research and Innovation",
          "score": 2.5,
          "frequency": 2,
          "keywords": [
            "research",
            "innovation",
            "analysis"
          ],
          "matches": [],
          "confidence": 25
        },
        {
          "name": "AI Ethics",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "ethics",
            "responsible",
            "benefit",
            "respect"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Transparency",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "transparent",
            "open"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Restrictive",
        "confidence": 54,
        "method": "hybrid",
        "scores": {
          "Restrictive": 14.218749738418419,
          "Moderate": 10.40001895964998,
          "Permissive": 10.701231301931605
        },
        "reasoning": "Classified as Restrictive based on: - 'academic misconduct' appears 6 time(s) - 'limited' appears 2 time(s) - 'required' appears 2 time(s) - Penalty/consequence language present",
        "details": {
          "rule_based": {
            "classification": "Restrictive",
            "confidence": 38,
            "scores": {
              "Restrictive": 20.200000000000003,
              "Moderate": 15.0,
              "Permissive": 17.0
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "limited",
                  2,
                  3.0
                ],
                [
                  "academic misconduct",
                  6,
                  12.0
                ],
                [
                  "required",
                  2,
                  2.6
                ],
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "essential",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "reasonable",
                  2,
                  3.0
                ],
                [
                  "guidelines",
                  1,
                  1.2
                ],
                [
                  "requirements",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  1,
                  1.3
                ],
                [
                  "respect",
                  1,
                  1.3
                ],
                [
                  "context",
                  2,
                  2.8
                ],
                [
                  "particular",
                  3,
                  4.199999999999999
                ]
              ],
              "Permissive": [
                [
                  "support",
                  3,
                  5.4
                ],
                [
                  "benefits",
                  1,
                  1.2
                ],
                [
                  "improve",
                  1,
                  1.2
                ],
                [
                  "innovation",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  5,
                  5.0
                ],
                [
                  "support",
                  3,
                  3.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Restrictive",
            "confidence": 52,
            "scores": {
              "Moderate": 0.3500047399124949,
              "Permissive": 0.12530782548290156,
              "Restrictive": 0.5246874346046043
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 10,
        "top_theme": "Academic Integrity",
        "classification_type": "Restrictive",
        "confidence": 54
      },
      "analysis_id": "analysis_3"
    },
    {
      "_id": "analysis_4",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_180434_2_20250614_220705_cambridge-ai-policy.pdf",
      "analysis_date": "2025-06-26T17:04:37.340633",
      "text_data": {
        "original_text": "Whether it\u2019s ChatGPT or DALL\u00b7E, generative AI tools are changing the communications sector fast.  As a team of writers, designers and strategists who communicate the work of the University of Cambridge to the world, we\u2019re interested in how these tools could support our efforts to share ground-breaking research or create campaigns to attract students. From speeding up tasks like transcribing audio to sparking ideas for news features, we can see lots of opportunities for how these tools can support our work.  But it\u2019s also imperative that we acknowledge the risks when using generative AI tools and understand how to utilise them properly. Upholding factual accuracy, observing ethical usage and investing in our skills are all imperative and especially relevant as the University is built on over 800 years of knowledge.  Through research and workshops, we have created these new guidelines to support our teams in using generative AI tools safely, ethically and effectively.   The guidelines can be read alongside the Russell Group principles on the use of generative AI tools in education, which include that universities will support students and staff to become AI-literate, universities will ensure academic rigour and integrity is upheld, and universities will work collaboratively to share best practice as the technology and its application in education evolves.     Using AI text generators like ChatGPT, Bard and LaMDA  We may use text generator tools to help us research a topic  Generative AI tools like ChatGPT are capable of processing vast amounts of information to quickly produce an easy-to-understand summary of a complex topic. This can help us work faster and understand new ideas.  For example, if a research communications manager is writing a feature on the discovery of DNA, they could ask ChatGPT to summarise the key moments in 500 words and use the answer generated as a starting point for whom to interview or which journal articles to read next. We may use these tools in a similar way to how we use search engines for researching topics and will always carefully fact-check before publication.   We may use text generators to spark inspiration  The ability of generative AI tools to analyse huge datasets can also be used to help spark creative inspiration. This can help us if we\u2019re struggling for time or battling writer\u2019s block.  \nFor example, if a social media manager is looking for ideas on how to engage alumni on Instagram, they could ask ChatGPT for suggestions based on recent popular content. They could then pick the best ideas from ChatGPT\u2019s response and adapt them. We may use these tools in a similar way to how we ask a colleague for an idea on how to approach a creative task.  We do not publish any content that has been written 100% by text generators  We will be critical and responsible users of generative AI tools \u2013 and that means not publishing anything written completely by ChatGPT and being aware of the many reasons why we shouldn\u2019t.  First, our experience with these tools so far is that the default written style and tone of content produced by generative AI is not appropriate for our audiences in its unedited form. We always need to apply the University\u2019s brand and tone of voice guidelines to all content.  Second, generative AI tools do not produce neutral answers because the information sources they are drawing from have all been created by humans and contain our biases and stereotypes. These tools also often create content that contains errors and \u2018hallucinations\u2019. As a seat of learning, it is imperative that the University only publishes unbiased and factual written content.  Thirdly, the risk of plagiarism is high with lifting something completely from an AI content generator, and these tools are often opaque about their original sources and who owns the output. It is essential that our outputs are original and do not plagiarise.  For all of these reasons, we will not publish any press releases, articles, social media posts, blog posts, internal emails or other written content that is 100% produced by generative AI. We will always apply brand guidelines, fact-check responses, and re-write in our own words. The one exception to this is if we are publishing something about AI and would like to demonstrate what AI can do, and we will always make that clear to audiences.     Using AI photo editor tools and image generator tools like DALL\u00b7E and Midjourney  We may use image tools to correct and make minor edits  Image editing with AI tools can speed up work for several teams. We may use these tools to make minor changes to a photo to make it more usable without changing the subject matter or original essence.  For example, if a website manager needs a photo in a landscape ratio but only has one in a portrait ratio, they could use Photoshop\u2019s inbuilt AI tools to extend the background of the photo to create an image with the correct dimensions for the website. \n We will use image editing tools ethical",
        "cleaned_text": "Whether it is ChatGPT or DALL E, generative AI tools are changing the communications sector fast. As a team of writers, designers and strategists who communicate the work of the University of Cambridge to the world, we are interested in how these tools could support our efforts to share ground-breaking research or create campaigns to attract students. From speeding up tasks like transcribing audio to sparking ideas for news features, we can see lots of opportunities for how these tools can support our work. But it is also imperative that we acknowledge the risks when using generative AI tools and understand how to utilise them properly. Upholding factual accuracy, observing ethical usage and investing in our skills are all imperative and especially relevant as the University is built on over 800 years of knowledge. Through research and workshops, we have created these new guidelines to support our teams in using generative AI tools safely, ethically and effectively. The guidelines can be read alongside the Russell Group principles on the use of generative AI tools in education, which include that universities will support students and staff to become AI-literate, universities will ensure academic rigour and integrity is upheld, and universities will work collaboratively to share best practice as the technology and its application in education evolves. Using AI text generators like ChatGPT, Bard and LaMDA We may use text generator tools to help us research a topic Generative AI tools like ChatGPT are capable of processing vast amounts of information to quickly produce an easy-to-understand summary of a complex topic. This can help us work faster and understand new ideas. For example, if a research communications manager is writing a feature on the discovery of DNA, they could ask ChatGPT to summarise the key moments in 500 words and use the answer generated as a starting point for whom to interview or which journal articles to read next. We may use these tools in a similar way to how we use search engines for researching topics and will always carefully fact-check before publication. We may use text generators to spark inspiration The ability of generative AI tools to analyse huge datasets can also be used to help spark creative inspiration. This can help us if we are struggling for time or battling writer s block. For example, if a social media manager is looking for ideas on how to engage alumni on Instagram, they could ask ChatGPT for suggestions based on recent popular content. They could then pick the best ideas from ChatGPT s response and adapt them. We may use these tools in a similar way to how we ask a colleague for an idea on how to approach a creative task. We do not publish any content that has been written 100 by text generators We will be critical and responsible users of generative AI tools and that means not publishing anything written completely by ChatGPT and being aware of the many reasons why we should not. First, our experience with these tools so far is that the default written style and tone of content produced by generative AI is not appropriate for our audiences in its unedited form. We always need to apply the University s brand and tone of voice guidelines to all content. Second, generative AI tools do not produce neutral answers because the information sources they are drawing from have all been created by humans and contain our biases and stereotypes. These tools also often create content that contains errors and hallucinations. As a seat of learning, it is imperative that the University only publishes unbiased and factual written content. Thirdly, the risk of plagiarism is high with lifting something completely from an AI content generator, and these tools are often opaque about their original sources and who owns the output. It is essential that our outputs are original and do not plagiarise. For all of these reasons, we will not publish any press releases, articles, social media posts, blog posts, internal emails or other written content that is 100 produced by generative AI. We will always apply brand guidelines, fact-check responses, and re-write in our own words. The one exception to this is if we are publishing something about AI and would like to demonstrate what AI can do, and we will always make that clear to audiences. Using AI photo editor tools and image generator tools like DALL E and Midjourney We may use image tools to correct and make minor edits Image editing with AI tools can speed up work for several teams. We may use these tools to make minor changes to a photo to make it more usable without changing the subject matter or original essence. For example, if a website manager needs a photo in a landscape ratio but only has one in a portrait ratio, they could use Photoshop s inbuilt AI tools to extend the background of the photo to create an image with the correct dimensions for the website. We will use image editing tools ethically We do not use any image editin",
        "text_length": 9283
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 7.5,
          "frequency": 7,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "consent"
          ],
          "matches": [],
          "confidence": 75
        },
        {
          "name": "Research and Innovation",
          "score": 2.5,
          "frequency": 2,
          "keywords": [
            "research",
            "discovery"
          ],
          "matches": [],
          "confidence": 25
        },
        {
          "name": "Academic Integrity",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "plagiarism",
            "academic",
            "integrity"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Ethics",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "ethical",
            "responsible"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "Student Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "students"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "Transparency",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "transparency",
            "transparent"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "AI Technology",
          "score": 1.0,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 10,
          "entities": [
            "AI",
            "the University of Cambridge s Office of External Affairs and Communications"
          ]
        },
        {
          "name": "Accountability",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 5
        },
        {
          "name": "Faculty Guidelines",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "staff"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 34,
        "method": "hybrid",
        "scores": {
          "Restrictive": 5.234996312168429,
          "Moderate": 13.828121547550868,
          "Permissive": 14.996882140280702
        },
        "reasoning": "Classified as Permissive based on: - 'support' appears 5 time(s) - 'support' appears 5 time(s) - 'creative' appears 3 time(s)",
        "details": {
          "rule_based": {
            "classification": "Permissive",
            "confidence": 46,
            "scores": {
              "Restrictive": 6.1,
              "Moderate": 20.5,
              "Permissive": 23.5
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "cannot",
                  1,
                  2.0
                ],
                [
                  "restricted",
                  1,
                  1.5
                ],
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "essential",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  2,
                  3.0
                ],
                [
                  "guidelines",
                  7,
                  8.4
                ],
                [
                  "principles",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  1,
                  1.3
                ],
                [
                  "ethical",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  1,
                  1.3
                ],
                [
                  "transparency",
                  1,
                  1.3
                ],
                [
                  "situation",
                  1,
                  1.4
                ]
              ],
              "Permissive": [
                [
                  "support",
                  5,
                  9.0
                ],
                [
                  "foster",
                  1,
                  1.8
                ],
                [
                  "creative",
                  3,
                  4.5
                ],
                [
                  "opportunities",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  1,
                  1.0
                ],
                [
                  "suggestions",
                  1,
                  1.0
                ],
                [
                  "support",
                  5,
                  5.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Restrictive",
            "confidence": 39,
            "scores": {
              "Moderate": 0.38203038688771707,
              "Permissive": 0.2242205350701757,
              "Restrictive": 0.3937490780421071
            },
            "method": "ml_based"
          },
          "agreement": false
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Permissive",
        "confidence": 34
      },
      "analysis_id": "analysis_4"
    },
    {
      "_id": "analysis_5",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_180435_2_20250614_220705_chicago-ai-policy.docx",
      "analysis_date": "2025-06-26T17:04:38.792893",
      "text_data": {
        "original_text": "Generative AI tools offer many capabilities and efficiencies that can greatly enhance our work. When using these tools, members of the University community must consider issues related to information security, privacy, compliance, and academic integrity.\nView guidance on using and procuring generative AI tools such as PhoenixAI, OpenAI\u2019s ChatGPT, Microsoft Copilot, and Google\u2019s Gemini.\n\u00a0\nGUIDELINES ON USING AND PROCURING GENERATIVE AI TOOLS\n1. PROTECTION OF UNIVERSITY DATA\nThe use of confidential data with publicly available generative AI tools is prohibited without prior security and privacy review. This includes personally identifiable employee data, FERPA-covered student data, HIPAA-covered patient data, and may include research that is not yet publicly available. Some grantors,\u00a0including the National Institutes of Health, have policies prohibiting the use of generative AI tools in analyzing or reviewing grant applications or proposals. Information shared with publicly available generative AI tools may expose sensitive information to unauthorized parties or violate data use agreements. (Please see\u00a0Policy 601\u00a0or definitions of confidential data and its use for more information.)\n\n\n\n2. RESPONSIBILITY FOR CONTENT ACCURACY AND OWNERSHIP\nAI-generated content may be misleading or inaccurate. Generative AI technology may create citations to content that does not exist. Responses from generative AI tools may contain content and materials from other authors and may be copyrighted. It is the responsibility of the tool user to review the accuracy and ownership of any AI-generated content.\n\n\n\n3. ACADEMIC INTEGRITY\nFor guidance on how generative AI tools intersect with academic honesty, it is recommended that instructors contact the\u00a0Chicago Center for Teaching and Learning. (See\u00a0Academic Honesty & Plagiarism\u00a0in the Student Manual for University policy.)\n\n\n\n4.\u00a0PROCURING AND ACQUIRING GENERATIVE AI TOOLS\nGenerative AI systems, applications, and software products that process, analyze, or move\u00a0confidential data\u00a0require a security review before they are acquired, even if the software is free. This review will help ensure the security and privacy of University data.\u00a0\nPlease contact IT Services by submitting our\u00a0Generative AI Tool Review form\u00a0before acquiring or using any tools, add-ons, or modules that include generative AI technology with University confidential data, even if they are free. For more information, see the\u00a0Policy on the Use of External Services\u00a0and the\u00a0Policy of Procurement and Engagement.\nCONTACTS\nIf you have questions about the guidelines, please contact:\nKevin Boyd, Chief Information Officer, at\u00a0cio@uchicago.edu\nMatt Morton, Chief Information Security Officer, at\u00a0ciso@uchicago.edu\n\n",
        "cleaned_text": "Generative AI tools offer many capabilities and efficiencies that can greatly enhance our work. When using these tools, members of the University community must consider issues related to information security, privacy, compliance, and academic integrity. View guidance on using and procuring generative AI tools such as PhoenixAI, OpenAI s ChatGPT, Microsoft Copilot, and Google s Gemini. GUIDELINES ON USING AND PROCURING GENERATIVE AI TOOLS 1. PROTECTION OF UNIVERSITY DATA The use of confidential data with publicly available generative AI tools is prohibited without prior security and privacy review. This includes personally identifiable employee data, FERPA-covered student data, HIPAA-covered patient data, and may include research that is not yet publicly available. Some grantors, including the National Institutes of Health, have policies prohibiting the use of generative AI tools in analyzing or reviewing grant applications or proposals. Information shared with publicly available generative AI tools may expose sensitive information to unauthorized parties or violate data use agreements. (Please see Policy 601 or definitions of confidential data and its use for more information. ) 2. RESPONSIBILITY FOR CONTENT ACCURACY AND OWNERSHIP AI-generated content may be misleading or inaccurate. Generative AI technology may create citations to content that does not exist. Responses from generative AI tools may contain content and materials from other authors and may be copyrighted. It is the responsibility of the tool user to review the accuracy and ownership of any AI-generated content. 3. ACADEMIC INTEGRITY For guidance on how generative AI tools intersect with academic honesty, it is recommended that instructors contact the Chicago Center for Teaching and Learning. (See Academic Honesty Plagiarism in the Student Manual for University policy. ) 4. PROCURING AND ACQUIRING GENERATIVE AI TOOLS Generative AI systems, applications, and software products that process, analyze, or move confidential data require a security review before they are acquired, even if the software is free. This review will help ensure the security and privacy of University data. Please contact IT Services by submitting our Generative AI Tool Review form before acquiring or using any tools, add-ons, or modules that include generative AI technology with University confidential data, even if they are free. For more information, see the Policy on the Use of External Services and the Policy of Procurement and Engagement. CONTACTS If you have questions about the guidelines, please contact: Kevin Boyd, Chief Information Officer, at cio uchicago. edu Matt Morton, Chief Information Security Officer, at ciso uchicago. edu",
        "text_length": 2722
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 15.0,
          "frequency": 15,
          "keywords": [
            "privacy",
            "data",
            "confidential",
            "protection",
            "security"
          ],
          "matches": [],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 7.5,
          "frequency": 7,
          "keywords": [
            "plagiarism",
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "ACADEMIC INTEGRITY"
          ],
          "confidence": 75
        },
        {
          "name": "Assessment and Evaluation",
          "score": 2.5,
          "frequency": 2,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 25
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsibility"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Student Guidelines",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "student"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "AI Technology",
          "score": 1.0,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 10,
          "entities": [
            "OWNERSHIP AI-generated",
            "AI"
          ]
        },
        {
          "name": "Faculty Guidelines",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "teaching"
          ],
          "matches": [],
          "confidence": 5
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 45,
        "method": "hybrid",
        "scores": {
          "Restrictive": 4.244920282347031,
          "Moderate": 4.503106857734661,
          "Permissive": 2.931972859918311
        },
        "reasoning": "Classified as Moderate based on: - 'integrity' appears 2 time(s) - 'guidelines' appears 2 time(s) - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 39,
            "scores": {
              "Restrictive": 4.6,
              "Moderate": 5.0,
              "Permissive": 3.2
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "prohibited",
                  1,
                  2.0
                ],
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "guidelines",
                  2,
                  2.4
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "enhance",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  2,
                  2.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 37,
            "scores": {
              "Moderate": 0.37577671443366517,
              "Permissive": 0.2529932149795777,
              "Restrictive": 0.3712300705867578
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 8,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 45
      },
      "analysis_id": "analysis_5"
    },
    {
      "_id": "analysis_6",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_180435_2_20250614_220705_columbia-ai-policy.pdf",
      "analysis_date": "2025-06-26T17:04:39.492311",
      "text_data": {
        "original_text": "Generative AI Policy Please note that this policy is a \u201cwork in progress\u201d as the technology, the law and the Columbia community usage evolves. PURPOSE Columbia University is dedicated to advancing knowledge and learning, and embraces generative AI tools. The landscape of Generative AI is rapidly changing, and will change the way we teach, learn, and work. We encourage you to explore and experiment with these tools. The Office of the Provost has convened a working group of faculty and senior administrators from various parts of the University to develop policies and guidelines around the responsible use of these Generative AI tools (the \u201dAI Team\u201d). We ask that you review this guidance on the responsible use of generative AI in your work and study at Columbia University. Based on our collective experience with Generative AI use at the University, we anticipate that this guidance will evolve and be updated regularly. Generative AI (or \u201cAI\u201d) tools such as OpenAI\u2019s ChatGPT, Google\u2019s Bard, Stability AI\u2019s Stable Diffusion, and others, have captured the public\u2019s imagination as these tools become widely available for everyday use. Generative AI tools have the capacity to expedite existing processes and make possible new ones. These tools also have the potential to foster student learning and advance many aspects of research and health care delivery. While the University supports the responsible use of AI, these novel tools have notable limitations and present new risks that must be taken into consideration when using these technologies. Two key attributes of these tools are the risk that an input could potentially become public, and the risk that the output may be biased, misleading, or inaccurate. There are risks related to information security, data privacy, copyright, and academic integrity and bias, for example: \u2022 if Generative AI is given access to personal information, the technology may not respect the privacy rights of individuals, including in a manner that may be required for compliance with applicable data protection laws; \u2022 if Generative AI is given access to confidential information or trade secrets, the University may lose its intellectual property (IP) rights to that information and the information may be disclosed to unauthorized \nthird parties through their independent use of the Generative AI technology; \u2022 Generative AI outputs may violate the intellectual property rights of others, and might not themselves be protected by intellectual property laws; \u2022 Generative AI outputs might be factually inaccurate, and we might be exposed to liability if we rely on those outputs without properly reviewing them; and \u2022 Generative AI may produce decisions that are biased, discriminatory, or otherwise inconsistent with our policies, or that are otherwise in violation of applicable law.  In this initial policy, Columbia University requires that any use of Generative AI be in a manner reflective of its inherent limitations and to avoid these limitations and other emerging risks to the University, its faculty, researchers, students and staff and other stakeholders. Because AI is a rapidly evolving technology, the University will continue to monitor developments and will consider responses from the University community. This initial policy contains overarching guidelines that apply to all in the Columbia community while pursuing their Columbia activities.  After these general requirements, the policy includes specific guidelines related to instruction (for both faculty and students) and research.   SCOPE This Generative AI policy (\u201cPolicy\u201d) governs the use of Generative AI tools by staff, faculty, students, and researchers (the \u201cColumbia community\u201d) in the performance of their functions for or on behalf of Columbia. Because this Policy may be updated from time to time, Columbia community members are encouraged to regularly review the most recent version of this Policy. Constructive comments from Columbia community members may be submitted here: AIpolicy@columbia.edu   DEFINITIONS \u2022 \u201cConfidential Information\u201d means any business or technical information or research result belonging to Columbia, a Columbia community member, collaborators or other third parties, that is not publicly known or that has been provided or received under an obligation to maintain the information as confidential. Please note this \nincludes Protected Health Information or PHI. (see also the Information Security Charter)  \u2022 \u201cGenerative AI\u201d includes any machine-based tool designed to consider user questions, prompts, and other inputs (e.g., text, images, videos) to generate a human-like output (e.g., a response to a question, a written document, software code, or a product design). Generative AI includes both standalone offerings such as ChatGPT, Bard, Stable Diffusion, and offerings that are embedded in other software, such as Github\u2019s Copilot. \u2022 \u201cPersonal Information\u201d means any information that, whether alone or in combination with other availab",
        "cleaned_text": "Generative AI Policy Please note that this policy is a work in progress as the technology, the law and the Columbia community usage evolves. PURPOSE Columbia University is dedicated to advancing knowledge and learning, and embraces generative AI tools. The landscape of Generative AI is rapidly changing, and will change the way we teach, learn, and work. We encourage you to explore and experiment with these tools. The Office of the Provost has convened a working group of faculty and senior administrators from various parts of the University to develop policies and guidelines around the responsible use of these Generative AI tools (the AI Team ). We ask that you review this guidance on the responsible use of generative AI in your work and study at Columbia University. Based on our collective experience with Generative AI use at the University, we anticipate that this guidance will evolve and be updated regularly. Generative AI (or AI ) tools such as OpenAI s ChatGPT, Google s Bard, Stability AI s Stable Diffusion, and others, have captured the public s imagination as these tools become widely available for everyday use. Generative AI tools have the capacity to expedite existing processes and make possible new ones. These tools also have the potential to foster student learning and advance many aspects of research and health care delivery. While the University supports the responsible use of AI, these novel tools have notable limitations and present new risks that must be taken into consideration when using these technologies. Two key attributes of these tools are the risk that an input could potentially become public, and the risk that the output may be biased, misleading, or inaccurate. There are risks related to information security, data privacy, copyright, and academic integrity and bias, for example: if Generative AI is given access to personal information, the technology may not respect the privacy rights of individuals, including in a manner that may be required for compliance with applicable data protection laws; if Generative AI is given access to confidential information or trade secrets, the University may lose its intellectual property (IP) rights to that information and the information may be disclosed to unauthorized third parties through their independent use of the Generative AI technology; Generative AI outputs may violate the intellectual property rights of others, and might not themselves be protected by intellectual property laws; Generative AI outputs might be factually inaccurate, and we might be exposed to liability if we rely on those outputs without properly reviewing them; and Generative AI may produce decisions that are biased, discriminatory, or otherwise inconsistent with our policies, or that are otherwise in violation of applicable law. In this initial policy, Columbia University requires that any use of Generative AI be in a manner reflective of its inherent limitations and to avoid these limitations and other emerging risks to the University, its faculty, researchers, students and staff and other stakeholders. Because AI is a rapidly evolving technology, the University will continue to monitor developments and will consider responses from the University community. This initial policy contains overarching guidelines that apply to all in the Columbia community while pursuing their Columbia activities. After these general requirements, the policy includes specific guidelines related to instruction (for both faculty and students) and research. SCOPE This Generative AI policy ( Policy ) governs the use of Generative AI tools by staff, faculty, students, and researchers (the Columbia community ) in the performance of their functions for or on behalf of Columbia. Because this Policy may be updated from time to time, Columbia community members are encouraged to regularly review the most recent version of this Policy. Constructive comments from Columbia community members may be submitted here: AIpolicy columbia. edu DEFINITIONS Confidential Information means any business or technical information or research result belonging to Columbia, a Columbia community member, collaborators or other third parties, that is not publicly known or that has been provided or received under an obligation to maintain the information as confidential. Please note this includes Protected Health Information or PHI. (see also the Information Security Charter) Generative AI includes any machine-based tool designed to consider user questions, prompts, and other inputs (e. g. , text, images, videos) to generate a human-like output (e. g. , a response to a question, a written document, software code, or a product design). Generative AI includes both standalone offerings such as ChatGPT, Bard, Stable Diffusion, and offerings that are embedded in other software, such as Github s Copilot. Personal Information means any information that, whether alone or in combination with other available information, identifies, rel",
        "text_length": 18715
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 43.5,
          "frequency": 43,
          "keywords": [
            "privacy",
            "data",
            "personal",
            "confidential",
            "protection"
          ],
          "matches": [
            "data privacy",
            "privacy rights",
            "data protection"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 35.0,
          "frequency": 35,
          "keywords": [
            "plagiarism",
            "academic",
            "integrity",
            "dishonesty",
            "misconduct"
          ],
          "matches": [
            "academic integrity",
            "ACADEMIC INTEGRITY",
            "intellectual honesty"
          ],
          "confidence": 100
        },
        {
          "name": "AI Technology",
          "score": 16.0,
          "frequency": 16,
          "keywords": [],
          "matches": [],
          "confidence": 100,
          "entities": [
            "Generative AI",
            "the AI Team",
            "Generative AI"
          ]
        },
        {
          "name": "Faculty Guidelines",
          "score": 15.0,
          "frequency": 15,
          "keywords": [
            "faculty",
            "instructor",
            "staff",
            "teaching",
            "course"
          ],
          "matches": [],
          "confidence": 100
        },
        {
          "name": "Research and Innovation",
          "score": 14.5,
          "frequency": 14,
          "keywords": [
            "research",
            "study",
            "experiment",
            "analysis"
          ],
          "matches": [
            "research integrity"
          ],
          "confidence": 100
        },
        {
          "name": "Student Guidelines",
          "score": 10.0,
          "frequency": 10,
          "keywords": [
            "student",
            "students",
            "assignment",
            "report"
          ],
          "matches": [],
          "confidence": 100
        },
        {
          "name": "AI Ethics",
          "score": 7.5,
          "frequency": 7,
          "keywords": [
            "responsible",
            "rights",
            "respect"
          ],
          "matches": [],
          "confidence": 75
        },
        {
          "name": "Accountability",
          "score": 4.0,
          "frequency": 4,
          "keywords": [
            "responsibility",
            "responsible",
            "liability",
            "control"
          ],
          "matches": [],
          "confidence": 40
        },
        {
          "name": "Bias and Fairness",
          "score": 3.0,
          "frequency": 3,
          "keywords": [
            "bias",
            "biased"
          ],
          "matches": [],
          "confidence": 30
        },
        {
          "name": "Assessment and Evaluation",
          "score": 2.5,
          "frequency": 2,
          "keywords": [
            "review",
            "grading"
          ],
          "matches": [],
          "confidence": 25
        },
        {
          "name": "Transparency",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "transparent",
            "disclosure"
          ],
          "matches": [],
          "confidence": 15
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 57,
        "method": "hybrid",
        "scores": {
          "Restrictive": 20.69437037098948,
          "Moderate": 42.45904908656572,
          "Permissive": 26.286580542444806
        },
        "reasoning": "Classified as Moderate based on: - 'integrity' appears 11 time(s) - 'appropriate' appears 8 time(s) - 'responsible' appears 5 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 47,
            "scores": {
              "Restrictive": 32.2,
              "Moderate": 67.5,
              "Permissive": 42.7
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "prohibited",
                  2,
                  4.0
                ],
                [
                  "not allowed",
                  1,
                  2.0
                ],
                [
                  "must not",
                  2,
                  4.0
                ],
                [
                  "violations",
                  1,
                  2.0
                ],
                [
                  "consequences",
                  1,
                  2.0
                ],
                [
                  "required",
                  1,
                  1.3
                ],
                [
                  "must",
                  11,
                  14.3
                ],
                [
                  "compliance",
                  2,
                  2.6
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  8,
                  12.0
                ],
                [
                  "careful",
                  1,
                  1.5
                ],
                [
                  "conditions",
                  2,
                  2.4
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "standards",
                  2,
                  2.4
                ],
                [
                  "requirements",
                  1,
                  1.2
                ],
                [
                  "considerations",
                  5,
                  6.0
                ],
                [
                  "responsible",
                  5,
                  6.5
                ],
                [
                  "integrity",
                  11,
                  14.3
                ],
                [
                  "respect",
                  2,
                  2.6
                ],
                [
                  "individual",
                  3,
                  4.199999999999999
                ],
                [
                  "specific",
                  3,
                  4.199999999999999
                ],
                [
                  "particular",
                  3,
                  4.199999999999999
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  2,
                  3.6
                ],
                [
                  "promote",
                  1,
                  1.8
                ],
                [
                  "support",
                  4,
                  7.2
                ],
                [
                  "foster",
                  1,
                  1.8
                ],
                [
                  "freedom",
                  1,
                  1.5
                ],
                [
                  "benefits",
                  1,
                  1.2
                ],
                [
                  "opportunities",
                  2,
                  2.4
                ],
                [
                  "progress",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  7,
                  7.0
                ],
                [
                  "best practices",
                  1,
                  1.0
                ],
                [
                  "support",
                  4,
                  4.0
                ],
                [
                  "assistance",
                  2,
                  2.0
                ],
                [
                  "resources",
                  8,
                  8.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 48,
            "scores": {
              "Moderate": 0.4897622716414301,
              "Permissive": 0.1666451356112017,
              "Restrictive": 0.3435925927473698
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 11,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 57
      },
      "analysis_id": "analysis_6"
    },
    {
      "_id": "analysis_7",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_180435_2_20250614_220705_cornell-ai-policy.docx",
      "analysis_date": "2025-06-26T17:04:39.697756",
      "text_data": {
        "original_text": "In Spring 2024, with a goal of inspiring other instructors through the sharing of new ideas, methods, and strategies at Cornell, five faculty were recognized for their creative classroom experiences and teaching implementations using \u2013 or creatively precluding use of \u2013 generative AI. Learn about the projects here:\u00a0Teaching Innovation Case Studies: Creative Responses to Generative AI\nSince the release of new generative artificial intelligence (AI) tools, including ChatGPT, we have all been navigating our way through both the landscape of AI in education and its implications for teaching. As we adapt to these quickly evolving tools and observe how students are using them, many of us are still formulating our own values around what this means for our classes.\u00a0\nOur CTI resources aim to provide support on what these tools are and how they work. We'll address common concerns and considerations in the context of AI, such as\u00a0academic integrity,\u00a0accessibility\u00a0and\u00a0ethical uses\u00a0of the technology. We'll also explore practical applications and pedagogical strategies for teaching and\u00a0assignment design\u00a0as you determine what approaches and policies regarding AI are the right fit for your classes.\nCornell University Committee Report on Generative Artificial Intelligence for Education and Pedagogy\nWhat is Generative Artificial Intelligence (AI)?\nHow Will Generative AI Affect Higher Education?\nThe Upside: Possibilities for Generative AI to Benefit Learning Environments\u00a0\nGenerative AI Literacy\nStay Engaged and Informed\nWhat is Generative Artificial Intelligence (AI)?\nGenerative artificial intelligence is a subset of AI that utilizes machine learning models to create new, original content, such as images, text, or music, based on patterns and structures learned from existing data. A prominent model type used by generative AI is the large language model (LLM).\u00a0\nAn LLM, like ChatGPT, is a type of generative AI system that can produce natural language texts based on a given input, such as a prompt, a keyword, or a query. LLMs typically consist of millions or billions of parameters that are \u201ctrained\u201d on massive amounts of text data, such as books, articles, websites, and social media posts, and can perform various tasks, such as answering questions, summarizing texts, writing essays, creating captions, and generating stories. LLMs can also learn from their own outputs and are likely to improve over time.\nIt\u2019s important to note that while LLMs can answer questions and provide explanations, they are not human and thus do not have knowledge or understanding of the material they generate. Rather, LLMs generate new content based on patterns in existing content, and build text by predicting most likely words.\u00a0\nBecause of how LLMs work, it is possible for these tools to generate content, explanations, or answers that are untrue. LLMs may state false facts as true because they do not truly understand the fact and fiction of what they produce. These generated fictions presented as fact are known as \u201challucinations.\"\nback to top\nHow Will Generative AI Affect Higher Education?\nNobody knows the true impact that generative AI will have on higher education. These technologies are rapidly evolving in complexity and type of use. What we do know is that generative AI is opening up a world of possibilities, while also generating significant concerns about academic integrity, ethics, access and bias.\n\nBefore we dig too deep into whether and how to incorporate generative AI into your courses, here are a few general steps you can take as you consider what generative AI means for your classroom:\u00a0\nReflect:\u00a0How do you feel about generative AI? Concerned? Excited? A little of both? What additional information do you need to feel able to make informed decisions about whether or not to incorporate it into your courses?\u00a0\nTry it out:\u00a0Experiment with generative AI platforms relevant to your discipline, like ChatGPT, Gemini (formerly known as Bard), or DALL-E 2. Choose a tool, then ask it to complete an assignment you\u2019d give your students. What are the results? Ask it to revise the assignment, and see how it responds. Can you identify possible areas of concern for academic integrity, or opportunities for student learning?\u00a0\nPredict and inquire: How might students use this technology in your course? Can you ask students how they are currently using generative AI tools? What clarity will students need to distinguish between appropriate and inappropriate uses of these tools? Consider how you might adjust assignments to either incorporate generative AI into your course, or to identify areas where students may lean on the technology, and turn those hot spots into opportunities to encourage deeper and more critical thinking.\u00a0How might you use these tools to assist your teaching? For example, could generative AI help generate practice problems for students?\nLearn more:\u00a0This technology is evolving, and none of us are experts yet. Be open to continuing to learn more and t",
        "cleaned_text": "In Spring 2024, with a goal of inspiring other instructors through the sharing of new ideas, methods, and strategies at Cornell, five faculty were recognized for their creative classroom experiences and teaching implementations using or creatively precluding use of generative AI. Learn about the projects here: Teaching Innovation Case Studies: Creative Responses to Generative AI Since the release of new generative artificial intelligence (AI) tools, including ChatGPT, we have all been navigating our way through both the landscape of AI in education and its implications for teaching. As we adapt to these quickly evolving tools and observe how students are using them, many of us are still formulating our own values around what this means for our classes. Our CTI resources aim to provide support on what these tools are and how they work. We will address common concerns and considerations in the context of AI, such as academic integrity, accessibility and ethical uses of the technology. We will also explore practical applications and pedagogical strategies for teaching and assignment design as you determine what approaches and policies regarding AI are the right fit for your classes. Cornell University Committee Report on Generative Artificial Intelligence for Education and Pedagogy What is Generative Artificial Intelligence (AI)? How Will Generative AI Affect Higher Education? The Upside: Possibilities for Generative AI to Benefit Learning Environments Generative AI Literacy Stay Engaged and Informed What is Generative Artificial Intelligence (AI)? Generative artificial intelligence is a subset of AI that utilizes machine learning models to create new, original content, such as images, text, or music, based on patterns and structures learned from existing data. A prominent model type used by generative AI is the large language model (LLM). An LLM, like ChatGPT, is a type of generative AI system that can produce natural language texts based on a given input, such as a prompt, a keyword, or a query. LLMs typically consist of millions or billions of parameters that are trained on massive amounts of text data, such as books, articles, websites, and social media posts, and can perform various tasks, such as answering questions, summarizing texts, writing essays, creating captions, and generating stories. LLMs can also learn from their own outputs and are likely to improve over time. It is important to note that while LLMs can answer questions and provide explanations, they are not human and thus do not have knowledge or understanding of the material they generate. Rather, LLMs generate new content based on patterns in existing content, and build text by predicting most likely words. Because of how LLMs work, it is possible for these tools to generate content, explanations, or answers that are untrue. LLMs may state false facts as true because they do not truly understand the fact and fiction of what they produce. These generated fictions presented as fact are known as hallucinations. back to top How Will Generative AI Affect Higher Education? Nobody knows the true impact that generative AI will have on higher education. These technologies are rapidly evolving in complexity and type of use. What we do know is that generative AI is opening up a world of possibilities, while also generating significant concerns about academic integrity, ethics, access and bias. Before we dig too deep into whether and how to incorporate generative AI into your courses, here are a few general steps you can take as you consider what generative AI means for your classroom: Reflect: How do you feel about generative AI? Concerned? Excited? A little of both? What additional information do you need to feel able to make informed decisions about whether or not to incorporate it into your courses? Try it out: Experiment with generative AI platforms relevant to your discipline, like ChatGPT, Gemini (formerly known as Bard), or DALL-E 2. Choose a tool, then ask it to complete an assignment you would give your students. What are the results? Ask it to revise the assignment, and see how it responds. Can you identify possible areas of concern for academic integrity, or opportunities for student learning? Predict and inquire: How might students use this technology in your course? Can you ask students how they are currently using generative AI tools? What clarity will students need to distinguish between appropriate and inappropriate uses of these tools? Consider how you might adjust assignments to either incorporate generative AI into your course, or to identify areas where students may lean on the technology, and turn those hot spots into opportunities to encourage deeper and more critical thinking. How might you use these tools to assist your teaching? For example, could generative AI help generate practice problems for students? Learn more: This technology is evolving, and none of us are experts yet. Be open to continuing to learn more and to havin",
        "text_length": 9709
      },
      "themes": [
        {
          "name": "Student Guidelines",
          "score": 14.0,
          "frequency": 14,
          "keywords": [
            "student",
            "students",
            "learner",
            "assignment",
            "report"
          ],
          "matches": [],
          "confidence": 100
        },
        {
          "name": "Faculty Guidelines",
          "score": 12.5,
          "frequency": 12,
          "keywords": [
            "faculty",
            "teacher",
            "teaching",
            "pedagogy",
            "course"
          ],
          "matches": [],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 9.0,
          "frequency": 9,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 90
        },
        {
          "name": "AI Ethics",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "ethics",
            "ethical",
            "responsible",
            "benefit"
          ],
          "matches": [
            "ethical considerations"
          ],
          "confidence": 65
        },
        {
          "name": "AI Technology",
          "score": 4.0,
          "frequency": 4,
          "keywords": [],
          "matches": [],
          "confidence": 40,
          "entities": [
            "Cornell University Committee Report on Generative Artificial Intelligence for Education and Pedagogy",
            "AI",
            "AI"
          ]
        },
        {
          "name": "Research and Innovation",
          "score": 3.0,
          "frequency": 3,
          "keywords": [
            "research",
            "innovation",
            "development",
            "experiment"
          ],
          "matches": [],
          "confidence": 30
        },
        {
          "name": "Privacy and Data",
          "score": 2.5,
          "frequency": 2,
          "keywords": [
            "data",
            "information"
          ],
          "matches": [],
          "confidence": 25
        },
        {
          "name": "Transparency",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "transparent",
            "open",
            "clarity"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "Bias and Fairness",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "bias",
            "diverse"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsibility",
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 51,
        "method": "hybrid",
        "scores": {
          "Restrictive": 6.175736542201026,
          "Moderate": 11.227408829276687,
          "Permissive": 13.95685462852228
        },
        "reasoning": "Classified as Permissive based on: - 'opportunities' appears 4 time(s) - 'support' appears 2 time(s) - 'creative' appears 2 time(s)",
        "details": {
          "rule_based": {
            "classification": "Permissive",
            "confidence": 44,
            "scores": {
              "Restrictive": 7.9,
              "Moderate": 17.2,
              "Permissive": 20.5
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "cannot",
                  2,
                  4.0
                ],
                [
                  "required",
                  1,
                  1.3
                ],
                [
                  "essential",
                  2,
                  2.6
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  2,
                  3.0
                ],
                [
                  "considerations",
                  2,
                  2.4
                ],
                [
                  "responsible",
                  1,
                  1.3
                ],
                [
                  "ethical",
                  4,
                  5.2
                ],
                [
                  "integrity",
                  3,
                  3.9000000000000004
                ],
                [
                  "context",
                  1,
                  1.4
                ]
              ],
              "Permissive": [
                [
                  "support",
                  2,
                  3.6
                ],
                [
                  "judgment",
                  1,
                  1.5
                ],
                [
                  "creative",
                  2,
                  3.0
                ],
                [
                  "opportunities",
                  4,
                  4.8
                ],
                [
                  "improve",
                  2,
                  2.4
                ],
                [
                  "innovation",
                  1,
                  1.2
                ],
                [
                  "support",
                  2,
                  2.0
                ],
                [
                  "resources",
                  2,
                  2.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Permissive",
            "confidence": 41,
            "scores": {
              "Moderate": 0.2268522073191723,
              "Permissive": 0.4142136571305702,
              "Restrictive": 0.3589341355502564
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 10,
        "top_theme": "Student Guidelines",
        "classification_type": "Permissive",
        "confidence": 51
      },
      "analysis_id": "analysis_7"
    },
    {
      "_id": "analysis_8",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_180435_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-26T17:04:39.810897",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_8"
    },
    {
      "_id": "analysis_9",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_180435_2_20250614_220705_imperial-ai-policy.docx",
      "analysis_date": "2025-06-26T17:04:40.148613",
      "text_data": {
        "original_text": "Generative AI Principles\nGenerative AI and Study Guidance Hub\nGenerative AI Principles\nThese principles are intended to provide a starting point for approaches to using generative AI in teaching, learning and assessment at Imperial. Imperial supports the use of the principles to frame and underpin activities university-wide as we as a community explore the use of generative AI, progress and develop policy, and establish guidelines.\nThe principles are presented with careful consideration of, and alignment with, the areas set forth in the university\u2019s\u00a0Learning and Teaching Strategy. These include growing our digital education capabilities and providing opportunities for discovery-based learning. We aim to do this while maintaining a supportive environment that fosters an increasingly diverse student community, \u2018supporting staff and students to turn diverse backgrounds and cultures into an opportunity for mutual learning of different experiences and perspectives.\u2019\nWe intend to develop our approaches to teaching the competencies and skills associated with generative AI literacy, and to continue to define what it means to be a generative AI-literate learner as it relates to students and staff, including applications for lifelong learning.\nWe support the idea that the use and application of generative AI will vary across academic disciplines and recognise the opportunities to learn from each other as we apply these principles. We will work collaboratively to identify, create and provide staff development opportunities related to applications of generative AI for learning, teaching, and assessment.\nThe Principles\nThe principles are underpinned by Imperial\u2019s core Values. At Imperial, we take a proactive approach to generative AI. The\u00a0Imperial Values and Behaviours Framework\u00a0presents the opportunity to approach the use of generative AI thoughtfully and help us to develop sound approaches to \u2018how\u2019 we do things, as a critical part of \u2018what\u2019 we do. We can apply the core Values in the context of generative AI as follows:\nRespect\u00a0- We value each other\u2019s perspectives and encourage an environment where everyone can grow, pursue opportunities, and express their individuality, ensuring generative AI is used to empower, rather than diminish, individual experience, perspective and contributions.\nCollaboration\u00a0- We work together to cultivate inclusive, impactful and community-driven generative AI applications and solutions.\nExcellence\u00a0- We employ generative AI carefully to enhance our impact, making a positive difference and taking personal responsibility for its effective use, maintaining the highest level of quality in our work.\nIntegrity\u00a0- We act in a principled way with generative AI and inspire trust in our actions by using it responsibly, ethically, and transparently, while addressing challenges with honesty and openness.\nInnovation\u00a0- We approach the opportunities that generative AI brings with an open mind, exploring new ideas and continuously adapting to its rapidly evolving potential, within a culture of innovation, as outlined in\u00a0Imperial\u2019s Science for Humanity Strategy.\u00a0\nThe principles, below, are not intended to be exhaustive. They are indicative, providing a foundational outline for key areas of work related to our education remit. \u00a0\nThe Principles - accordion\nCollapse all\n\n\n\nPromoting the critical use of generative AI in teaching, learning and assessment\nTake critical approaches to the selection, adoption and use of these tools, as well as their outputs.\nRecognise the limitations of generative AI tools and understand where use of the tools cannot and should not replace human skills and knowledge in teaching, learning and assessment.\nEncourage all in our community to consider appropriate, effective and responsible uses of generative AI in their own learning and to actively develop a mindset of continually evaluating and reflecting on their use of Gen AI as use cases and technologies change.\nPrioritise our understanding of how AI affects academic integrity standards, recognising that new challenges will arise, and that we will therefore need to adapt policy accordingly.\nAdopting a consistent ethical approach to the use of generative AI\nAdopt transparent processes in evaluating and sharing how generative AI tools are integrated, used, and maintained across the university.\nProactively address equity related to the adoption and use of generative AI tools and remain open to feedback from our community on these issues.\nEngage in regular assessments of the carbon footprint associated with generative AI tools used across the institution and aim to reduce it through sustainable technology practices.\nCommit to regular reviews of generative AI accessibility for all students and staff, ensuring inclusive access to generative AI tools and that any identified gaps are promptly addressed.\nBuilding a proactive research community around the use of generative AI\nDevelop our capacity to undertake research in the use and impact of gen",
        "cleaned_text": "Generative AI Principles Generative AI and Study Guidance Hub Generative AI Principles These principles are intended to provide a starting point for approaches to using generative AI in teaching, learning and assessment at Imperial. Imperial supports the use of the principles to frame and underpin activities university-wide as we as a community explore the use of generative AI, progress and develop policy, and establish guidelines. The principles are presented with careful consideration of, and alignment with, the areas set forth in the university s Learning and Teaching Strategy. These include growing our digital education capabilities and providing opportunities for discovery-based learning. We aim to do this while maintaining a supportive environment that fosters an increasingly diverse student community, supporting staff and students to turn diverse backgrounds and cultures into an opportunity for mutual learning of different experiences and perspectives. We intend to develop our approaches to teaching the competencies and skills associated with generative AI literacy, and to continue to define what it means to be a generative AI-literate learner as it relates to students and staff, including applications for lifelong learning. We support the idea that the use and application of generative AI will vary across academic disciplines and recognise the opportunities to learn from each other as we apply these principles. We will work collaboratively to identify, create and provide staff development opportunities related to applications of generative AI for learning, teaching, and assessment. The Principles The principles are underpinned by Imperial s core Values. At Imperial, we take a proactive approach to generative AI. The Imperial Values and Behaviours Framework presents the opportunity to approach the use of generative AI thoughtfully and help us to develop sound approaches to how we do things, as a critical part of what we do. We can apply the core Values in the context of generative AI as follows: Respect - We value each other s perspectives and encourage an environment where everyone can grow, pursue opportunities, and express their individuality, ensuring generative AI is used to empower, rather than diminish, individual experience, perspective and contributions. Collaboration - We work together to cultivate inclusive, impactful and community-driven generative AI applications and solutions. Excellence - We employ generative AI carefully to enhance our impact, making a positive difference and taking personal responsibility for its effective use, maintaining the highest level of quality in our work. Integrity - We act in a principled way with generative AI and inspire trust in our actions by using it responsibly, ethically, and transparently, while addressing challenges with honesty and openness. Innovation - We approach the opportunities that generative AI brings with an open mind, exploring new ideas and continuously adapting to its rapidly evolving potential, within a culture of innovation, as outlined in Imperial s Science for Humanity Strategy. The principles, below, are not intended to be exhaustive. They are indicative, providing a foundational outline for key areas of work related to our education remit. The Principles - accordion Collapse all Promoting the critical use of generative AI in teaching, learning and assessment Take critical approaches to the selection, adoption and use of these tools, as well as their outputs. Recognise the limitations of generative AI tools and understand where use of the tools cannot and should not replace human skills and knowledge in teaching, learning and assessment. Encourage all in our community to consider appropriate, effective and responsible uses of generative AI in their own learning and to actively develop a mindset of continually evaluating and reflecting on their use of Gen AI as use cases and technologies change. Prioritise our understanding of how AI affects academic integrity standards, recognising that new challenges will arise, and that we will therefore need to adapt policy accordingly. Adopting a consistent ethical approach to the use of generative AI Adopt transparent processes in evaluating and sharing how generative AI tools are integrated, used, and maintained across the university. Proactively address equity related to the adoption and use of generative AI tools and remain open to feedback from our community on these issues. Engage in regular assessments of the carbon footprint associated with generative AI tools used across the institution and aim to reduce it through sustainable technology practices. Commit to regular reviews of generative AI accessibility for all students and staff, ensuring inclusive access to generative AI tools and that any identified gaps are promptly addressed. Building a proactive research community around the use of generative AI Develop our capacity to undertake research in the use and impact of generative AI i",
        "text_length": 16477
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 13.5,
          "frequency": 13,
          "keywords": [
            "privacy",
            "data",
            "personal",
            "confidential",
            "information"
          ],
          "matches": [
            "personal data"
          ],
          "confidence": 100
        },
        {
          "name": "Student Guidelines",
          "score": 11.0,
          "frequency": 11,
          "keywords": [
            "student",
            "students",
            "learner",
            "assignment",
            "essay"
          ],
          "matches": [],
          "confidence": 100
        },
        {
          "name": "Faculty Guidelines",
          "score": 11.0,
          "frequency": 11,
          "keywords": [
            "faculty",
            "teacher",
            "staff",
            "teaching"
          ],
          "matches": [],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 10.0,
          "frequency": 10,
          "keywords": [
            "plagiarism",
            "cheating",
            "academic",
            "integrity",
            "misconduct"
          ],
          "matches": [
            "academic integrity",
            "Academic Misconduct"
          ],
          "confidence": 100
        },
        {
          "name": "Research and Innovation",
          "score": 9.5,
          "frequency": 9,
          "keywords": [
            "research",
            "innovation",
            "development",
            "discovery",
            "study"
          ],
          "matches": [],
          "confidence": 95
        },
        {
          "name": "AI Technology",
          "score": 7.5,
          "frequency": 7,
          "keywords": [],
          "matches": [],
          "confidence": 75,
          "entities": [
            "AI",
            "Gen AI",
            "AI"
          ]
        },
        {
          "name": "Assessment and Evaluation",
          "score": 4.0,
          "frequency": 4,
          "keywords": [
            "assessment",
            "testing",
            "review",
            "examination"
          ],
          "matches": [],
          "confidence": 40
        },
        {
          "name": "Bias and Fairness",
          "score": 2.5,
          "frequency": 2,
          "keywords": [
            "equity",
            "inclusive",
            "diverse"
          ],
          "matches": [],
          "confidence": 25
        },
        {
          "name": "Transparency",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "transparent",
            "open"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "AI Ethics",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "ethical",
            "responsible",
            "respect"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsibility",
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 63,
        "method": "hybrid",
        "scores": {
          "Restrictive": 6.390480999477859,
          "Moderate": 29.419639310431396,
          "Permissive": 25.78987969009073
        },
        "reasoning": "Classified as Moderate based on: - 'principles' appears 14 time(s) - 'specific' appears 3 time(s) - 'particular' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 46,
            "scores": {
              "Restrictive": 9.5,
              "Moderate": 44.999999999999986,
              "Permissive": 41.5
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "cannot",
                  1,
                  2.0
                ],
                [
                  "limited",
                  1,
                  1.5
                ],
                [
                  "disciplinary",
                  1,
                  2.0
                ],
                [
                  "consequences",
                  1,
                  2.0
                ],
                [
                  "academic misconduct",
                  1,
                  2.0
                ]
              ],
              "Moderate": [
                [
                  "reasonable",
                  1,
                  1.5
                ],
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "careful",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  1,
                  1.2
                ],
                [
                  "framework",
                  2,
                  2.4
                ],
                [
                  "principles",
                  14,
                  16.8
                ],
                [
                  "standards",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  1,
                  1.3
                ],
                [
                  "responsibly",
                  2,
                  2.6
                ],
                [
                  "ethical",
                  1,
                  1.3
                ],
                [
                  "integrity",
                  2,
                  2.6
                ],
                [
                  "respect",
                  1,
                  1.3
                ],
                [
                  "context",
                  1,
                  1.4
                ],
                [
                  "individual",
                  1,
                  1.4
                ],
                [
                  "specific",
                  3,
                  4.199999999999999
                ],
                [
                  "particular",
                  2,
                  2.8
                ]
              ],
              "Permissive": [
                [
                  "welcome",
                  1,
                  1.8
                ],
                [
                  "support",
                  5,
                  9.0
                ],
                [
                  "judgment",
                  1,
                  1.5
                ],
                [
                  "advantages",
                  1,
                  1.2
                ],
                [
                  "opportunities",
                  5,
                  6.0
                ],
                [
                  "enhance",
                  1,
                  1.2
                ],
                [
                  "improve",
                  1,
                  1.2
                ],
                [
                  "innovation",
                  2,
                  2.4
                ],
                [
                  "progress",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  8,
                  8.0
                ],
                [
                  "best practices",
                  1,
                  1.0
                ],
                [
                  "advice",
                  1,
                  1.0
                ],
                [
                  "support",
                  5,
                  5.0
                ],
                [
                  "resources",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 60,
            "scores": {
              "Moderate": 0.6049098276078517,
              "Permissive": 0.22246992252268216,
              "Restrictive": 0.17262024986946464
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 11,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 63
      },
      "analysis_id": "analysis_9"
    },
    {
      "_id": "analysis_10",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_180435_2_20250614_220705_mit-ai-policy.pdf",
      "analysis_date": "2025-06-26T17:04:40.382632",
      "text_data": {
        "original_text": "Information Systems and Technology (IS&T) is providing this initial guidance to encourage community members to consider factors including information security, data privacy, regulatory and policy compliance, compliance with confidentiality restrictions concerning third party information and data, intellectual property (e.g., copyright and patent), and academic integrity when choosing to use or purchase software that makes use of generative artificial intelligence (AI). For\tMIT\tfaculty\tand\tinstructors: MIT's Chair of the Faculty has\trecommended the use of these\tresources\ton\tgenerative\tAI\tand\tteaching provided by the Teaching and Learning Laboratory. This guidance does not address all issues for consideration when using generative AI tools as part of MIT research, nor does this guidance focus on when MIT researchers are creating generative AI tools or publicly releasing datasets/information that could become ingestible data for use by MIT researchers or third parties for training generative AI tools. Please contact mitogc@mit.edu for further guidance on research-related applications or development of AI tools. Consult with IS&T before purchasing or using generative AI tools IS&T is working closely with the MIT IT\tGovernance\tCommittee, Information\tTechnology\tPolicy\tCommittee and the Office\tof\tGeneral\tCounsel to determine appropriate terms for vendor agreements relating to generative AI. \u2022 IS&T\trecommends\tMIT\tcommunity\tmembers\tfirst\tconsider\tusing\ttools\tand\tservices\twhich\thave\talready\tbeen\tlicensed\tby\tIS&T\tfor\tuse\tby\tthe\tMIT\tcommunity.\t\u2022 Before\tentering\tinto\tany\tlicense\tagreements\tfor\ta\tnew\tAI\ttool\tor\tservice,\twe\trecommend\treaching\tout\tto\tai-guidance@mit.edu\tfor\ta\tconsultation\ton\tyour\tplanned\tuse\tof\tAI\tand\tan\tassessment\tof\tthe\ttool\tor\tservice\tbefore\tit\tis\tused.\t\u2022 If\tan\tMIT\tDLCI\tis\talready\tusing\ta\tgenerative\tAI\ttool\tor\tservice,\tensure\tthat\tthe\ttool\tcomplies\twith\tall\tInstitute\tpolicies\tand\tInformation\tProtection\tguidelines.\tContact\tai-guidance@mit.edu\tif\tyou\thave\ta\tneed\tfor\ta\tconsultation\tor\tassessment.\t\u2022 Do\tnot\tuse\tgenerative\tAI\tfor\tpurposes\tthat\tmay\trequire\tin-depth\trisk\tassessments\twithout\tcontacting\tai-guidance@mit.edu.\tExamples\tinclude:\trecruitment\tand\thiring\tof\t\nemployees,\tevaluating\tstudent\tacademic\tperformance,\tmaking\tinvestment\tdecisions,\tand\tcomplaint\tand\tdispute\tresolution.\tProtect MIT information and confidential data As with any use of information technology at MIT, ensure that your use of generative AI tools and services complies with all applicable federal and state laws and orders (including, without limitation, FERPA, HIPAA, Massachusetts\tData\tProtection\tStandards, export control laws, and Executive\tOrder\ton\tthe\tSafe,\tSecure,\tand\tTrustworthy\tDevelopment\tand\tUse\tof\tArtificial\tIntelligence), Institute\tpolicies (including 10.1\tAcademic\tand\tResearch\tMisconduct, 11.0\tPrivacy\tand\tDisclosure\tof\tPersonal\tInformation, and 13.0\tInformation\tPolicies), follows all guidelines outlined on Information\tProtection, the Institute's Written Information Security Program (WISP), and complies with any additional policies established by your department, lab, center, or institute (DLCI). Do not enter MIT information or research/administrative data classified\tas\tMedium\tRisk\tor\tHigh\tRisk into publicly available generative AI tools or services not subject to an Institute licensing agreement. Examples of such information and data include non-public research results and data, unpublished research papers, confidential information received from third parties (such as research sponsors and collaborators), unpublished invention disclosures and patent applications, Institute financial and human resources information, personally identifiable information (including, for example, student records, medical records), any information subject to legal or regulatory requirements necessitating its proper safeguarding and handling, and any other information not intended to be freely available to the general public, or to the MIT community without access controls. Ensure accuracy before publishing AI-generated information Be aware that information generated by AI may be inaccurate, incomplete, misleading, biased, fabricated, or may contain material subject to a third party\u2019s intellectual property ownership. You are responsible for the accuracy of any information you publish, including AI-generated content. Be transparent about your use of AI tools \nYou should disclose the use of generative AI tools for all academic, educational, and research-related uses.  Do not publish research results that rely on content generated through the use of a generative AI tool without disclosing the nature of the use of such generative AI tool in producing the content.  \n",
        "cleaned_text": "Information Systems and Technology (IS T) is providing this initial guidance to encourage community members to consider factors including information security, data privacy, regulatory and policy compliance, compliance with confidentiality restrictions concerning third party information and data, intellectual property (e. g. , copyright and patent), and academic integrity when choosing to use or purchase software that makes use of generative artificial intelligence (AI). For MIT faculty and instructors: MIT s Chair of the Faculty has recommended the use of these resources on generative AI and teaching provided by the Teaching and Learning Laboratory. This guidance does not address all issues for consideration when using generative AI tools as part of MIT research, nor does this guidance focus on when MIT researchers are creating generative AI tools or publicly releasing datasets information that could become ingestible data for use by MIT researchers or third parties for training generative AI tools. Please contact mitogc mit. edu for further guidance on research-related applications or development of AI tools. Consult with IS T before purchasing or using generative AI tools IS T is working closely with the MIT IT Governance Committee, Information Technology Policy Committee and the Office of General Counsel to determine appropriate terms for vendor agreements relating to generative AI. IS T recommends MIT community members first consider using tools and services which have already been licensed by IS T for use by the MIT community. Before entering into any license agreements for a new AI tool or service, we recommend reaching out to ai-guidance mit. edu for a consultation on your planned use of AI and an assessment of the tool or service before it is used. If an MIT DLCI is already using a generative AI tool or service, ensure that the tool complies with all Institute policies and Information Protection guidelines. Contact ai-guidance mit. edu if you have a need for a consultation or assessment. Do not use generative AI for purposes that may require in-depth risk assessments without contacting ai-guidance mit. edu. Examples include: recruitment and hiring of employees, evaluating student academic performance, making investment decisions, and complaint and dispute resolution. Protect MIT information and confidential data As with any use of information technology at MIT, ensure that your use of generative AI tools and services complies with all applicable federal and state laws and orders (including, without limitation, FERPA, HIPAA, Massachusetts Data Protection Standards, export control laws, and Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence), Institute policies (including 10. 1 Academic and Research Misconduct, 11. 0 Privacy and Disclosure of Personal Information, and 13. 0 Information Policies), follows all guidelines outlined on Information Protection, the Institute s Written Information Security Program (WISP), and complies with any additional policies established by your department, lab, center, or institute (DLCI). Do not enter MIT information or research administrative data classified as Medium Risk or High Risk into publicly available generative AI tools or services not subject to an Institute licensing agreement. Examples of such information and data include non-public research results and data, unpublished research papers, confidential information received from third parties (such as research sponsors and collaborators), unpublished invention disclosures and patent applications, Institute financial and human resources information, personally identifiable information (including, for example, student records, medical records), any information subject to legal or regulatory requirements necessitating its proper safeguarding and handling, and any other information not intended to be freely available to the general public, or to the MIT community without access controls. Ensure accuracy before publishing AI-generated information Be aware that information generated by AI may be inaccurate, incomplete, misleading, biased, fabricated, or may contain material subject to a third party s intellectual property ownership. You are responsible for the accuracy of any information you publish, including AI-generated content. Be transparent about your use of AI tools You should disclose the use of generative AI tools for all academic, educational, and research-related uses. Do not publish research results that rely on content generated through the use of a generative AI tool without disclosing the nature of the use of such generative AI tool in producing the content.",
        "text_length": 4700
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 24.0,
          "frequency": 24,
          "keywords": [
            "privacy",
            "data",
            "personal",
            "confidential",
            "protection"
          ],
          "matches": [
            "data privacy",
            "Data Protection"
          ],
          "confidence": 100
        },
        {
          "name": "Research and Innovation",
          "score": 5.5,
          "frequency": 5,
          "keywords": [
            "research",
            "development"
          ],
          "matches": [],
          "confidence": 55
        },
        {
          "name": "Academic Integrity",
          "score": 5.0,
          "frequency": 5,
          "keywords": [
            "academic",
            "integrity",
            "misconduct"
          ],
          "matches": [
            "academic integrity"
          ],
          "confidence": 50
        },
        {
          "name": "Faculty Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Accountability",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "responsible",
            "governance",
            "control"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "Transparency",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "transparent",
            "disclosure"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "assessment"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Student Guidelines",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "student"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "AI Ethics",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 5
        },
        {
          "name": "Bias and Fairness",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "biased"
          ],
          "matches": [],
          "confidence": 5
        },
        {
          "name": "AI Technology",
          "score": 0.5,
          "frequency": 0,
          "keywords": [],
          "matches": [],
          "confidence": 5,
          "entities": [
            "Massachusetts Data Protection Standards"
          ]
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 66,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.506491255176923,
          "Moderate": 8.621689605336929,
          "Permissive": 5.891819139486155
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 2 time(s) - 'appropriate' appears 1 time(s) - 'responsible' appears 1 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 46,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 10.100000000000001,
              "Permissive": 9.0
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "compliance",
                  2,
                  2.6
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  2,
                  2.4
                ],
                [
                  "standards",
                  1,
                  1.2
                ],
                [
                  "requirements",
                  1,
                  1.2
                ],
                [
                  "factors",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  1,
                  1.3
                ],
                [
                  "integrity",
                  1,
                  1.3
                ]
              ],
              "Permissive": [
                [
                  "guidance",
                  7,
                  7.0
                ],
                [
                  "resources",
                  2,
                  2.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 64,
            "scores": {
              "Moderate": 0.6404224013342321,
              "Permissive": 0.12295478487153891,
              "Restrictive": 0.23662281379423072
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 11,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 66
      },
      "analysis_id": "analysis_10"
    },
    {
      "_id": "analysis_11",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_180436_2_20250614_220705_oxford-ai-policy.pdf",
      "analysis_date": "2025-06-26T17:04:40.964999",
      "text_data": {
        "original_text": "As AI tools and capabilities develop, this guidance, and our approach to refining it, may adapt. Suggestions and feedback are very welcome to the Head of Digital Campaigns and Communications. AI tools are changing the way we work at Oxford, whether that\u2019s improving the quality of our written communications or helping speed up our research tasks. But it is important that we work together to ensure that their use aligns with our values and our obligations.  Oxford is committed to setting a clear, confident standard for the responsible use of AI in communications. These guidelines outline expectations for the use of generative AI tools by communicators at Oxford. Technology and approaches change rapidly; we will aim to test and review these on a regular basis and update as necessary. Our aim is not to be restrictive, but rather to ensure those using these tools feel confident in the boundaries of what is acceptable in our community. These guidelines were developed with input from Divisional Communications Leads, the AI Competency Centre and Information Security. The guidelines can be read alongside the Russell Group principles on the use of generative AI tools in education, which support the development and use of AI in a way that enables their ethical use and upholds academic rigour and integrity. Scope: context  This guidance has been developed for the use of staff in the Public Affairs Directorate. However it is written for any staff member employed as communications professionals at the University of Oxford or working with communications in their role, and we encourage its adoption across the profession. It may be of use to anyone who wishes to consider the use of GenAI in their communication and content generation. Scope: technology This guidance relates to the use of generative AI (GenAI) tools \u2014 artificial intelligence systems that simulate creativity by predicting \nand assembling outputs based on learned patterns from the data they have been trained on, not human comprehension. Their results are intended to appear plausible but can contain inaccurate information or off topic hallucinations, requiring careful human oversight. This includes tools built on large language models (LLMs), such as ChatGPT, Claude, Gemini or Copilot, as well as those that generate images, audio or video, like Midjourney, Notebook LM or Sora. GenAI is increasingly embedded within the platforms and systems we already use, enhancing functionality and speeding up tasks when used effectively. For this reason, it is important to remember that GenAI and LLMs do not comprehend information in any way akin to humans. Their neural networks can generate outputs that are factually inaccurate and misleading. This guidance applies to both standalone GenAI tools and AI-powered features within other software or platforms. It does not cover all forms of artificial intelligence used across the University \u2013 only tools that generate content. However, some of the broad principles outlined here may be useful in other contexts too. Understanding what AI is \u2013 and what it isn\u2019t \u2013 is critical for its effective use. Here are a few helpful guides: \u2022 PAD\u2019s AI campaign material \u2013 \u2018What is AI?\u2019 \u2022 A quick intro to generative AI from the University of South Australia (5 minutes) \u2022 Google\u2019s Introduction to Generative AI video (22 minutes) \u2022 What is generative AI and how does it work? \u2013 The Turing Lectures with Mirella Lapata (46 minutes) \u2022 Generative AI exists because of the transformer: this is how it works (The FT) Summary Our guidelines can be summarised by the following principles: 1. We prioritise human creativity, curiosity and judgement. \n2. Oxford's reputation stands on the trustworthiness of our research and our communications. We are transparent with each other and with our audiences about our use of AI and ensure that AI is not used to conceal or alter original intent or meaning. 3. As communications professionals, we are responsible for the quality and accuracy of the content we produce. The use of any AI tool should therefore be seen as a supportive mechanism to create value and enable productivity, with the recognition that outputs from generative AI are susceptible to bias, mistakes and misinformation. All AI-assisted outputs must undergo a human review for factual accuracy, appropriate tone and ethical integrity before public release.  4. We work to ensure we have the skills to use the right GenAI tools in the right way, working to understand the context of the tools we\u2019re using and the benefits and risks they offer. This includes using tools appropriately, in line with University guidance, and taking appropriate data and security precautions. Use of AI and text  In general, we may use AI as a supportive tool rather than a way to generate a final product; AI should never be \u2018the author\u2019 of anything we publish. We may use AI tools to help us: \u2022 research a topic, including helping us understand audiences, research papers and trends, or to provide ",
        "cleaned_text": "As AI tools and capabilities develop, this guidance, and our approach to refining it, may adapt. Suggestions and feedback are very welcome to the Head of Digital Campaigns and Communications. AI tools are changing the way we work at Oxford, whether that is improving the quality of our written communications or helping speed up our research tasks. But it is important that we work together to ensure that their use aligns with our values and our obligations. Oxford is committed to setting a clear, confident standard for the responsible use of AI in communications. These guidelines outline expectations for the use of generative AI tools by communicators at Oxford. Technology and approaches change rapidly; we will aim to test and review these on a regular basis and update as necessary. Our aim is not to be restrictive, but rather to ensure those using these tools feel confident in the boundaries of what is acceptable in our community. These guidelines were developed with input from Divisional Communications Leads, the AI Competency Centre and Information Security. The guidelines can be read alongside the Russell Group principles on the use of generative AI tools in education, which support the development and use of AI in a way that enables their ethical use and upholds academic rigour and integrity. Scope: context This guidance has been developed for the use of staff in the Public Affairs Directorate. However it is written for any staff member employed as communications professionals at the University of Oxford or working with communications in their role, and we encourage its adoption across the profession. It may be of use to anyone who wishes to consider the use of GenAI in their communication and content generation. Scope: technology This guidance relates to the use of generative AI (GenAI) tools artificial intelligence systems that simulate creativity by predicting and assembling outputs based on learned patterns from the data they have been trained on, not human comprehension. Their results are intended to appear plausible but can contain inaccurate information or off topic hallucinations, requiring careful human oversight. This includes tools built on large language models (LLMs), such as ChatGPT, Claude, Gemini or Copilot, as well as those that generate images, audio or video, like Midjourney, Notebook LM or Sora. GenAI is increasingly embedded within the platforms and systems we already use, enhancing functionality and speeding up tasks when used effectively. For this reason, it is important to remember that GenAI and LLMs do not comprehend information in any way akin to humans. Their neural networks can generate outputs that are factually inaccurate and misleading. This guidance applies to both standalone GenAI tools and AI-powered features within other software or platforms. It does not cover all forms of artificial intelligence used across the University only tools that generate content. However, some of the broad principles outlined here may be useful in other contexts too. Understanding what AI is and what it is not is critical for its effective use. Here are a few helpful guides: PAD s AI campaign material What is AI? A quick intro to generative AI from the University of South Australia (5 minutes) Google s Introduction to Generative AI video (22 minutes) What is generative AI and how does it work? The Turing Lectures with Mirella Lapata (46 minutes) Generative AI exists because of the transformer: this is how it works (The FT) Summary Our guidelines can be summarised by the following principles: 1. We prioritise human creativity, curiosity and judgement. 2. Oxford s reputation stands on the trustworthiness of our research and our communications. We are transparent with each other and with our audiences about our use of AI and ensure that AI is not used to conceal or alter original intent or meaning. 3. As communications professionals, we are responsible for the quality and accuracy of the content we produce. The use of any AI tool should therefore be seen as a supportive mechanism to create value and enable productivity, with the recognition that outputs from generative AI are susceptible to bias, mistakes and misinformation. All AI-assisted outputs must undergo a human review for factual accuracy, appropriate tone and ethical integrity before public release. 4. We work to ensure we have the skills to use the right GenAI tools in the right way, working to understand the context of the tools we are using and the benefits and risks they offer. This includes using tools appropriately, in line with University guidance, and taking appropriate data and security precautions. Use of AI and text In general, we may use AI as a supportive tool rather than a way to generate a final product; AI should never be the author of anything we publish. We may use AI tools to help us: research a topic, including helping us understand audiences, research papers and trends, or to provide insight and information generat",
        "text_length": 13898
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 18.5,
          "frequency": 18,
          "keywords": [
            "privacy",
            "data",
            "personal",
            "confidential",
            "gdpr"
          ],
          "matches": [
            "data protection"
          ],
          "confidence": 100
        },
        {
          "name": "AI Technology",
          "score": 9.0,
          "frequency": 9,
          "keywords": [],
          "matches": [],
          "confidence": 90,
          "entities": [
            "Digital Campaigns and Communications",
            "the AI Competency Centre and Information Security",
            "the Public Affairs Directorate"
          ]
        },
        {
          "name": "Accountability",
          "score": 6.0,
          "frequency": 6,
          "keywords": [
            "responsible",
            "oversight"
          ],
          "matches": [
            "human oversight",
            "human oversight"
          ],
          "confidence": 60
        },
        {
          "name": "Transparency",
          "score": 5.5,
          "frequency": 5,
          "keywords": [
            "transparency",
            "transparent",
            "open",
            "visible"
          ],
          "matches": [
            "AI transparency"
          ],
          "confidence": 55
        },
        {
          "name": "Research and Innovation",
          "score": 5.5,
          "frequency": 5,
          "keywords": [
            "research",
            "development",
            "analysis"
          ],
          "matches": [],
          "confidence": 55
        },
        {
          "name": "AI Ethics",
          "score": 4.0,
          "frequency": 4,
          "keywords": [
            "ethics",
            "ethical",
            "responsible",
            "harm"
          ],
          "matches": [],
          "confidence": 40
        },
        {
          "name": "Faculty Guidelines",
          "score": 2.5,
          "frequency": 2,
          "keywords": [
            "staff",
            "teaching"
          ],
          "matches": [],
          "confidence": 25
        },
        {
          "name": "Academic Integrity",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "academic",
            "integrity",
            "collaboration"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Bias and Fairness",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "bias",
            "fairness",
            "equity"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "assessment",
            "review"
          ],
          "matches": [],
          "confidence": 15
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 65,
        "method": "hybrid",
        "scores": {
          "Restrictive": 7.191051973998952,
          "Moderate": 31.451309793997773,
          "Permissive": 24.21763823200327
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 7 time(s) - 'appropriate' appears 4 time(s) - 'principles' appears 5 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Restrictive": 10.5,
              "Moderate": 48.39999999999999,
              "Permissive": 39.2
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "not allowed",
                  1,
                  2.0
                ],
                [
                  "cannot",
                  1,
                  2.0
                ],
                [
                  "must",
                  2,
                  2.6
                ],
                [
                  "essential",
                  2,
                  2.6
                ],
                [
                  "necessary",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  4,
                  6.0
                ],
                [
                  "careful",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  7,
                  8.4
                ],
                [
                  "framework",
                  1,
                  1.2
                ],
                [
                  "principles",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "ethical",
                  4,
                  5.2
                ],
                [
                  "integrity",
                  2,
                  2.6
                ],
                [
                  "transparency",
                  2,
                  2.6
                ],
                [
                  "fairness",
                  1,
                  1.3
                ],
                [
                  "context",
                  3,
                  4.199999999999999
                ],
                [
                  "individual",
                  1,
                  1.4
                ],
                [
                  "specific",
                  2,
                  2.8
                ],
                [
                  "particular",
                  1,
                  1.4
                ]
              ],
              "Permissive": [
                [
                  "welcome",
                  1,
                  1.8
                ],
                [
                  "support",
                  4,
                  7.2
                ],
                [
                  "foster",
                  1,
                  1.8
                ],
                [
                  "enable",
                  1,
                  1.8
                ],
                [
                  "benefits",
                  2,
                  2.4
                ],
                [
                  "opportunities",
                  2,
                  2.4
                ],
                [
                  "enhance",
                  1,
                  1.2
                ],
                [
                  "improve",
                  2,
                  2.4
                ],
                [
                  "productivity",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  9,
                  9.0
                ],
                [
                  "suggestions",
                  1,
                  1.0
                ],
                [
                  "support",
                  4,
                  4.0
                ],
                [
                  "resources",
                  3,
                  3.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 60,
            "scores": {
              "Moderate": 0.6028274484994456,
              "Permissive": 0.1744095580008177,
              "Restrictive": 0.22276299349973794
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 10,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 65
      },
      "analysis_id": "analysis_11"
    },
    {
      "_id": "analysis_12",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_180436_2_20250614_220705_stanford-ai-policy.pdf",
      "analysis_date": "2025-06-26T17:04:41.054176",
      "text_data": {
        "original_text": "Generative AI Policy Guidance Main content start Guidance adopted on February 16, 2023    Honor Code Implications of Generative AI Tools The Board on Conduct Affairs (BCA) has been asked to address the Honor Code implications of generative AI tools such as ChatGPT, Bard, DALL-E, and Stable Diffusion. These are novel tools, and both students and instructors have been experimenting with their use in academic settings.   While these tools have applications that foster student learning and understanding, these tools can also be used in ways that bypass key learning objectives. To give sufficient space for instructors to explore uses of generative AI tools in their courses, and to set clear guidelines to students about what uses are and are not consistent with the Stanford Honor Code, the BCA has set forth the following policy guidance regarding generative AI in the context of coursework: Absent a clear statement from a course instructor, use of or consultation with generative AI shall be treated analogously to assistance from another person. In particular, using generative AI tools to substantially complete an assignment or exam (e.g. by entering exam or assignment questions) is not permitted. Students should acknowledge the use of generative AI (other than incidental use) and default to disclosing such assistance when in doubt. \n\nIndividual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools. Course instructors should set such policies in their course syllabi and clearly communicate such policies to students. Students who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification. The BCA will continue to monitor developments in these tools and their use in academic settings and may update this guidance. Members of the community are encouraged to contact the BCA to provide input, suggestions, and comments on this policy. NOTE: As part of the BCA\u2019s guidance on clear communication of a course\u2019s generative AI policy, OCS recommends course instructors provide clear advance notice that they may use detection software to review work submitted for use of generative AI.   \n",
        "cleaned_text": "Generative AI Policy Guidance Main content start Guidance adopted on February 16, 2023 Honor Code Implications of Generative AI Tools The Board on Conduct Affairs (BCA) has been asked to address the Honor Code implications of generative AI tools such as ChatGPT, Bard, DALL-E, and Stable Diffusion. These are novel tools, and both students and instructors have been experimenting with their use in academic settings. While these tools have applications that foster student learning and understanding, these tools can also be used in ways that bypass key learning objectives. To give sufficient space for instructors to explore uses of generative AI tools in their courses, and to set clear guidelines to students about what uses are and are not consistent with the Stanford Honor Code, the BCA has set forth the following policy guidance regarding generative AI in the context of coursework: Absent a clear statement from a course instructor, use of or consultation with generative AI shall be treated analogously to assistance from another person. In particular, using generative AI tools to substantially complete an assignment or exam (e. g. by entering exam or assignment questions) is not permitted. Students should acknowledge the use of generative AI (other than incidental use) and default to disclosing such assistance when in doubt. Individual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools. Course instructors should set such policies in their course syllabi and clearly communicate such policies to students. Students who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification. The BCA will continue to monitor developments in these tools and their use in academic settings and may update this guidance. Members of the community are encouraged to contact the BCA to provide input, suggestions, and comments on this policy. NOTE: As part of the BCA s guidance on clear communication of a course s generative AI policy, OCS recommends course instructors provide clear advance notice that they may use detection software to review work submitted for use of generative AI.",
        "text_length": 2268
      },
      "themes": [
        {
          "name": "Student Guidelines",
          "score": 4.5,
          "frequency": 4,
          "keywords": [
            "student",
            "students",
            "coursework",
            "assignment"
          ],
          "matches": [],
          "confidence": 45
        },
        {
          "name": "Faculty Guidelines",
          "score": 3.5,
          "frequency": 3,
          "keywords": [
            "instructor",
            "course"
          ],
          "matches": [],
          "confidence": 35
        },
        {
          "name": "Academic Integrity",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "academic"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "AI Technology",
          "score": 1.0,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 10,
          "entities": [
            "Guidance Main",
            "AI"
          ]
        },
        {
          "name": "Assessment and Evaluation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 45,
        "method": "hybrid",
        "scores": {
          "Restrictive": 1.3154834942672964,
          "Moderate": 4.994435190664014,
          "Permissive": 8.970081315068684
        },
        "reasoning": "Classified as Permissive based on: - 'guidance' appears 5 time(s) - 'encouraged' appears 2 time(s) - 'assistance' appears 2 time(s) - Encouraging language detected",
        "details": {
          "rule_based": {
            "classification": "Permissive",
            "confidence": 71,
            "scores": {
              "Restrictive": 0.0,
              "Moderate": 5.3999999999999995,
              "Permissive": 13.4
            },
            "keyword_matches": {
              "Restrictive": [],
              "Moderate": [
                [
                  "guidelines",
                  1,
                  1.2
                ],
                [
                  "context",
                  1,
                  1.4
                ],
                [
                  "individual",
                  1,
                  1.4
                ],
                [
                  "particular",
                  1,
                  1.4
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  2,
                  3.6
                ],
                [
                  "foster",
                  1,
                  1.8
                ],
                [
                  "guidance",
                  5,
                  5.0
                ],
                [
                  "suggestions",
                  1,
                  1.0
                ],
                [
                  "assistance",
                  2,
                  2.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 43,
            "scores": {
              "Moderate": 0.43860879766600336,
              "Permissive": 0.23252032876717124,
              "Restrictive": 0.3288708735668241
            },
            "method": "ml_based"
          },
          "agreement": false
        }
      },
      "summary": {
        "total_themes": 5,
        "top_theme": "Student Guidelines",
        "classification_type": "Permissive",
        "confidence": 45
      },
      "analysis_id": "analysis_12"
    },
    {
      "_id": "analysis_13",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_180436_2_20250614_232901_belfast_university.pdf",
      "analysis_date": "2025-06-26T17:04:41.406853",
      "text_data": {
        "original_text": " \n1 \n \nGuidance on the use of AI in Assessments 2022-23 \n \nIntroduction \nThe goal of this document is to provide guidance on how we can effectively integrate \nor manage the use of generative AI in our assessments.  Generative AI is a type of \nArtificial Intelligence that generates new outputs such as images, audio, or text \nbased on learned patterns and inputs. \nThe Quality Assurance Agency has recently produced a briefing paper on the threat \nto academic integrity posed by AI, which is available on the QAA website.  The \nbriefing paper provides background on AI software tools, as well as the potential \nimplications for academic standards and actions that providers can take.  It is \nimportant to stress that a blanket ban on generative AI is not viable. Instead, we \nneed to focus on responsible usage by staff and students and associated ethical \nconsiderations to ensure the safe and productive deployment of this technology.  \nThis requires a consideration of how to use AI in an ethical, responsible, and \nappropriate manner that considers fairness and transparency. This is particularly \nimportant as the text created through generative AI tools may not be text-matched \nthrough tools such as Turnitin. This is because the text created by AI tools is original.  \nThe challenge of preserving academic integrity is not a new problem and the \nintroduction of generative AIs will not change the fundamental challenges that face \neducators.  To effectively prevent cheating, it is essential to address its root causes.  \nResearch shows this can be achieved by promoting a strong culture of academic \nintegrity that emphasises the importance of honesty and integrity.  Our ultimate \nobjective is to prepare students for the dynamic and evolving job market of the \nfuture. \n \nContext \nMuch of the work undertaken across the HE sector in recent weeks has shown that \nAI tools and their outputs vary.  Essentially the quality of the response from a \ngenerative AI tool will depend on the quality and extent of the training material.  \nWhere this is limited, then the responses will tend to be bland/generic, lack critical \nanalysis, and so on.  However, the inverse is also true and where the training data is \ngood, then the ability to answer diverse/complex questions on those topics is likely to \nbe broadly good as well. \n \nAdvice for Staff \nAs such our advice to staff is to be clear with students as to what you regard as a \npermissible use of AI in any given assignment and outline how its inclusion or use \nshould be acknowledged. To do this consider: \n\u2022 Having open and transparent discussions with students regarding the \nacceptable and unacceptable use of AI in assessments.  This should include \n \n2 \n \nexplicit instructions on what constitutes appropriate use, such as the production of \noriginal work.  Students should be required to acknowledge requirements through a \ndeclaration of integrity form and be informed that any unacceptable use of AI will be \nconsidered academic misconduct. \n\u2022 Review how generative AI might enhance student learning through the assessment. \nFor example, the AI could be used to analyse and summarise relevant materials, \nprovide a draft structure or starting point, or otherwise free up time for students to \nfocus on other critical aspects of their learning such as evaluation, synthesis, critical \nthinking, or reflection.  \n\u2022 Have a conversation with students around data privacy.  One of the key ways in \nwhich many AI tools learn is through processing larger data sets.  When you register \nwith an AI tool you may be required to provide permission that any submitted data \nand any interactions with the AI can be used to improve future versions. \n \nWhen is it appropriate for students to use AI? \nThere are a number of assessment situations where AI can be used, for example: \n\u2022 When it is approved for use and is part of the assessment process. \n\u2022 For revision of a course or other materials. \n\u2022 When the use of AI in an assignment has been referenced appropriately and \nis allowed as part of the resources drawn upon to create an assessment. \n\u2022 As a tool for refining writing. \n \nHow might you adapt assessments for students? \nThere are a number of steps that you can take to make assessments more robust: \n\u2022 Shifting the assessment from recall of knowledge to real world application, for \nexample the requirement to include current events, research and activities.  \n\u2022 Asking students to provide an annotated bibliography to demonstrate sources \nof evidence used. \n\u2022 Citation presents a problem because AI is neither an author nor a source of \ninformation, but a writing aid. It would be more useful for staff to know how the \ntool was used in the writing process. Staff could provide direction or guidance \non acceptable references or sources of referencing for the assessment and \nprovide more explicit guidance on acceptable source material. \n\u2022 Include the use of reflective responses that build in personal insight.  AI tools \nare far le",
        "cleaned_text": "1 Guidance on the use of AI in Assessments 2022-23 Introduction The goal of this document is to provide guidance on how we can effectively integrate or manage the use of generative AI in our assessments. Generative AI is a type of Artificial Intelligence that generates new outputs such as images, audio, or text based on learned patterns and inputs. The Quality Assurance Agency has recently produced a briefing paper on the threat to academic integrity posed by AI, which is available on the QAA website. The briefing paper provides background on AI software tools, as well as the potential implications for academic standards and actions that providers can take. It is important to stress that a blanket ban on generative AI is not viable. Instead, we need to focus on responsible usage by staff and students and associated ethical considerations to ensure the safe and productive deployment of this technology. This requires a consideration of how to use AI in an ethical, responsible, and appropriate manner that considers fairness and transparency. This is particularly important as the text created through generative AI tools may not be text-matched through tools such as Turnitin. This is because the text created by AI tools is original. The challenge of preserving academic integrity is not a new problem and the introduction of generative AIs will not change the fundamental challenges that face educators. To effectively prevent cheating, it is essential to address its root causes. Research shows this can be achieved by promoting a strong culture of academic integrity that emphasises the importance of honesty and integrity. Our ultimate objective is to prepare students for the dynamic and evolving job market of the future. Context Much of the work undertaken across the HE sector in recent weeks has shown that AI tools and their outputs vary. Essentially the quality of the response from a generative AI tool will depend on the quality and extent of the training material. Where this is limited, then the responses will tend to be bland generic, lack critical analysis, and so on. However, the inverse is also true and where the training data is good, then the ability to answer diverse complex questions on those topics is likely to be broadly good as well. Advice for Staff As such our advice to staff is to be clear with students as to what you regard as a permissible use of AI in any given assignment and outline how its inclusion or use should be acknowledged. To do this consider: Having open and transparent discussions with students regarding the acceptable and unacceptable use of AI in assessments. This should include 2 explicit instructions on what constitutes appropriate use, such as the production of original work. Students should be required to acknowledge requirements through a declaration of integrity form and be informed that any unacceptable use of AI will be considered academic misconduct. Review how generative AI might enhance student learning through the assessment. For example, the AI could be used to analyse and summarise relevant materials, provide a draft structure or starting point, or otherwise free up time for students to focus on other critical aspects of their learning such as evaluation, synthesis, critical thinking, or reflection. Have a conversation with students around data privacy. One of the key ways in which many AI tools learn is through processing larger data sets. When you register with an AI tool you may be required to provide permission that any submitted data and any interactions with the AI can be used to improve future versions. When is it appropriate for students to use AI? There are a number of assessment situations where AI can be used, for example: When it is approved for use and is part of the assessment process. For revision of a course or other materials. When the use of AI in an assignment has been referenced appropriately and is allowed as part of the resources drawn upon to create an assessment. As a tool for refining writing. How might you adapt assessments for students? There are a number of steps that you can take to make assessments more robust: Shifting the assessment from recall of knowledge to real world application, for example the requirement to include current events, research and activities. Asking students to provide an annotated bibliography to demonstrate sources of evidence used. Citation presents a problem because AI is neither an author nor a source of information, but a writing aid. It would be more useful for staff to know how the tool was used in the writing process. Staff could provide direction or guidance on acceptable references or sources of referencing for the assessment and provide more explicit guidance on acceptable source material. Include the use of reflective responses that build in personal insight. AI tools are far less useful in this context. Request a critique of already written responses (including AI). In addition the following types of asse",
        "text_length": 9061
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 35.5,
          "frequency": 35,
          "keywords": [
            "cheating",
            "academic",
            "integrity",
            "misconduct",
            "citation"
          ],
          "matches": [
            "academic integrity",
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 100
        },
        {
          "name": "Student Guidelines",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "student",
            "students",
            "assignment",
            "submission"
          ],
          "matches": [],
          "confidence": 100
        },
        {
          "name": "Assessment and Evaluation",
          "score": 9.0,
          "frequency": 9,
          "keywords": [
            "assessment",
            "evaluation",
            "review"
          ],
          "matches": [],
          "confidence": 90
        },
        {
          "name": "Privacy and Data",
          "score": 5.5,
          "frequency": 5,
          "keywords": [
            "privacy",
            "data",
            "personal",
            "information"
          ],
          "matches": [
            "data privacy"
          ],
          "confidence": 55
        },
        {
          "name": "AI Ethics",
          "score": 5.0,
          "frequency": 5,
          "keywords": [
            "ethical",
            "moral",
            "responsible"
          ],
          "matches": [
            "ethical considerations"
          ],
          "confidence": 50
        },
        {
          "name": "Research and Innovation",
          "score": 4.0,
          "frequency": 4,
          "keywords": [
            "research",
            "advancement",
            "investigation",
            "analysis"
          ],
          "matches": [],
          "confidence": 40
        },
        {
          "name": "Faculty Guidelines",
          "score": 3.0,
          "frequency": 3,
          "keywords": [
            "staff",
            "course"
          ],
          "matches": [],
          "confidence": 30
        },
        {
          "name": "Transparency",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "transparency",
            "transparent",
            "open"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "Bias and Fairness",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "fairness",
            "diverse"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "AI Technology",
          "score": 1.0,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 10,
          "entities": [
            "Artificial Intelligence",
            "AI"
          ]
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 64,
        "method": "hybrid",
        "scores": {
          "Restrictive": 11.384753756099911,
          "Moderate": 24.824055291062773,
          "Permissive": 12.731190952837306
        },
        "reasoning": "Classified as Moderate based on: - 'integrity' appears 8 time(s) - 'appropriate' appears 3 time(s) - 'ethical' appears 3 time(s) - Emphasis on responsible use",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 50,
            "scores": {
              "Restrictive": 16.7,
              "Moderate": 37.5,
              "Permissive": 20.699999999999996
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "limited",
                  1,
                  1.5
                ],
                [
                  "penalties",
                  1,
                  2.0
                ],
                [
                  "consequences",
                  1,
                  2.0
                ],
                [
                  "academic misconduct",
                  3,
                  6.0
                ],
                [
                  "required",
                  2,
                  2.6
                ],
                [
                  "essential",
                  1,
                  1.3
                ],
                [
                  "necessary",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  3,
                  4.5
                ],
                [
                  "considered",
                  1,
                  1.5
                ],
                [
                  "standards",
                  1,
                  1.2
                ],
                [
                  "requirements",
                  1,
                  1.2
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "ethical",
                  3,
                  3.9000000000000004
                ],
                [
                  "integrity",
                  8,
                  10.4
                ],
                [
                  "transparency",
                  1,
                  1.3
                ],
                [
                  "fairness",
                  1,
                  1.3
                ],
                [
                  "context",
                  2,
                  2.8
                ],
                [
                  "individual",
                  2,
                  2.8
                ],
                [
                  "specific",
                  1,
                  1.4
                ],
                [
                  "particular",
                  1,
                  1.4
                ]
              ],
              "Permissive": [
                [
                  "support",
                  2,
                  3.6
                ],
                [
                  "facilitate",
                  1,
                  1.8
                ],
                [
                  "creative",
                  1,
                  1.5
                ],
                [
                  "enhance",
                  1,
                  1.2
                ],
                [
                  "improve",
                  1,
                  1.2
                ],
                [
                  "advancement",
                  1,
                  1.2
                ],
                [
                  "progress",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  4,
                  4.0
                ],
                [
                  "advice",
                  2,
                  2.0
                ],
                [
                  "support",
                  2,
                  2.0
                ],
                [
                  "resources",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 58,
            "scores": {
              "Moderate": 0.5810138227656932,
              "Permissive": 0.07779773820932716,
              "Restrictive": 0.34118843902497803
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 11,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 64
      },
      "analysis_id": "analysis_13"
    },
    {
      "_id": "analysis_14",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-26T17:18:34.549365",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_14"
    },
    {
      "_id": "analysis_15",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-26T17:36:08.097045",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_15"
    },
    {
      "_id": "analysis_16",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_180436_2_20250614_220705_stanford-ai-policy.pdf",
      "analysis_date": "2025-06-26T17:55:54.259730",
      "text_data": {
        "original_text": "Generative AI Policy Guidance Main content start Guidance adopted on February 16, 2023    Honor Code Implications of Generative AI Tools The Board on Conduct Affairs (BCA) has been asked to address the Honor Code implications of generative AI tools such as ChatGPT, Bard, DALL-E, and Stable Diffusion. These are novel tools, and both students and instructors have been experimenting with their use in academic settings.   While these tools have applications that foster student learning and understanding, these tools can also be used in ways that bypass key learning objectives. To give sufficient space for instructors to explore uses of generative AI tools in their courses, and to set clear guidelines to students about what uses are and are not consistent with the Stanford Honor Code, the BCA has set forth the following policy guidance regarding generative AI in the context of coursework: Absent a clear statement from a course instructor, use of or consultation with generative AI shall be treated analogously to assistance from another person. In particular, using generative AI tools to substantially complete an assignment or exam (e.g. by entering exam or assignment questions) is not permitted. Students should acknowledge the use of generative AI (other than incidental use) and default to disclosing such assistance when in doubt. \n\nIndividual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools. Course instructors should set such policies in their course syllabi and clearly communicate such policies to students. Students who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification. The BCA will continue to monitor developments in these tools and their use in academic settings and may update this guidance. Members of the community are encouraged to contact the BCA to provide input, suggestions, and comments on this policy. NOTE: As part of the BCA\u2019s guidance on clear communication of a course\u2019s generative AI policy, OCS recommends course instructors provide clear advance notice that they may use detection software to review work submitted for use of generative AI.   \n",
        "cleaned_text": "Generative AI Policy Guidance Main content start Guidance adopted on February 16, 2023 Honor Code Implications of Generative AI Tools The Board on Conduct Affairs (BCA) has been asked to address the Honor Code implications of generative AI tools such as ChatGPT, Bard, DALL-E, and Stable Diffusion. These are novel tools, and both students and instructors have been experimenting with their use in academic settings. While these tools have applications that foster student learning and understanding, these tools can also be used in ways that bypass key learning objectives. To give sufficient space for instructors to explore uses of generative AI tools in their courses, and to set clear guidelines to students about what uses are and are not consistent with the Stanford Honor Code, the BCA has set forth the following policy guidance regarding generative AI in the context of coursework: Absent a clear statement from a course instructor, use of or consultation with generative AI shall be treated analogously to assistance from another person. In particular, using generative AI tools to substantially complete an assignment or exam (e. g. by entering exam or assignment questions) is not permitted. Students should acknowledge the use of generative AI (other than incidental use) and default to disclosing such assistance when in doubt. Individual course instructors are free to set their own policies regulating the use of generative AI tools in their courses, including allowing or disallowing some or all uses of such tools. Course instructors should set such policies in their course syllabi and clearly communicate such policies to students. Students who are unsure of policies regarding generative AI tools are encouraged to ask their instructors for clarification. The BCA will continue to monitor developments in these tools and their use in academic settings and may update this guidance. Members of the community are encouraged to contact the BCA to provide input, suggestions, and comments on this policy. NOTE: As part of the BCA s guidance on clear communication of a course s generative AI policy, OCS recommends course instructors provide clear advance notice that they may use detection software to review work submitted for use of generative AI.",
        "text_length": 2268
      },
      "themes": [
        {
          "name": "Student Guidelines",
          "score": 4.5,
          "frequency": 4,
          "keywords": [
            "student",
            "students",
            "coursework",
            "assignment"
          ],
          "matches": [],
          "confidence": 45
        },
        {
          "name": "Faculty Guidelines",
          "score": 3.5,
          "frequency": 3,
          "keywords": [
            "instructor",
            "course"
          ],
          "matches": [],
          "confidence": 35
        },
        {
          "name": "Academic Integrity",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "academic"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "AI Technology",
          "score": 1.0,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 10,
          "entities": [
            "Guidance Main",
            "AI"
          ]
        },
        {
          "name": "Assessment and Evaluation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 45,
        "method": "hybrid",
        "scores": {
          "Restrictive": 1.3154834942672964,
          "Moderate": 4.994435190664014,
          "Permissive": 8.970081315068684
        },
        "reasoning": "Classified as Permissive based on: - 'guidance' appears 5 time(s) - 'encouraged' appears 2 time(s) - 'assistance' appears 2 time(s) - Encouraging language detected",
        "details": {
          "rule_based": {
            "classification": "Permissive",
            "confidence": 71,
            "scores": {
              "Restrictive": 0.0,
              "Moderate": 5.3999999999999995,
              "Permissive": 13.4
            },
            "keyword_matches": {
              "Restrictive": [],
              "Moderate": [
                [
                  "guidelines",
                  1,
                  1.2
                ],
                [
                  "context",
                  1,
                  1.4
                ],
                [
                  "individual",
                  1,
                  1.4
                ],
                [
                  "particular",
                  1,
                  1.4
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  2,
                  3.6
                ],
                [
                  "foster",
                  1,
                  1.8
                ],
                [
                  "guidance",
                  5,
                  5.0
                ],
                [
                  "suggestions",
                  1,
                  1.0
                ],
                [
                  "assistance",
                  2,
                  2.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 43,
            "scores": {
              "Moderate": 0.43860879766600336,
              "Permissive": 0.23252032876717124,
              "Restrictive": 0.3288708735668241
            },
            "method": "ml_based"
          },
          "agreement": false
        }
      },
      "summary": {
        "total_themes": 5,
        "top_theme": "Student Guidelines",
        "classification_type": "Permissive",
        "confidence": 45
      },
      "analysis_id": "analysis_16"
    },
    {
      "_id": "analysis_17",
      "user_id": 1,
      "document_id": null,
      "filename": "1_20250626_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-26T18:05:24.801092",
      "text_data": {
        "original_text": "Test 1_20250626_harvard AI policy document content with academic integrity guidelines...",
        "cleaned_text": "Test 1_20250626_harvard AI policy document content with academic integrity guidelines...",
        "text_length": 88
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        },
        {
          "name": "Student Guidelines",
          "score": 4.2,
          "confidence": 82
        },
        {
          "name": "Assessment Policy",
          "score": 3.9,
          "confidence": 75
        },
        {
          "name": "Plagiarism Detection",
          "score": 4.1,
          "confidence": 88
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 75,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 5,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 75
      },
      "analysis_id": "analysis_17"
    },
    {
      "_id": "analysis_18",
      "user_id": 1,
      "document_id": null,
      "filename": "1_20250626_stanford-ai-policy.pdf",
      "analysis_date": "2025-06-26T18:05:24.805540",
      "text_data": {
        "original_text": "Test 1_20250626_stanford AI policy document content with academic integrity guidelines...",
        "cleaned_text": "Test 1_20250626_stanford AI policy document content with academic integrity guidelines...",
        "text_length": 89
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        },
        {
          "name": "Student Guidelines",
          "score": 4.2,
          "confidence": 82
        },
        {
          "name": "Assessment Policy",
          "score": 3.9,
          "confidence": 75
        },
        {
          "name": "Plagiarism Detection",
          "score": 4.1,
          "confidence": 88
        }
      ],
      "classification": {
        "classification": "Restrictive",
        "confidence": 82,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 5,
        "top_theme": "Academic Integrity",
        "classification_type": "Restrictive",
        "confidence": 82
      },
      "analysis_id": "analysis_18"
    },
    {
      "_id": "analysis_19",
      "user_id": 1,
      "document_id": null,
      "filename": "1_20250626_mit-ai-policy.pdf",
      "analysis_date": "2025-06-26T18:05:24.808629",
      "text_data": {
        "original_text": "Test 1_20250626_mit AI policy document content with academic integrity guidelines...",
        "cleaned_text": "Test 1_20250626_mit AI policy document content with academic integrity guidelines...",
        "text_length": 84
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        },
        {
          "name": "Student Guidelines",
          "score": 4.2,
          "confidence": 82
        },
        {
          "name": "Assessment Policy",
          "score": 3.9,
          "confidence": 75
        },
        {
          "name": "Plagiarism Detection",
          "score": 4.1,
          "confidence": 88
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 78,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 5,
        "top_theme": "Academic Integrity",
        "classification_type": "Permissive",
        "confidence": 78
      },
      "analysis_id": "analysis_19"
    },
    {
      "_id": "analysis_20",
      "user_id": 1,
      "document_id": null,
      "filename": "1_20250626_oxford-ai-policy.pdf",
      "analysis_date": "2025-06-26T18:05:24.811487",
      "text_data": {
        "original_text": "Test 1_20250626_oxford AI policy document content with academic integrity guidelines...",
        "cleaned_text": "Test 1_20250626_oxford AI policy document content with academic integrity guidelines...",
        "text_length": 87
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        },
        {
          "name": "Student Guidelines",
          "score": 4.2,
          "confidence": 82
        },
        {
          "name": "Assessment Policy",
          "score": 3.9,
          "confidence": 75
        },
        {
          "name": "Plagiarism Detection",
          "score": 4.1,
          "confidence": 88
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 80,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 5,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 80
      },
      "analysis_id": "analysis_20"
    },
    {
      "_id": "analysis_21",
      "user_id": 1,
      "document_id": null,
      "filename": "1_20250626_cambridge-ai-policy.pdf",
      "analysis_date": "2025-06-26T18:05:24.814624",
      "text_data": {
        "original_text": "Test 1_20250626_cambridge AI policy document content with academic integrity guidelines...",
        "cleaned_text": "Test 1_20250626_cambridge AI policy document content with academic integrity guidelines...",
        "text_length": 90
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        },
        {
          "name": "Student Guidelines",
          "score": 4.2,
          "confidence": 82
        },
        {
          "name": "Assessment Policy",
          "score": 3.9,
          "confidence": 75
        },
        {
          "name": "Plagiarism Detection",
          "score": 4.1,
          "confidence": 88
        }
      ],
      "classification": {
        "classification": "Restrictive",
        "confidence": 85,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 5,
        "top_theme": "Academic Integrity",
        "classification_type": "Restrictive",
        "confidence": 85
      },
      "analysis_id": "analysis_21"
    },
    {
      "_id": "analysis_22",
      "user_id": 1,
      "document_id": null,
      "filename": "1_20250626_columbia-ai-policy.pdf",
      "analysis_date": "2025-06-26T18:05:24.817464",
      "text_data": {
        "original_text": "Test 1_20250626_columbia AI policy document content with academic integrity guidelines...",
        "cleaned_text": "Test 1_20250626_columbia AI policy document content with academic integrity guidelines...",
        "text_length": 89
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        },
        {
          "name": "Student Guidelines",
          "score": 4.2,
          "confidence": 82
        },
        {
          "name": "Assessment Policy",
          "score": 3.9,
          "confidence": 75
        },
        {
          "name": "Plagiarism Detection",
          "score": 4.1,
          "confidence": 88
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 73,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 5,
        "top_theme": "Academic Integrity",
        "classification_type": "Permissive",
        "confidence": 73
      },
      "analysis_id": "analysis_22"
    },
    {
      "_id": "analysis_23",
      "user_id": 1,
      "document_id": null,
      "filename": "1_20250626_chicago-ai-policy.docx",
      "analysis_date": "2025-06-26T18:05:24.820824",
      "text_data": {
        "original_text": "Test 1_20250626_chicago AI policy document content with academic integrity guidelines...",
        "cleaned_text": "Test 1_20250626_chicago AI policy document content with academic integrity guidelines...",
        "text_length": 88
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        },
        {
          "name": "Student Guidelines",
          "score": 4.2,
          "confidence": 82
        },
        {
          "name": "Assessment Policy",
          "score": 3.9,
          "confidence": 75
        },
        {
          "name": "Plagiarism Detection",
          "score": 4.1,
          "confidence": 88
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 77,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 5,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 77
      },
      "analysis_id": "analysis_23"
    },
    {
      "_id": "analysis_24",
      "user_id": 1,
      "document_id": null,
      "filename": "1_20250626_belfast-university-ai-policy.pdf",
      "analysis_date": "2025-06-26T18:05:24.823748",
      "text_data": {
        "original_text": "Test 1_20250626_belfast AI policy document content with academic integrity guidelines...",
        "cleaned_text": "Test 1_20250626_belfast AI policy document content with academic integrity guidelines...",
        "text_length": 88
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        },
        {
          "name": "Student Guidelines",
          "score": 4.2,
          "confidence": 82
        },
        {
          "name": "Assessment Policy",
          "score": 3.9,
          "confidence": 75
        },
        {
          "name": "Plagiarism Detection",
          "score": 4.1,
          "confidence": 88
        }
      ],
      "classification": {
        "classification": "Restrictive",
        "confidence": 81,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 5,
        "top_theme": "Academic Integrity",
        "classification_type": "Restrictive",
        "confidence": 81
      },
      "analysis_id": "analysis_24"
    },
    {
      "_id": "analysis_25",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T10:18:35.771882",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_25"
    },
    {
      "_id": "analysis_26",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250627_111922_2_20250614_220705_chicago-ai-policy.docx",
      "analysis_date": "2025-06-27T10:19:22.430069",
      "text_data": {
        "original_text": "Generative AI tools offer many capabilities and efficiencies that can greatly enhance our work. When using these tools, members of the University community must consider issues related to information security, privacy, compliance, and academic integrity.\nView guidance on using and procuring generative AI tools such as PhoenixAI, OpenAI\u2019s ChatGPT, Microsoft Copilot, and Google\u2019s Gemini.\n\u00a0\nGUIDELINES ON USING AND PROCURING GENERATIVE AI TOOLS\n1. PROTECTION OF UNIVERSITY DATA\nThe use of confidential data with publicly available generative AI tools is prohibited without prior security and privacy review. This includes personally identifiable employee data, FERPA-covered student data, HIPAA-covered patient data, and may include research that is not yet publicly available. Some grantors,\u00a0including the National Institutes of Health, have policies prohibiting the use of generative AI tools in analyzing or reviewing grant applications or proposals. Information shared with publicly available generative AI tools may expose sensitive information to unauthorized parties or violate data use agreements. (Please see\u00a0Policy 601\u00a0or definitions of confidential data and its use for more information.)\n\n\n\n2. RESPONSIBILITY FOR CONTENT ACCURACY AND OWNERSHIP\nAI-generated content may be misleading or inaccurate. Generative AI technology may create citations to content that does not exist. Responses from generative AI tools may contain content and materials from other authors and may be copyrighted. It is the responsibility of the tool user to review the accuracy and ownership of any AI-generated content.\n\n\n\n3. ACADEMIC INTEGRITY\nFor guidance on how generative AI tools intersect with academic honesty, it is recommended that instructors contact the\u00a0Chicago Center for Teaching and Learning. (See\u00a0Academic Honesty & Plagiarism\u00a0in the Student Manual for University policy.)\n\n\n\n4.\u00a0PROCURING AND ACQUIRING GENERATIVE AI TOOLS\nGenerative AI systems, applications, and software products that process, analyze, or move\u00a0confidential data\u00a0require a security review before they are acquired, even if the software is free. This review will help ensure the security and privacy of University data.\u00a0\nPlease contact IT Services by submitting our\u00a0Generative AI Tool Review form\u00a0before acquiring or using any tools, add-ons, or modules that include generative AI technology with University confidential data, even if they are free. For more information, see the\u00a0Policy on the Use of External Services\u00a0and the\u00a0Policy of Procurement and Engagement.\nCONTACTS\nIf you have questions about the guidelines, please contact:\nKevin Boyd, Chief Information Officer, at\u00a0cio@uchicago.edu\nMatt Morton, Chief Information Security Officer, at\u00a0ciso@uchicago.edu\n\n",
        "cleaned_text": "Generative AI tools offer many capabilities and efficiencies that can greatly enhance our work. When using these tools, members of the University community must consider issues related to information security, privacy, compliance, and academic integrity. View guidance on using and procuring generative AI tools such as PhoenixAI, OpenAI s ChatGPT, Microsoft Copilot, and Google s Gemini. GUIDELINES ON USING AND PROCURING GENERATIVE AI TOOLS 1. PROTECTION OF UNIVERSITY DATA The use of confidential data with publicly available generative AI tools is prohibited without prior security and privacy review. This includes personally identifiable employee data, FERPA-covered student data, HIPAA-covered patient data, and may include research that is not yet publicly available. Some grantors, including the National Institutes of Health, have policies prohibiting the use of generative AI tools in analyzing or reviewing grant applications or proposals. Information shared with publicly available generative AI tools may expose sensitive information to unauthorized parties or violate data use agreements. (Please see Policy 601 or definitions of confidential data and its use for more information. ) 2. RESPONSIBILITY FOR CONTENT ACCURACY AND OWNERSHIP AI-generated content may be misleading or inaccurate. Generative AI technology may create citations to content that does not exist. Responses from generative AI tools may contain content and materials from other authors and may be copyrighted. It is the responsibility of the tool user to review the accuracy and ownership of any AI-generated content. 3. ACADEMIC INTEGRITY For guidance on how generative AI tools intersect with academic honesty, it is recommended that instructors contact the Chicago Center for Teaching and Learning. (See Academic Honesty Plagiarism in the Student Manual for University policy. ) 4. PROCURING AND ACQUIRING GENERATIVE AI TOOLS Generative AI systems, applications, and software products that process, analyze, or move confidential data require a security review before they are acquired, even if the software is free. This review will help ensure the security and privacy of University data. Please contact IT Services by submitting our Generative AI Tool Review form before acquiring or using any tools, add-ons, or modules that include generative AI technology with University confidential data, even if they are free. For more information, see the Policy on the Use of External Services and the Policy of Procurement and Engagement. CONTACTS If you have questions about the guidelines, please contact: Kevin Boyd, Chief Information Officer, at cio uchicago. edu Matt Morton, Chief Information Security Officer, at ciso uchicago. edu",
        "text_length": 2722
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 15.0,
          "frequency": 15,
          "keywords": [
            "privacy",
            "data",
            "confidential",
            "protection",
            "security"
          ],
          "matches": [],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 7.5,
          "frequency": 7,
          "keywords": [
            "plagiarism",
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "ACADEMIC INTEGRITY"
          ],
          "confidence": 75
        },
        {
          "name": "Assessment and Evaluation",
          "score": 2.5,
          "frequency": 2,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 25
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsibility"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Student Guidelines",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "student"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "AI Technology",
          "score": 1.0,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 10,
          "entities": [
            "OWNERSHIP AI-generated",
            "AI"
          ]
        },
        {
          "name": "Faculty Guidelines",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "teaching"
          ],
          "matches": [],
          "confidence": 5
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 45,
        "method": "hybrid",
        "scores": {
          "Restrictive": 4.244920282347031,
          "Moderate": 4.503106857734661,
          "Permissive": 2.931972859918311
        },
        "reasoning": "Classified as Moderate based on: - 'integrity' appears 2 time(s) - 'guidelines' appears 2 time(s) - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 39,
            "scores": {
              "Restrictive": 4.6,
              "Moderate": 5.0,
              "Permissive": 3.2
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "prohibited",
                  1,
                  2.0
                ],
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "guidelines",
                  2,
                  2.4
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "enhance",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  2,
                  2.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 37,
            "scores": {
              "Moderate": 0.37577671443366517,
              "Permissive": 0.2529932149795777,
              "Restrictive": 0.3712300705867578
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 8,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 45
      },
      "analysis_id": "analysis_26"
    },
    {
      "_id": "analysis_27",
      "user_id": 1,
      "document_id": null,
      "filename": "181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T10:45:05.843712",
      "text_data": {
        "original_text": "Test content for 181834_2_20250614_220705_harvard-ai-policy.pdf",
        "cleaned_text": "Test content for 181834_2_20250614_220705_harvard-ai-policy.pdf",
        "text_length": 63
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 75,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 2,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 75
      },
      "analysis_id": "analysis_27"
    },
    {
      "_id": "analysis_28",
      "user_id": 1,
      "document_id": null,
      "filename": "180436_2_20250614_220705_stanford-ai-policy.pdf",
      "analysis_date": "2025-06-27T10:45:05.848495",
      "text_data": {
        "original_text": "Test content for 180436_2_20250614_220705_stanford-ai-policy.pdf",
        "cleaned_text": "Test content for 180436_2_20250614_220705_stanford-ai-policy.pdf",
        "text_length": 64
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 78,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 2,
        "top_theme": "Academic Integrity",
        "classification_type": "Permissive",
        "confidence": 78
      },
      "analysis_id": "analysis_28"
    },
    {
      "_id": "analysis_29",
      "user_id": 1,
      "document_id": null,
      "filename": "111922_2_20250614_220705_chicago-ai-policy.docx",
      "analysis_date": "2025-06-27T10:45:05.852502",
      "text_data": {
        "original_text": "Test content for 111922_2_20250614_220705_chicago-ai-policy.docx",
        "cleaned_text": "Test content for 111922_2_20250614_220705_chicago-ai-policy.docx",
        "text_length": 64
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 80,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 2,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 80
      },
      "analysis_id": "analysis_29"
    },
    {
      "_id": "analysis_30",
      "user_id": 1,
      "document_id": null,
      "filename": "181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T10:46:47.289375",
      "text_data": {
        "original_text": "Test content for 181834_2_20250614_220705_harvard-ai-policy.pdf",
        "cleaned_text": "Test content for 181834_2_20250614_220705_harvard-ai-policy.pdf",
        "text_length": 63
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        },
        {
          "name": "Privacy and Data",
          "score": 4.0,
          "confidence": 80
        },
        {
          "name": "Student Guidelines",
          "score": 3.9,
          "confidence": 75
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 75,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 4,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 75
      },
      "analysis_id": "analysis_30"
    },
    {
      "_id": "analysis_31",
      "user_id": 1,
      "document_id": null,
      "filename": "180436_2_20250614_220705_stanford-ai-policy.pdf",
      "analysis_date": "2025-06-27T10:46:47.294132",
      "text_data": {
        "original_text": "Test content for 180436_2_20250614_220705_stanford-ai-policy.pdf",
        "cleaned_text": "Test content for 180436_2_20250614_220705_stanford-ai-policy.pdf",
        "text_length": 64
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        },
        {
          "name": "Privacy and Data",
          "score": 4.0,
          "confidence": 80
        },
        {
          "name": "Student Guidelines",
          "score": 3.9,
          "confidence": 75
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 78,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 4,
        "top_theme": "Academic Integrity",
        "classification_type": "Permissive",
        "confidence": 78
      },
      "analysis_id": "analysis_31"
    },
    {
      "_id": "analysis_32",
      "user_id": 1,
      "document_id": null,
      "filename": "111922_2_20250614_220705_chicago-ai-policy.docx",
      "analysis_date": "2025-06-27T10:46:47.298168",
      "text_data": {
        "original_text": "Test content for 111922_2_20250614_220705_chicago-ai-policy.docx",
        "cleaned_text": "Test content for 111922_2_20250614_220705_chicago-ai-policy.docx",
        "text_length": 64
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 4.5,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 3.8,
          "confidence": 78
        },
        {
          "name": "Privacy and Data",
          "score": 4.0,
          "confidence": 80
        },
        {
          "name": "Student Guidelines",
          "score": 3.9,
          "confidence": 75
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 80,
        "method": "hybrid"
      },
      "summary": {
        "total_themes": 4,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 80
      },
      "analysis_id": "analysis_32"
    },
    {
      "_id": "analysis_33",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T10:59:21.849493",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_33"
    },
    {
      "_id": "analysis_34",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T11:40:57.092261",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_34"
    },
    {
      "_id": "analysis_35",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T11:42:22.868435",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_35"
    },
    {
      "_id": "analysis_36",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T11:42:54.773162",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_36"
    },
    {
      "_id": "analysis_37",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T11:43:14.423853",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_37"
    },
    {
      "_id": "analysis_38",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T11:48:52.545385",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_38"
    },
    {
      "_id": "analysis_39",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T11:49:26.516615",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_39"
    },
    {
      "_id": "analysis_40",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T11:52:19.270963",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_40"
    },
    {
      "_id": "analysis_41",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T11:52:22.788461",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_41"
    },
    {
      "_id": "analysis_42",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T11:57:51.654698",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_42"
    },
    {
      "_id": "analysis_43",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T12:08:24.583592",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_43"
    },
    {
      "_id": "analysis_44",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T12:10:05.005844",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_44"
    },
    {
      "_id": "analysis_45",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T12:10:06.397145",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_45"
    },
    {
      "_id": "analysis_46",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T12:10:19.622366",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_46"
    },
    {
      "_id": "analysis_47",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T12:11:56.015551",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_47"
    },
    {
      "_id": "analysis_48",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T12:39:31.659410",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_48"
    },
    {
      "_id": "analysis_49",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T12:39:55.745723",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_49"
    },
    {
      "_id": "analysis_50",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T12:41:58.436786",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_50"
    },
    {
      "_id": "analysis_51",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T12:43:15.168856",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_51"
    },
    {
      "_id": "analysis_52",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T12:53:41.231930",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_52"
    },
    {
      "_id": "analysis_53",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T12:54:13.028366",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_53"
    },
    {
      "_id": "analysis_54",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T13:18:46.025229",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_54"
    },
    {
      "_id": "analysis_55",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T13:18:57.255864",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_55"
    },
    {
      "_id": "analysis_56",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T13:20:11.637543",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_56"
    },
    {
      "_id": "analysis_57",
      "user_id": 3,
      "document_id": null,
      "filename": "3_20250626_181834_2_20250614_220705_harvard-ai-policy.pdf",
      "analysis_date": "2025-06-27T13:21:42.275679",
      "text_data": {
        "original_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard  Dear Members of the Harvard Community,   We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI\u2019s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity.   Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly.   Initial guidelines for use of generative AI tools: \u2022 Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University\u2019s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. \u2022 You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called \u201challucinations\u201d), or may contain copyrighted material. Review your AI-generated content before publication. \u2022 Adhere to current policies on academic integrity: Review your School\u2019s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they\u2019re teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. \u2022 Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing@harvard.edu. \u2022 Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp@harvard.edu.  o Vendor generative AI tools must be assessed for risk by Harvard\u2019s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.   \n",
        "cleaned_text": "Guidelines for Using ChatGPT and other Generative AI tools at Harvard Dear Members of the Harvard Community, We write today with initial guidelines on the use and procurement of generative artificial intelligence (AI) tools, such as OpenAI s ChatGPT and Google Bard. The University supports responsible experimentation with generative AI tools, but there are important considerations to keep in mind when using these tools, including information security and data privacy, compliance, copyright, and academic integrity. Generative AI is a rapidly evolving technology, and the University will continue to monitor developments and incorporate feedback from the Harvard community to update our guidelines accordingly. Initial guidelines for use of generative AI tools: Protect confidential data: You should not enter data classified as confidential (Level 2 and above), including non-public research data, into publicly-available generative AI tools, in accordance with the University s Information Security Policy. Information shared with generative AI tools using default settings is not private and could expose proprietary or sensitive information to unauthorized parties. You are responsible for any content that you produce or publish that includes AI-generated material: AI-generated content can be inaccurate, misleading, or entirely fabricated (sometimes called hallucinations ), or may contain copyrighted material. Review your AI-generated content before publication. Adhere to current policies on academic integrity: Review your School s student and faculty handbooks and policies. We expect that Schools will be developing and updating their policies as we better understand the implications of using generative AI tools. In the meantime, faculty should be clear with students they are teaching and advising about their policies on permitted uses, if any, of generative AI in classes and on academic work. Students are also encouraged to ask their instructors for clarification about these policies as needed. Be alert for AI-enabled phishing: Generative AI has made it easier for malicious actors to create sophisticated scams at a far greater scale. Continue to follow security best practices and report suspicious messages to phishing harvard. edu. Connect with HUIT before procuring generative AI tools: The University is working to ensure that tools procured on behalf of Harvard have the appropriate privacy and security protections and provide the best use of Harvard funds. o If you have procured or are considering procuring generative AI tools or have questions, contact HUIT at ithelp harvard. edu. o Vendor generative AI tools must be assessed for risk by Harvard s Information Security and Data Privacy office prior to use. It is important to note that these guidelines are not new University policy; rather, they leverage existing University policies. You can find more information about generative AI, including a survey to collect data on its potential use, on the HUIT website, which will be updated as new information becomes available.",
        "text_length": 3064
      },
      "themes": [
        {
          "name": "Privacy and Data",
          "score": 16.0,
          "frequency": 16,
          "keywords": [
            "privacy",
            "private",
            "data",
            "confidential",
            "security"
          ],
          "matches": [
            "data privacy",
            "Data Privacy"
          ],
          "confidence": 100
        },
        {
          "name": "Academic Integrity",
          "score": 6.5,
          "frequency": 6,
          "keywords": [
            "academic",
            "integrity"
          ],
          "matches": [
            "academic integrity",
            "academic integrity"
          ],
          "confidence": 65
        },
        {
          "name": "Student Guidelines",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "student",
            "students",
            "report"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Faculty Guidelines",
          "score": 1.5,
          "frequency": 1,
          "keywords": [
            "faculty",
            "teaching"
          ],
          "matches": [],
          "confidence": 15
        },
        {
          "name": "AI Technology",
          "score": 1.5,
          "frequency": 1,
          "keywords": [],
          "matches": [],
          "confidence": 15,
          "entities": [
            "OpenAI s ChatGPT",
            "Generative AI",
            "Data Privacy"
          ]
        },
        {
          "name": "AI Ethics",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsible"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Assessment and Evaluation",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "review"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Research and Innovation",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "research"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 72,
        "method": "hybrid",
        "scores": {
          "Restrictive": 2.884896052587446,
          "Moderate": 10.323163446306687,
          "Permissive": 2.371940501105871
        },
        "reasoning": "Classified as Moderate based on: - 'guidelines' appears 5 time(s) - 'responsible' appears 2 time(s) - 'integrity' appears 2 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 72,
            "scores": {
              "Restrictive": 2.6,
              "Moderate": 13.899999999999999,
              "Permissive": 2.8
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "must",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  1,
                  1.5
                ],
                [
                  "guidelines",
                  5,
                  6.0
                ],
                [
                  "considerations",
                  1,
                  1.2
                ],
                [
                  "responsible",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  2,
                  2.6
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "best practices",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Moderate",
            "confidence": 49,
            "scores": {
              "Moderate": 0.495790861576672,
              "Permissive": 0.17298512527646767,
              "Restrictive": 0.33122401314686145
            },
            "method": "ml_based"
          },
          "agreement": true
        }
      },
      "summary": {
        "total_themes": 9,
        "top_theme": "Privacy and Data",
        "classification_type": "Moderate",
        "confidence": 72
      },
      "analysis_id": "analysis_57"
    },
    {
      "_id": "analysis_58",
      "user_id": 6,
      "document_id": "sample_oxford",
      "filename": "[BASELINE] University of Oxford - oxford-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:05:29.653045+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Oxford",
        "cleaned_text": "Sample policy from University of Oxford (UK)",
        "text_length": 44
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Research Guidelines",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_58"
    },
    {
      "_id": "analysis_59",
      "user_id": 6,
      "document_id": "sample_cambridge",
      "filename": "[BASELINE] University of Cambridge - cambridge-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:05:29.673248+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Cambridge",
        "cleaned_text": "Sample policy from University of Cambridge (UK)",
        "text_length": 47
      },
      "themes": [
        {
          "name": "Transparency",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Academic Standards",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Innovation",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Transparency",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_59"
    },
    {
      "_id": "analysis_60",
      "user_id": 6,
      "document_id": "sample_mit",
      "filename": "[BASELINE] Massachusetts Institute of Technology - mit-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:05:29.687696+00:00",
      "text_data": {
        "original_text": "Sample policy from Massachusetts Institute of Technology",
        "cleaned_text": "Sample policy from Massachusetts Institute of Technology (USA)",
        "text_length": 62
      },
      "themes": [
        {
          "name": "Innovation",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Research Excellence",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Ethical AI",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Innovation",
        "classification_type": "Permissive",
        "confidence": 85
      },
      "analysis_id": "analysis_60"
    },
    {
      "_id": "analysis_61",
      "user_id": 6,
      "document_id": "sample_stanford",
      "filename": "[BASELINE] Stanford University - stanford-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:05:29.698842+00:00",
      "text_data": {
        "original_text": "Sample policy from Stanford University",
        "cleaned_text": "Sample policy from Stanford University (USA)",
        "text_length": 44
      },
      "themes": [
        {
          "name": "AI Literacy",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Student Empowerment",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Innovation",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "AI Literacy",
        "classification_type": "Permissive",
        "confidence": 85
      },
      "analysis_id": "analysis_61"
    },
    {
      "_id": "analysis_62",
      "user_id": 6,
      "document_id": "sample_harvard",
      "filename": "[BASELINE] Harvard University - harvard-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:05:29.708335+00:00",
      "text_data": {
        "original_text": "Sample policy from Harvard University",
        "cleaned_text": "Sample policy from Harvard University (USA)",
        "text_length": 43
      },
      "themes": [
        {
          "name": "Academic Rigor",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Integrity",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Governance",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Academic Rigor",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_62"
    },
    {
      "_id": "analysis_63",
      "user_id": 6,
      "document_id": "sample_edinburgh",
      "filename": "[BASELINE] University of Edinburgh - edinburgh university-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:05:29.716800+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Edinburgh",
        "cleaned_text": "Sample policy from University of Edinburgh (UK)",
        "text_length": 47
      },
      "themes": [
        {
          "name": "Research Ethics",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "AI Governance",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Accountability",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Research Ethics",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_63"
    },
    {
      "_id": "analysis_64",
      "user_id": 7,
      "document_id": "sample_oxford",
      "filename": "[BASELINE] University of Oxford - oxford-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:28:58.051648+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Oxford",
        "cleaned_text": "Sample policy from University of Oxford (UK)",
        "text_length": 44
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Research Guidelines",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_64"
    },
    {
      "_id": "analysis_65",
      "user_id": 7,
      "document_id": "sample_cambridge",
      "filename": "[BASELINE] University of Cambridge - cambridge-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:28:58.059973+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Cambridge",
        "cleaned_text": "Sample policy from University of Cambridge (UK)",
        "text_length": 47
      },
      "themes": [
        {
          "name": "Transparency",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Academic Standards",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Innovation",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Transparency",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_65"
    },
    {
      "_id": "analysis_66",
      "user_id": 7,
      "document_id": "sample_mit",
      "filename": "[BASELINE] Massachusetts Institute of Technology - mit-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:28:58.066820+00:00",
      "text_data": {
        "original_text": "Sample policy from Massachusetts Institute of Technology",
        "cleaned_text": "Sample policy from Massachusetts Institute of Technology (USA)",
        "text_length": 62
      },
      "themes": [
        {
          "name": "Innovation",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Research Excellence",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Ethical AI",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Innovation",
        "classification_type": "Permissive",
        "confidence": 85
      },
      "analysis_id": "analysis_66"
    },
    {
      "_id": "analysis_67",
      "user_id": 7,
      "document_id": "sample_stanford",
      "filename": "[BASELINE] Stanford University - stanford-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:28:58.073316+00:00",
      "text_data": {
        "original_text": "Sample policy from Stanford University",
        "cleaned_text": "Sample policy from Stanford University (USA)",
        "text_length": 44
      },
      "themes": [
        {
          "name": "AI Literacy",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Student Empowerment",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Innovation",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "AI Literacy",
        "classification_type": "Permissive",
        "confidence": 85
      },
      "analysis_id": "analysis_67"
    },
    {
      "_id": "analysis_68",
      "user_id": 7,
      "document_id": "sample_harvard",
      "filename": "[BASELINE] Harvard University - harvard-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:28:58.080212+00:00",
      "text_data": {
        "original_text": "Sample policy from Harvard University",
        "cleaned_text": "Sample policy from Harvard University (USA)",
        "text_length": 43
      },
      "themes": [
        {
          "name": "Academic Rigor",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Integrity",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Governance",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Academic Rigor",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_68"
    },
    {
      "_id": "analysis_69",
      "user_id": 7,
      "document_id": "sample_edinburgh",
      "filename": "[BASELINE] University of Edinburgh - edinburgh university-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:28:58.086909+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Edinburgh",
        "cleaned_text": "Sample policy from University of Edinburgh (UK)",
        "text_length": 47
      },
      "themes": [
        {
          "name": "Research Ethics",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "AI Governance",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Accountability",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Research Ethics",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_69"
    },
    {
      "_id": "analysis_70",
      "user_id": 7,
      "document_id": "sample_columbia",
      "filename": "[BASELINE] Columbia University - columbia-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:28:58.093603+00:00",
      "text_data": {
        "original_text": "Sample policy from Columbia University",
        "cleaned_text": "Sample policy from Columbia University (USA)",
        "text_length": 44
      },
      "themes": [
        {
          "name": "Disclosure",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Academic Integrity",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Governance",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Disclosure",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_70"
    },
    {
      "_id": "analysis_71",
      "user_id": 7,
      "document_id": "sample_chicago",
      "filename": "[BASELINE] University of Chicago - chicago-ai-policy.docx",
      "analysis_date": "2025-07-01T16:28:58.100753+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Chicago",
        "cleaned_text": "Sample policy from University of Chicago (USA)",
        "text_length": 46
      },
      "themes": [
        {
          "name": "Research Ethics",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Academic Rigor",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Compliance",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Restrictive",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Research Ethics",
        "classification_type": "Restrictive",
        "confidence": 85
      },
      "analysis_id": "analysis_71"
    },
    {
      "_id": "analysis_72",
      "user_id": 7,
      "document_id": "sample_cornell",
      "filename": "[BASELINE] Cornell University - cornell-ai-policy.docx",
      "analysis_date": "2025-07-01T16:28:58.107380+00:00",
      "text_data": {
        "original_text": "Sample policy from Cornell University",
        "cleaned_text": "Sample policy from Cornell University (USA)",
        "text_length": 43
      },
      "themes": [
        {
          "name": "Student Support",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Academic Excellence",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Innovation",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Student Support",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_72"
    },
    {
      "_id": "analysis_73",
      "user_id": 7,
      "document_id": "sample_imperial",
      "filename": "[BASELINE] Imperial College London - imperial-ai-policy.docx",
      "analysis_date": "2025-07-01T16:28:58.114223+00:00",
      "text_data": {
        "original_text": "Sample policy from Imperial College London",
        "cleaned_text": "Sample policy from Imperial College London (UK)",
        "text_length": 47
      },
      "themes": [
        {
          "name": "STEM Innovation",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Research Excellence",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Technology Ethics",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "STEM Innovation",
        "classification_type": "Permissive",
        "confidence": 85
      },
      "analysis_id": "analysis_73"
    },
    {
      "_id": "analysis_74",
      "user_id": 7,
      "document_id": "sample_belfast",
      "filename": "[BASELINE] Queens University Belfast - belfast university-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:28:58.121125+00:00",
      "text_data": {
        "original_text": "Sample policy from Queens University Belfast",
        "cleaned_text": "Sample policy from Queens University Belfast (UK)",
        "text_length": 49
      },
      "themes": [
        {
          "name": "Practical Guidelines",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Regional Standards",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Academic Integrity",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Practical Guidelines",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_74"
    },
    {
      "_id": "analysis_75",
      "user_id": 7,
      "document_id": "sample_jagiellonian",
      "filename": "[BASELINE] Jagiellonian University - jagiellonian university-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:28:58.127943+00:00",
      "text_data": {
        "original_text": "Sample policy from Jagiellonian University",
        "cleaned_text": "Sample policy from Jagiellonian University (Poland)",
        "text_length": 51
      },
      "themes": [
        {
          "name": "European Standards",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "International Compliance",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "European Standards",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_75"
    },
    {
      "_id": "analysis_76",
      "user_id": 7,
      "document_id": "sample_leeds_trinity",
      "filename": "[BASELINE] Leeds Trinity University - leeds trinity university-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:28:58.135145+00:00",
      "text_data": {
        "original_text": "Sample policy from Leeds Trinity University",
        "cleaned_text": "Sample policy from Leeds Trinity University (UK)",
        "text_length": 48
      },
      "themes": [
        {
          "name": "Student-Centered",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Teaching Excellence",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Practical Application",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Student-Centered",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_76"
    },
    {
      "_id": "analysis_77",
      "user_id": 7,
      "document_id": "sample_tokyo",
      "filename": "[BASELINE] University of Tokyo - tokyo-ai-policy.docx",
      "analysis_date": "2025-07-01T16:28:58.141916+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Tokyo",
        "cleaned_text": "Sample policy from University of Tokyo (Japan)",
        "text_length": 46
      },
      "themes": [
        {
          "name": "Asian Standards",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Technology Integration",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Cultural Considerations",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Asian Standards",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_77"
    },
    {
      "_id": "analysis_78",
      "user_id": 7,
      "document_id": "sample_unknown_2",
      "filename": "[BASELINE] Research University Sample 2 - 2-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:28:58.148765+00:00",
      "text_data": {
        "original_text": "Sample policy from Research University Sample 2",
        "cleaned_text": "Sample policy from Research University Sample 2 (International)",
        "text_length": 63
      },
      "themes": [
        {
          "name": "Comparative Analysis",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Research Standards",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Policy Framework",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Comparative Analysis",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_78"
    },
    {
      "_id": "analysis_79",
      "user_id": 3,
      "document_id": "sample_oxford",
      "filename": "[BASELINE] University of Oxford - oxford-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:32:50.369917+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Oxford",
        "cleaned_text": "Sample policy from University of Oxford (UK)",
        "text_length": 44
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Research Guidelines",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_79"
    },
    {
      "_id": "analysis_80",
      "user_id": 3,
      "document_id": "sample_cambridge",
      "filename": "[BASELINE] University of Cambridge - cambridge-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:32:50.378958+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Cambridge",
        "cleaned_text": "Sample policy from University of Cambridge (UK)",
        "text_length": 47
      },
      "themes": [
        {
          "name": "Transparency",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Academic Standards",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Innovation",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Transparency",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_80"
    },
    {
      "_id": "analysis_81",
      "user_id": 3,
      "document_id": "sample_mit",
      "filename": "[BASELINE] Massachusetts Institute of Technology - mit-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:32:50.386856+00:00",
      "text_data": {
        "original_text": "Sample policy from Massachusetts Institute of Technology",
        "cleaned_text": "Sample policy from Massachusetts Institute of Technology (USA)",
        "text_length": 62
      },
      "themes": [
        {
          "name": "Innovation",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Research Excellence",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Ethical AI",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Innovation",
        "classification_type": "Permissive",
        "confidence": 85
      },
      "analysis_id": "analysis_81"
    },
    {
      "_id": "analysis_82",
      "user_id": 3,
      "document_id": "sample_stanford",
      "filename": "[BASELINE] Stanford University - stanford-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:32:50.394463+00:00",
      "text_data": {
        "original_text": "Sample policy from Stanford University",
        "cleaned_text": "Sample policy from Stanford University (USA)",
        "text_length": 44
      },
      "themes": [
        {
          "name": "AI Literacy",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Student Empowerment",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Innovation",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "AI Literacy",
        "classification_type": "Permissive",
        "confidence": 85
      },
      "analysis_id": "analysis_82"
    },
    {
      "_id": "analysis_83",
      "user_id": 3,
      "document_id": "sample_harvard",
      "filename": "[BASELINE] Harvard University - harvard-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:32:50.401692+00:00",
      "text_data": {
        "original_text": "Sample policy from Harvard University",
        "cleaned_text": "Sample policy from Harvard University (USA)",
        "text_length": 43
      },
      "themes": [
        {
          "name": "Academic Rigor",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Integrity",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Governance",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Academic Rigor",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_83"
    },
    {
      "_id": "analysis_84",
      "user_id": 3,
      "document_id": "sample_edinburgh",
      "filename": "[BASELINE] University of Edinburgh - edinburgh university-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:32:50.409016+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Edinburgh",
        "cleaned_text": "Sample policy from University of Edinburgh (UK)",
        "text_length": 47
      },
      "themes": [
        {
          "name": "Research Ethics",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "AI Governance",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Accountability",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Research Ethics",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_84"
    },
    {
      "_id": "analysis_85",
      "user_id": 3,
      "document_id": "sample_columbia",
      "filename": "[BASELINE] Columbia University - columbia-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:32:50.416487+00:00",
      "text_data": {
        "original_text": "Sample policy from Columbia University",
        "cleaned_text": "Sample policy from Columbia University (USA)",
        "text_length": 44
      },
      "themes": [
        {
          "name": "Disclosure",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Academic Integrity",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Governance",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Disclosure",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_85"
    },
    {
      "_id": "analysis_86",
      "user_id": 3,
      "document_id": "sample_chicago",
      "filename": "[BASELINE] University of Chicago - chicago-ai-policy.docx",
      "analysis_date": "2025-07-01T16:32:50.423591+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Chicago",
        "cleaned_text": "Sample policy from University of Chicago (USA)",
        "text_length": 46
      },
      "themes": [
        {
          "name": "Research Ethics",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Academic Rigor",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Compliance",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Restrictive",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Research Ethics",
        "classification_type": "Restrictive",
        "confidence": 85
      },
      "analysis_id": "analysis_86"
    },
    {
      "_id": "analysis_87",
      "user_id": 3,
      "document_id": "sample_cornell",
      "filename": "[BASELINE] Cornell University - cornell-ai-policy.docx",
      "analysis_date": "2025-07-01T16:32:50.430582+00:00",
      "text_data": {
        "original_text": "Sample policy from Cornell University",
        "cleaned_text": "Sample policy from Cornell University (USA)",
        "text_length": 43
      },
      "themes": [
        {
          "name": "Student Support",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Academic Excellence",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Innovation",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Student Support",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_87"
    },
    {
      "_id": "analysis_88",
      "user_id": 3,
      "document_id": "sample_imperial",
      "filename": "[BASELINE] Imperial College London - imperial-ai-policy.docx",
      "analysis_date": "2025-07-01T16:32:50.437679+00:00",
      "text_data": {
        "original_text": "Sample policy from Imperial College London",
        "cleaned_text": "Sample policy from Imperial College London (UK)",
        "text_length": 47
      },
      "themes": [
        {
          "name": "STEM Innovation",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Research Excellence",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Technology Ethics",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Permissive",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "STEM Innovation",
        "classification_type": "Permissive",
        "confidence": 85
      },
      "analysis_id": "analysis_88"
    },
    {
      "_id": "analysis_89",
      "user_id": 3,
      "document_id": "sample_belfast",
      "filename": "[BASELINE] Queens University Belfast - belfast university-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:32:50.444821+00:00",
      "text_data": {
        "original_text": "Sample policy from Queens University Belfast",
        "cleaned_text": "Sample policy from Queens University Belfast (UK)",
        "text_length": 49
      },
      "themes": [
        {
          "name": "Practical Guidelines",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Regional Standards",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Academic Integrity",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Practical Guidelines",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_89"
    },
    {
      "_id": "analysis_90",
      "user_id": 3,
      "document_id": "sample_jagiellonian",
      "filename": "[BASELINE] Jagiellonian University - jagiellonian university-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:32:50.451953+00:00",
      "text_data": {
        "original_text": "Sample policy from Jagiellonian University",
        "cleaned_text": "Sample policy from Jagiellonian University (Poland)",
        "text_length": 51
      },
      "themes": [
        {
          "name": "European Standards",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "AI Ethics",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "International Compliance",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "European Standards",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_90"
    },
    {
      "_id": "analysis_91",
      "user_id": 3,
      "document_id": "sample_leeds_trinity",
      "filename": "[BASELINE] Leeds Trinity University - leeds trinity university-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:32:50.458929+00:00",
      "text_data": {
        "original_text": "Sample policy from Leeds Trinity University",
        "cleaned_text": "Sample policy from Leeds Trinity University (UK)",
        "text_length": 48
      },
      "themes": [
        {
          "name": "Student-Centered",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Teaching Excellence",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Practical Application",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Student-Centered",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_91"
    },
    {
      "_id": "analysis_92",
      "user_id": 3,
      "document_id": "sample_tokyo",
      "filename": "[BASELINE] University of Tokyo - tokyo-ai-policy.docx",
      "analysis_date": "2025-07-01T16:32:50.466089+00:00",
      "text_data": {
        "original_text": "Sample policy from University of Tokyo",
        "cleaned_text": "Sample policy from University of Tokyo (Japan)",
        "text_length": 46
      },
      "themes": [
        {
          "name": "Asian Standards",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Technology Integration",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Cultural Considerations",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Asian Standards",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_92"
    },
    {
      "_id": "analysis_93",
      "user_id": 3,
      "document_id": "sample_unknown_2",
      "filename": "[BASELINE] Research University Sample 2 - 2-ai-policy.pdf",
      "analysis_date": "2025-07-01T16:32:50.473167+00:00",
      "text_data": {
        "original_text": "Sample policy from Research University Sample 2",
        "cleaned_text": "Sample policy from Research University Sample 2 (International)",
        "text_length": 63
      },
      "themes": [
        {
          "name": "Comparative Analysis",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Research Standards",
          "score": 0.8,
          "confidence": 85
        },
        {
          "name": "Policy Framework",
          "score": 0.8,
          "confidence": 85
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 85,
        "source": "Sample Dataset"
      },
      "summary": {
        "total_themes": 3,
        "top_theme": "Comparative Analysis",
        "classification_type": "Moderate",
        "confidence": 85
      },
      "analysis_id": "analysis_93"
    },
    {
      "_id": "analysis_99",
      "user_id": 7,
      "document_id": null,
      "filename": "7_20250708_163455_guidance-on-the-use-of-generative-ai.pdf",
      "analysis_date": "2025-07-08T16:25:00.692574+00:00",
      "text_data": {
        "original_text": "Support | Engage | Innovate\n University Guidance on  the use of\nGenerative Artificial \nIntelligence by \nstudents and staff, in \nlearning, teaching, and\nassessment \nA cademic Year  2024/2025\nAuthors:\nDr Ceri Coulby, Dr Sam Saunders & Dr Claire Ellison | Date: February 2024\nRevised 2024, by Prof David Webster, Gordon Sandison, \nRob Lindsay, Dr Sam Saunders, Dr Claire Ellison &\nRichard McKenna\nContents\n2\nIntroduction 3\nData Protection Policy and Your Responsibility  4\nGenerative AI and Copyright  5\nAcceptable Uses of GenAI Tools  5\nUnacceptable Uses of GenAI Tools  7\nPoor Academic Practice  8\nAcademic Misuse of GenAI Tools  9\nReferencing GenAI Tools 9\nFurther Information/Contact 10\nIntroduction\n3\nGenerative Artificial Intelligence (GenAI) tools are software applications that create content \nin any form (including but not limited to text, graphs, data, code, images, audio, and video) \nautomatically based on the prompt entered by the user. Examples include, but are not \nlimited to, OpenAI\u2019s ChatGPT, Google Bard/Gemini, Microsoft\u2019s Bing Chat/Copilot, DALL-E (and \nDALL-E2/3), and Perplexity.ai. \nArtificial intelligence tools are rapidly developing and increasingly becoming embedded into \neveryday activities across personal and professional contexts. To ensure our graduates have \nthe skills and experience they will need in their future careers, the University of Liverpool seeks \nto incorporate the technology in their pedagogical approach. However, it is also vital that \nsuch technology is used ethically and does not undermine academic integrity principles.  \nTo help students and staff to use such technologies appropriately, the University has \ndeveloped this guidance on acceptable and unacceptable uses of GenAI and AI (Artificial \nIntelligence) technology and its appropriate citation. This guidance is designed to be applied \nalongside Appendix L of the Code of Practice on Assessment (CoPA), which supersedes this \nguidance in regulatory terms. This guidance also serves as the University\u2019s default position \non usage. If module or programme/course component leaders wish students to use GenAI \nas part of an assessment, students will be informed of this specifically in module materials \nand assignment briefs. In these cases, specific guidance will be provided in the assignment \nbrief on what constitutes appropriate use of the GenAI tools and how the work from such tools \nshould be cited. \nThe underlying philosophy and/or ethos surrounding the use of Generative AI at the University \nof Liverpool is one of promoting literacy around the technology in both students and staff, \nand using the technology as openly, honestly, and transparently as possible. This ensures that \nany engagement with the technology is in line with both the Liverpool Curriculum Framework \n(LCF) - which promotes \u2018Digital Fluency\u2019 as a Graduate Attribute \u2013 and Strategy 2031, which \nstates that the university will work to integrate AI into its practice(s). An outright ban on \nthe use of the technology, therefore, contravenes both requirements, in addition to being \nimpossible to enforce.\n4\nIn general, therefore, the guiding principle(s) of the use of Generative AI that should inform all \npractice is that \n1. Both students and staff should openly discuss, experiment with, and engage with the \ntechnology in discursive ways where possible, to help improve general understanding of \nits capabilities, functionalities, limitations, and problems/biases.\n2. Any use of the technology in either an assessment or any other context should always \nbe declared and evaluated/reflected on, and if necessary, cited and referenced. \nCitations and references to AI should follow the same process as if referencing or citing \nan academic source. This goes for both staff and students \u2013 for example, students can \nbe asked to declare their use of GenAI on an assessment cover sheet, while staff should \nmake it clear on their teaching materials where and how GenAI has been used to help \ncreate them.  \n3. The technology is not used as a substitute for original thought, independent research, and \nthe production of original work. Rather, it is used to support these processes.  \nThat said, the university recognises that there are some situations where the use of \nGenerative AI is simply unacceptable and will attract sanctions under Appendix L of the Code \nof Practice on Assessment (CoPA). This guidance provides some steerage on what that \nunacceptable use of Generative AI looks like and should be consulted in line with the relevant \nsections of CoPA that deal with academic integrity.   \nData Protection Policy and Your Responsibility \nStudents and Staff interacting with GAI systems bear responsibilities for ensuring the \nUniversities Data Protection Policy is followed.  \nThe UK General Data Protection Regulation (GDPR) and Data Protection Act (DPA) 2018 \nemphasise the importance of not submitting sensitive or personal data unless necessary and \nappropriate measures are in place. Users sho",
        "cleaned_text": "Support Engage Innovate University Guidance on the use of Generative Artificial Intelligence by students and staff, in learning, teaching, and assessment A cademic Year 2024 2025 Authors: Dr Ceri Coulby, Dr Sam Saunders Dr Claire Ellison Date: February 2024 Revised 2024, by Prof David Webster, Gordon Sandison, Rob Lindsay, Dr Sam Saunders, Dr Claire Ellison Richard McKenna Contents 2 Introduction 3 Data Protection Policy and Your Responsibility 4 Generative AI and Copyright 5 Acceptable Uses of GenAI Tools 5 Unacceptable Uses of GenAI Tools 7 Poor Academic Practice 8 Academic Misuse of GenAI Tools 9 Referencing GenAI Tools 9 Further Information Contact 10 Introduction 3 Generative Artificial Intelligence (GenAI) tools are software applications that create content in any form (including but not limited to text, graphs, data, code, images, audio, and video) automatically based on the prompt entered by the user. Examples include, but are not limited to, OpenAI s ChatGPT, Google Bard Gemini, Microsoft s Bing Chat Copilot, DALL-E (and DALL-E2 3), and Perplexity. ai. Artificial intelligence tools are rapidly developing and increasingly becoming embedded into everyday activities across personal and professional contexts. To ensure our graduates have the skills and experience they will need in their future careers, the University of Liverpool seeks to incorporate the technology in their pedagogical approach. However, it is also vital that such technology is used ethically and does not undermine academic integrity principles. To help students and staff to use such technologies appropriately, the University has developed this guidance on acceptable and unacceptable uses of GenAI and AI (Artificial Intelligence) technology and its appropriate citation. This guidance is designed to be applied alongside Appendix L of the Code of Practice on Assessment (CoPA), which supersedes this guidance in regulatory terms. This guidance also serves as the University s default position on usage. If module or programme course component leaders wish students to use GenAI as part of an assessment, students will be informed of this specifically in module materials and assignment briefs. In these cases, specific guidance will be provided in the assignment brief on what constitutes appropriate use of the GenAI tools and how the work from such tools should be cited. The underlying philosophy and or ethos surrounding the use of Generative AI at the University of Liverpool is one of promoting literacy around the technology in both students and staff, and using the technology as openly, honestly, and transparently as possible. This ensures that any engagement with the technology is in line with both the Liverpool Curriculum Framework (LCF) - which promotes Digital Fluency as a Graduate Attribute and Strategy 2031, which states that the university will work to integrate AI into its practice(s). An outright ban on the use of the technology, therefore, contravenes both requirements, in addition to being impossible to enforce. 4 In general, therefore, the guiding principle(s) of the use of Generative AI that should inform all practice is that 1. Both students and staff should openly discuss, experiment with, and engage with the technology in discursive ways where possible, to help improve general understanding of its capabilities, functionalities, limitations, and problems biases. 2. Any use of the technology in either an assessment or any other context should always be declared and evaluated reflected on, and if necessary, cited and referenced. Citations and references to AI should follow the same process as if referencing or citing an academic source. This goes for both staff and students for example, students can be asked to declare their use of GenAI on an assessment cover sheet, while staff should make it clear on their teaching materials where and how GenAI has been used to help create them. 3. The technology is not used as a substitute for original thought, independent research, and the production of original work. Rather, it is used to support these processes. That said, the university recognises that there are some situations where the use of Generative AI is simply unacceptable and will attract sanctions under Appendix L of the Code of Practice on Assessment (CoPA). This guidance provides some steerage on what that unacceptable use of Generative AI looks like and should be consulted in line with the relevant sections of CoPA that deal with academic integrity. Data Protection Policy and Your Responsibility Students and Staff interacting with GAI systems bear responsibilities for ensuring the Universities Data Protection Policy is followed. The UK General Data Protection Regulation (GDPR) and Data Protection Act (DPA) 2018 emphasise the importance of not submitting sensitive or personal data unless necessary and appropriate measures are in place. Users should be aware of the information they provide to AI systems and avoid sharing personal",
        "text_length": 16477
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 48.5,
          "frequency": 48,
          "keywords": [
            "plagiarism",
            "academic",
            "integrity",
            "misconduct",
            "citation"
          ],
          "matches": [
            "academic integrity",
            "original work",
            "academic integrity"
          ],
          "confidence": 100
        },
        {
          "name": "Privacy and Data",
          "score": 31.5,
          "frequency": 31,
          "keywords": [
            "privacy",
            "data",
            "personal",
            "gdpr",
            "protection"
          ],
          "matches": [
            "Data Protection",
            "Data Protection",
            "Data Protection"
          ],
          "confidence": 100
        },
        {
          "name": "Student Guidelines",
          "score": 30.0,
          "frequency": 30,
          "keywords": [
            "student",
            "students",
            "assignment",
            "submission",
            "essay"
          ],
          "matches": [
            "Student Conduct"
          ],
          "confidence": 100
        },
        {
          "name": "AI Technology",
          "score": 20.5,
          "frequency": 20,
          "keywords": [],
          "matches": [],
          "confidence": 100,
          "entities": [
            "Generative Artificial Intelligence",
            "Claire Ellison Date",
            "Claire Ellison Richard"
          ]
        },
        {
          "name": "Faculty Guidelines",
          "score": 8.5,
          "frequency": 8,
          "keywords": [
            "staff",
            "teaching",
            "curriculum",
            "course"
          ],
          "matches": [],
          "confidence": 85
        },
        {
          "name": "Assessment and Evaluation",
          "score": 8.0,
          "frequency": 8,
          "keywords": [
            "assessment"
          ],
          "matches": [],
          "confidence": 80
        },
        {
          "name": "Research and Innovation",
          "score": 5.0,
          "frequency": 5,
          "keywords": [
            "research",
            "innovation",
            "study",
            "experiment"
          ],
          "matches": [],
          "confidence": 50
        },
        {
          "name": "AI Ethics",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "ethical",
            "rights",
            "respect"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsibility"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Bias and Fairness",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "fairness"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 42,
        "method": "hybrid",
        "scores": {
          "Restrictive": 28.916883621860574,
          "Moderate": 37.75839544230978,
          "Permissive": 18.384720935829645
        },
        "reasoning": "Classified as Moderate based on: - 'appropriate' appears 8 time(s) - 'specific' appears 8 time(s) - 'integrity' appears 8 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 45,
            "scores": {
              "Restrictive": 44.099999999999994,
              "Moderate": 60.89999999999999,
              "Permissive": 30.099999999999998
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "cannot",
                  1,
                  2.0
                ],
                [
                  "limited",
                  3,
                  4.5
                ],
                [
                  "penalty",
                  1,
                  2.0
                ],
                [
                  "penalties",
                  3,
                  6.0
                ],
                [
                  "disciplinary",
                  2,
                  4.0
                ],
                [
                  "sanctions",
                  1,
                  2.0
                ],
                [
                  "academic misconduct",
                  4,
                  8.0
                ],
                [
                  "required",
                  3,
                  3.9000000000000004
                ],
                [
                  "must",
                  4,
                  5.2
                ],
                [
                  "necessary",
                  3,
                  3.9000000000000004
                ],
                [
                  "enforce",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  8,
                  12.0
                ],
                [
                  "considered",
                  1,
                  1.5
                ],
                [
                  "conditions",
                  1,
                  1.2
                ],
                [
                  "guidelines",
                  2,
                  2.4
                ],
                [
                  "framework",
                  1,
                  1.2
                ],
                [
                  "principles",
                  1,
                  1.2
                ],
                [
                  "standards",
                  2,
                  2.4
                ],
                [
                  "requirements",
                  2,
                  2.4
                ],
                [
                  "ethical",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  8,
                  10.4
                ],
                [
                  "fairness",
                  1,
                  1.3
                ],
                [
                  "respect",
                  1,
                  1.3
                ],
                [
                  "context",
                  4,
                  5.6
                ],
                [
                  "circumstances",
                  1,
                  1.4
                ],
                [
                  "specific",
                  8,
                  11.2
                ],
                [
                  "particular",
                  2,
                  2.8
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "support",
                  3,
                  5.4
                ],
                [
                  "creative",
                  1,
                  1.5
                ],
                [
                  "improve",
                  1,
                  1.2
                ],
                [
                  "innovation",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  13,
                  13.0
                ],
                [
                  "advice",
                  2,
                  2.0
                ],
                [
                  "support",
                  3,
                  3.0
                ],
                [
                  "resources",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Restrictive",
            "confidence": 61,
            "scores": {
              "Moderate": 0.30459886057744584,
              "Permissive": 0.08118023395741143,
              "Restrictive": 0.6142209054651444
            },
            "method": "ml_based"
          },
          "agreement": false
        }
      },
      "summary": {
        "total_themes": 10,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 42
      },
      "analysis_id": "analysis_99"
    },
    {
      "_id": "analysis_95",
      "user_id": 7,
      "document_id": null,
      "filename": "7_20250709_124942_guidance-on-the-use-of-generative-ai.pdf",
      "analysis_date": "2025-07-09T11:49:43.415185+00:00",
      "text_data": {
        "original_text": "Support | Engage | Innovate\n University Guidance on  the use of\nGenerative Artificial \nIntelligence by \nstudents and staff, in \nlearning, teaching, and\nassessment \nA cademic Year  2024/2025\nAuthors:\nDr Ceri Coulby, Dr Sam Saunders & Dr Claire Ellison | Date: February 2024\nRevised 2024, by Prof David Webster, Gordon Sandison, \nRob Lindsay, Dr Sam Saunders, Dr Claire Ellison &\nRichard McKenna\nContents\n2\nIntroduction 3\nData Protection Policy and Your Responsibility  4\nGenerative AI and Copyright  5\nAcceptable Uses of GenAI Tools  5\nUnacceptable Uses of GenAI Tools  7\nPoor Academic Practice  8\nAcademic Misuse of GenAI Tools  9\nReferencing GenAI Tools 9\nFurther Information/Contact 10\nIntroduction\n3\nGenerative Artificial Intelligence (GenAI) tools are software applications that create content \nin any form (including but not limited to text, graphs, data, code, images, audio, and video) \nautomatically based on the prompt entered by the user. Examples include, but are not \nlimited to, OpenAI\u2019s ChatGPT, Google Bard/Gemini, Microsoft\u2019s Bing Chat/Copilot, DALL-E (and \nDALL-E2/3), and Perplexity.ai. \nArtificial intelligence tools are rapidly developing and increasingly becoming embedded into \neveryday activities across personal and professional contexts. To ensure our graduates have \nthe skills and experience they will need in their future careers, the University of Liverpool seeks \nto incorporate the technology in their pedagogical approach. However, it is also vital that \nsuch technology is used ethically and does not undermine academic integrity principles.  \nTo help students and staff to use such technologies appropriately, the University has \ndeveloped this guidance on acceptable and unacceptable uses of GenAI and AI (Artificial \nIntelligence) technology and its appropriate citation. This guidance is designed to be applied \nalongside Appendix L of the Code of Practice on Assessment (CoPA), which supersedes this \nguidance in regulatory terms. This guidance also serves as the University\u2019s default position \non usage. If module or programme/course component leaders wish students to use GenAI \nas part of an assessment, students will be informed of this specifically in module materials \nand assignment briefs. In these cases, specific guidance will be provided in the assignment \nbrief on what constitutes appropriate use of the GenAI tools and how the work from such tools \nshould be cited. \nThe underlying philosophy and/or ethos surrounding the use of Generative AI at the University \nof Liverpool is one of promoting literacy around the technology in both students and staff, \nand using the technology as openly, honestly, and transparently as possible. This ensures that \nany engagement with the technology is in line with both the Liverpool Curriculum Framework \n(LCF) - which promotes \u2018Digital Fluency\u2019 as a Graduate Attribute \u2013 and Strategy 2031, which \nstates that the university will work to integrate AI into its practice(s). An outright ban on \nthe use of the technology, therefore, contravenes both requirements, in addition to being \nimpossible to enforce.\n4\nIn general, therefore, the guiding principle(s) of the use of Generative AI that should inform all \npractice is that \n1. Both students and staff should openly discuss, experiment with, and engage with the \ntechnology in discursive ways where possible, to help improve general understanding of \nits capabilities, functionalities, limitations, and problems/biases.\n2. Any use of the technology in either an assessment or any other context should always \nbe declared and evaluated/reflected on, and if necessary, cited and referenced. \nCitations and references to AI should follow the same process as if referencing or citing \nan academic source. This goes for both staff and students \u2013 for example, students can \nbe asked to declare their use of GenAI on an assessment cover sheet, while staff should \nmake it clear on their teaching materials where and how GenAI has been used to help \ncreate them.  \n3. The technology is not used as a substitute for original thought, independent research, and \nthe production of original work. Rather, it is used to support these processes.  \nThat said, the university recognises that there are some situations where the use of \nGenerative AI is simply unacceptable and will attract sanctions under Appendix L of the Code \nof Practice on Assessment (CoPA). This guidance provides some steerage on what that \nunacceptable use of Generative AI looks like and should be consulted in line with the relevant \nsections of CoPA that deal with academic integrity.   \nData Protection Policy and Your Responsibility \nStudents and Staff interacting with GAI systems bear responsibilities for ensuring the \nUniversities Data Protection Policy is followed.  \nThe UK General Data Protection Regulation (GDPR) and Data Protection Act (DPA) 2018 \nemphasise the importance of not submitting sensitive or personal data unless necessary and \nappropriate measures are in place. Users sho",
        "cleaned_text": "Support Engage Innovate University Guidance on the use of Generative Artificial Intelligence by students and staff, in learning, teaching, and assessment A cademic Year 2024 2025 Authors: Dr Ceri Coulby, Dr Sam Saunders Dr Claire Ellison Date: February 2024 Revised 2024, by Prof David Webster, Gordon Sandison, Rob Lindsay, Dr Sam Saunders, Dr Claire Ellison Richard McKenna Contents 2 Introduction 3 Data Protection Policy and Your Responsibility 4 Generative AI and Copyright 5 Acceptable Uses of GenAI Tools 5 Unacceptable Uses of GenAI Tools 7 Poor Academic Practice 8 Academic Misuse of GenAI Tools 9 Referencing GenAI Tools 9 Further Information Contact 10 Introduction 3 Generative Artificial Intelligence (GenAI) tools are software applications that create content in any form (including but not limited to text, graphs, data, code, images, audio, and video) automatically based on the prompt entered by the user. Examples include, but are not limited to, OpenAI s ChatGPT, Google Bard Gemini, Microsoft s Bing Chat Copilot, DALL-E (and DALL-E2 3), and Perplexity. ai. Artificial intelligence tools are rapidly developing and increasingly becoming embedded into everyday activities across personal and professional contexts. To ensure our graduates have the skills and experience they will need in their future careers, the University of Liverpool seeks to incorporate the technology in their pedagogical approach. However, it is also vital that such technology is used ethically and does not undermine academic integrity principles. To help students and staff to use such technologies appropriately, the University has developed this guidance on acceptable and unacceptable uses of GenAI and AI (Artificial Intelligence) technology and its appropriate citation. This guidance is designed to be applied alongside Appendix L of the Code of Practice on Assessment (CoPA), which supersedes this guidance in regulatory terms. This guidance also serves as the University s default position on usage. If module or programme course component leaders wish students to use GenAI as part of an assessment, students will be informed of this specifically in module materials and assignment briefs. In these cases, specific guidance will be provided in the assignment brief on what constitutes appropriate use of the GenAI tools and how the work from such tools should be cited. The underlying philosophy and or ethos surrounding the use of Generative AI at the University of Liverpool is one of promoting literacy around the technology in both students and staff, and using the technology as openly, honestly, and transparently as possible. This ensures that any engagement with the technology is in line with both the Liverpool Curriculum Framework (LCF) - which promotes Digital Fluency as a Graduate Attribute and Strategy 2031, which states that the university will work to integrate AI into its practice(s). An outright ban on the use of the technology, therefore, contravenes both requirements, in addition to being impossible to enforce. 4 In general, therefore, the guiding principle(s) of the use of Generative AI that should inform all practice is that 1. Both students and staff should openly discuss, experiment with, and engage with the technology in discursive ways where possible, to help improve general understanding of its capabilities, functionalities, limitations, and problems biases. 2. Any use of the technology in either an assessment or any other context should always be declared and evaluated reflected on, and if necessary, cited and referenced. Citations and references to AI should follow the same process as if referencing or citing an academic source. This goes for both staff and students for example, students can be asked to declare their use of GenAI on an assessment cover sheet, while staff should make it clear on their teaching materials where and how GenAI has been used to help create them. 3. The technology is not used as a substitute for original thought, independent research, and the production of original work. Rather, it is used to support these processes. That said, the university recognises that there are some situations where the use of Generative AI is simply unacceptable and will attract sanctions under Appendix L of the Code of Practice on Assessment (CoPA). This guidance provides some steerage on what that unacceptable use of Generative AI looks like and should be consulted in line with the relevant sections of CoPA that deal with academic integrity. Data Protection Policy and Your Responsibility Students and Staff interacting with GAI systems bear responsibilities for ensuring the Universities Data Protection Policy is followed. The UK General Data Protection Regulation (GDPR) and Data Protection Act (DPA) 2018 emphasise the importance of not submitting sensitive or personal data unless necessary and appropriate measures are in place. Users should be aware of the information they provide to AI systems and avoid sharing personal",
        "text_length": 16477
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 48.5,
          "frequency": 48,
          "keywords": [
            "plagiarism",
            "academic",
            "integrity",
            "misconduct",
            "citation"
          ],
          "matches": [
            "academic integrity",
            "original work",
            "academic integrity"
          ],
          "confidence": 100
        },
        {
          "name": "Privacy and Data",
          "score": 31.5,
          "frequency": 31,
          "keywords": [
            "privacy",
            "data",
            "personal",
            "gdpr",
            "protection"
          ],
          "matches": [
            "Data Protection",
            "Data Protection",
            "Data Protection"
          ],
          "confidence": 100
        },
        {
          "name": "Student Guidelines",
          "score": 30.0,
          "frequency": 30,
          "keywords": [
            "student",
            "students",
            "assignment",
            "submission",
            "essay"
          ],
          "matches": [
            "Student Conduct"
          ],
          "confidence": 100
        },
        {
          "name": "AI Technology",
          "score": 20.5,
          "frequency": 20,
          "keywords": [],
          "matches": [],
          "confidence": 100,
          "entities": [
            "Generative Artificial Intelligence",
            "Claire Ellison Date",
            "Claire Ellison Richard"
          ]
        },
        {
          "name": "Faculty Guidelines",
          "score": 8.5,
          "frequency": 8,
          "keywords": [
            "staff",
            "teaching",
            "curriculum",
            "course"
          ],
          "matches": [],
          "confidence": 85
        },
        {
          "name": "Assessment and Evaluation",
          "score": 8.0,
          "frequency": 8,
          "keywords": [
            "assessment"
          ],
          "matches": [],
          "confidence": 80
        },
        {
          "name": "Research and Innovation",
          "score": 5.0,
          "frequency": 5,
          "keywords": [
            "research",
            "innovation",
            "study",
            "experiment"
          ],
          "matches": [],
          "confidence": 50
        },
        {
          "name": "AI Ethics",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "ethical",
            "rights",
            "respect"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsibility"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Bias and Fairness",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "fairness"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 42,
        "method": "hybrid",
        "scores": {
          "Restrictive": 28.916883621860574,
          "Moderate": 37.75839544230978,
          "Permissive": 18.384720935829645
        },
        "reasoning": "Classified as Moderate based on: - 'appropriate' appears 8 time(s) - 'specific' appears 8 time(s) - 'integrity' appears 8 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 45,
            "scores": {
              "Restrictive": 44.099999999999994,
              "Moderate": 60.89999999999999,
              "Permissive": 30.099999999999998
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "cannot",
                  1,
                  2.0
                ],
                [
                  "limited",
                  3,
                  4.5
                ],
                [
                  "penalty",
                  1,
                  2.0
                ],
                [
                  "penalties",
                  3,
                  6.0
                ],
                [
                  "disciplinary",
                  2,
                  4.0
                ],
                [
                  "sanctions",
                  1,
                  2.0
                ],
                [
                  "academic misconduct",
                  4,
                  8.0
                ],
                [
                  "required",
                  3,
                  3.9000000000000004
                ],
                [
                  "must",
                  4,
                  5.2
                ],
                [
                  "necessary",
                  3,
                  3.9000000000000004
                ],
                [
                  "enforce",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  8,
                  12.0
                ],
                [
                  "considered",
                  1,
                  1.5
                ],
                [
                  "conditions",
                  1,
                  1.2
                ],
                [
                  "guidelines",
                  2,
                  2.4
                ],
                [
                  "framework",
                  1,
                  1.2
                ],
                [
                  "principles",
                  1,
                  1.2
                ],
                [
                  "standards",
                  2,
                  2.4
                ],
                [
                  "requirements",
                  2,
                  2.4
                ],
                [
                  "ethical",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  8,
                  10.4
                ],
                [
                  "fairness",
                  1,
                  1.3
                ],
                [
                  "respect",
                  1,
                  1.3
                ],
                [
                  "context",
                  4,
                  5.6
                ],
                [
                  "circumstances",
                  1,
                  1.4
                ],
                [
                  "specific",
                  8,
                  11.2
                ],
                [
                  "particular",
                  2,
                  2.8
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "support",
                  3,
                  5.4
                ],
                [
                  "creative",
                  1,
                  1.5
                ],
                [
                  "improve",
                  1,
                  1.2
                ],
                [
                  "innovation",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  13,
                  13.0
                ],
                [
                  "advice",
                  2,
                  2.0
                ],
                [
                  "support",
                  3,
                  3.0
                ],
                [
                  "resources",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Restrictive",
            "confidence": 61,
            "scores": {
              "Moderate": 0.30459886057744584,
              "Permissive": 0.08118023395741143,
              "Restrictive": 0.6142209054651444
            },
            "method": "ml_based"
          },
          "agreement": false
        }
      },
      "summary": {
        "total_themes": 10,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 42
      }
    },
    {
      "_id": "analysis_96",
      "user_id": 7,
      "document_id": null,
      "filename": "7_20250709_124942_guidance-on-the-use-of-generative-ai.pdf",
      "analysis_date": "2025-07-09T11:49:53.968859+00:00",
      "text_data": {
        "original_text": "Support | Engage | Innovate\n University Guidance on  the use of\nGenerative Artificial \nIntelligence by \nstudents and staff, in \nlearning, teaching, and\nassessment \nA cademic Year  2024/2025\nAuthors:\nDr Ceri Coulby, Dr Sam Saunders & Dr Claire Ellison | Date: February 2024\nRevised 2024, by Prof David Webster, Gordon Sandison, \nRob Lindsay, Dr Sam Saunders, Dr Claire Ellison &\nRichard McKenna\nContents\n2\nIntroduction 3\nData Protection Policy and Your Responsibility  4\nGenerative AI and Copyright  5\nAcceptable Uses of GenAI Tools  5\nUnacceptable Uses of GenAI Tools  7\nPoor Academic Practice  8\nAcademic Misuse of GenAI Tools  9\nReferencing GenAI Tools 9\nFurther Information/Contact 10\nIntroduction\n3\nGenerative Artificial Intelligence (GenAI) tools are software applications that create content \nin any form (including but not limited to text, graphs, data, code, images, audio, and video) \nautomatically based on the prompt entered by the user. Examples include, but are not \nlimited to, OpenAI\u2019s ChatGPT, Google Bard/Gemini, Microsoft\u2019s Bing Chat/Copilot, DALL-E (and \nDALL-E2/3), and Perplexity.ai. \nArtificial intelligence tools are rapidly developing and increasingly becoming embedded into \neveryday activities across personal and professional contexts. To ensure our graduates have \nthe skills and experience they will need in their future careers, the University of Liverpool seeks \nto incorporate the technology in their pedagogical approach. However, it is also vital that \nsuch technology is used ethically and does not undermine academic integrity principles.  \nTo help students and staff to use such technologies appropriately, the University has \ndeveloped this guidance on acceptable and unacceptable uses of GenAI and AI (Artificial \nIntelligence) technology and its appropriate citation. This guidance is designed to be applied \nalongside Appendix L of the Code of Practice on Assessment (CoPA), which supersedes this \nguidance in regulatory terms. This guidance also serves as the University\u2019s default position \non usage. If module or programme/course component leaders wish students to use GenAI \nas part of an assessment, students will be informed of this specifically in module materials \nand assignment briefs. In these cases, specific guidance will be provided in the assignment \nbrief on what constitutes appropriate use of the GenAI tools and how the work from such tools \nshould be cited. \nThe underlying philosophy and/or ethos surrounding the use of Generative AI at the University \nof Liverpool is one of promoting literacy around the technology in both students and staff, \nand using the technology as openly, honestly, and transparently as possible. This ensures that \nany engagement with the technology is in line with both the Liverpool Curriculum Framework \n(LCF) - which promotes \u2018Digital Fluency\u2019 as a Graduate Attribute \u2013 and Strategy 2031, which \nstates that the university will work to integrate AI into its practice(s). An outright ban on \nthe use of the technology, therefore, contravenes both requirements, in addition to being \nimpossible to enforce.\n4\nIn general, therefore, the guiding principle(s) of the use of Generative AI that should inform all \npractice is that \n1. Both students and staff should openly discuss, experiment with, and engage with the \ntechnology in discursive ways where possible, to help improve general understanding of \nits capabilities, functionalities, limitations, and problems/biases.\n2. Any use of the technology in either an assessment or any other context should always \nbe declared and evaluated/reflected on, and if necessary, cited and referenced. \nCitations and references to AI should follow the same process as if referencing or citing \nan academic source. This goes for both staff and students \u2013 for example, students can \nbe asked to declare their use of GenAI on an assessment cover sheet, while staff should \nmake it clear on their teaching materials where and how GenAI has been used to help \ncreate them.  \n3. The technology is not used as a substitute for original thought, independent research, and \nthe production of original work. Rather, it is used to support these processes.  \nThat said, the university recognises that there are some situations where the use of \nGenerative AI is simply unacceptable and will attract sanctions under Appendix L of the Code \nof Practice on Assessment (CoPA). This guidance provides some steerage on what that \nunacceptable use of Generative AI looks like and should be consulted in line with the relevant \nsections of CoPA that deal with academic integrity.   \nData Protection Policy and Your Responsibility \nStudents and Staff interacting with GAI systems bear responsibilities for ensuring the \nUniversities Data Protection Policy is followed.  \nThe UK General Data Protection Regulation (GDPR) and Data Protection Act (DPA) 2018 \nemphasise the importance of not submitting sensitive or personal data unless necessary and \nappropriate measures are in place. Users sho",
        "cleaned_text": "Support Engage Innovate University Guidance on the use of Generative Artificial Intelligence by students and staff, in learning, teaching, and assessment A cademic Year 2024 2025 Authors: Dr Ceri Coulby, Dr Sam Saunders Dr Claire Ellison Date: February 2024 Revised 2024, by Prof David Webster, Gordon Sandison, Rob Lindsay, Dr Sam Saunders, Dr Claire Ellison Richard McKenna Contents 2 Introduction 3 Data Protection Policy and Your Responsibility 4 Generative AI and Copyright 5 Acceptable Uses of GenAI Tools 5 Unacceptable Uses of GenAI Tools 7 Poor Academic Practice 8 Academic Misuse of GenAI Tools 9 Referencing GenAI Tools 9 Further Information Contact 10 Introduction 3 Generative Artificial Intelligence (GenAI) tools are software applications that create content in any form (including but not limited to text, graphs, data, code, images, audio, and video) automatically based on the prompt entered by the user. Examples include, but are not limited to, OpenAI s ChatGPT, Google Bard Gemini, Microsoft s Bing Chat Copilot, DALL-E (and DALL-E2 3), and Perplexity. ai. Artificial intelligence tools are rapidly developing and increasingly becoming embedded into everyday activities across personal and professional contexts. To ensure our graduates have the skills and experience they will need in their future careers, the University of Liverpool seeks to incorporate the technology in their pedagogical approach. However, it is also vital that such technology is used ethically and does not undermine academic integrity principles. To help students and staff to use such technologies appropriately, the University has developed this guidance on acceptable and unacceptable uses of GenAI and AI (Artificial Intelligence) technology and its appropriate citation. This guidance is designed to be applied alongside Appendix L of the Code of Practice on Assessment (CoPA), which supersedes this guidance in regulatory terms. This guidance also serves as the University s default position on usage. If module or programme course component leaders wish students to use GenAI as part of an assessment, students will be informed of this specifically in module materials and assignment briefs. In these cases, specific guidance will be provided in the assignment brief on what constitutes appropriate use of the GenAI tools and how the work from such tools should be cited. The underlying philosophy and or ethos surrounding the use of Generative AI at the University of Liverpool is one of promoting literacy around the technology in both students and staff, and using the technology as openly, honestly, and transparently as possible. This ensures that any engagement with the technology is in line with both the Liverpool Curriculum Framework (LCF) - which promotes Digital Fluency as a Graduate Attribute and Strategy 2031, which states that the university will work to integrate AI into its practice(s). An outright ban on the use of the technology, therefore, contravenes both requirements, in addition to being impossible to enforce. 4 In general, therefore, the guiding principle(s) of the use of Generative AI that should inform all practice is that 1. Both students and staff should openly discuss, experiment with, and engage with the technology in discursive ways where possible, to help improve general understanding of its capabilities, functionalities, limitations, and problems biases. 2. Any use of the technology in either an assessment or any other context should always be declared and evaluated reflected on, and if necessary, cited and referenced. Citations and references to AI should follow the same process as if referencing or citing an academic source. This goes for both staff and students for example, students can be asked to declare their use of GenAI on an assessment cover sheet, while staff should make it clear on their teaching materials where and how GenAI has been used to help create them. 3. The technology is not used as a substitute for original thought, independent research, and the production of original work. Rather, it is used to support these processes. That said, the university recognises that there are some situations where the use of Generative AI is simply unacceptable and will attract sanctions under Appendix L of the Code of Practice on Assessment (CoPA). This guidance provides some steerage on what that unacceptable use of Generative AI looks like and should be consulted in line with the relevant sections of CoPA that deal with academic integrity. Data Protection Policy and Your Responsibility Students and Staff interacting with GAI systems bear responsibilities for ensuring the Universities Data Protection Policy is followed. The UK General Data Protection Regulation (GDPR) and Data Protection Act (DPA) 2018 emphasise the importance of not submitting sensitive or personal data unless necessary and appropriate measures are in place. Users should be aware of the information they provide to AI systems and avoid sharing personal",
        "text_length": 16477
      },
      "themes": [
        {
          "name": "Academic Integrity",
          "score": 48.5,
          "frequency": 48,
          "keywords": [
            "plagiarism",
            "academic",
            "integrity",
            "misconduct",
            "citation"
          ],
          "matches": [
            "academic integrity",
            "original work",
            "academic integrity"
          ],
          "confidence": 100
        },
        {
          "name": "Privacy and Data",
          "score": 31.5,
          "frequency": 31,
          "keywords": [
            "privacy",
            "data",
            "personal",
            "gdpr",
            "protection"
          ],
          "matches": [
            "Data Protection",
            "Data Protection",
            "Data Protection"
          ],
          "confidence": 100
        },
        {
          "name": "Student Guidelines",
          "score": 30.0,
          "frequency": 30,
          "keywords": [
            "student",
            "students",
            "assignment",
            "submission",
            "essay"
          ],
          "matches": [
            "Student Conduct"
          ],
          "confidence": 100
        },
        {
          "name": "AI Technology",
          "score": 20.5,
          "frequency": 20,
          "keywords": [],
          "matches": [],
          "confidence": 100,
          "entities": [
            "Generative Artificial Intelligence",
            "Claire Ellison Date",
            "Claire Ellison Richard"
          ]
        },
        {
          "name": "Faculty Guidelines",
          "score": 8.5,
          "frequency": 8,
          "keywords": [
            "staff",
            "teaching",
            "curriculum",
            "course"
          ],
          "matches": [],
          "confidence": 85
        },
        {
          "name": "Assessment and Evaluation",
          "score": 8.0,
          "frequency": 8,
          "keywords": [
            "assessment"
          ],
          "matches": [],
          "confidence": 80
        },
        {
          "name": "Research and Innovation",
          "score": 5.0,
          "frequency": 5,
          "keywords": [
            "research",
            "innovation",
            "study",
            "experiment"
          ],
          "matches": [],
          "confidence": 50
        },
        {
          "name": "AI Ethics",
          "score": 2.0,
          "frequency": 2,
          "keywords": [
            "ethical",
            "rights",
            "respect"
          ],
          "matches": [],
          "confidence": 20
        },
        {
          "name": "Accountability",
          "score": 1.0,
          "frequency": 1,
          "keywords": [
            "responsibility"
          ],
          "matches": [],
          "confidence": 10
        },
        {
          "name": "Bias and Fairness",
          "score": 0.5,
          "frequency": 0,
          "keywords": [
            "fairness"
          ],
          "matches": [],
          "confidence": 5
        }
      ],
      "classification": {
        "classification": "Moderate",
        "confidence": 42,
        "method": "hybrid",
        "scores": {
          "Restrictive": 28.916883621860574,
          "Moderate": 37.75839544230978,
          "Permissive": 18.384720935829645
        },
        "reasoning": "Classified as Moderate based on: - 'appropriate' appears 8 time(s) - 'specific' appears 8 time(s) - 'integrity' appears 8 time(s) - Emphasis on responsible use - Structured approach with guidelines",
        "details": {
          "rule_based": {
            "classification": "Moderate",
            "confidence": 45,
            "scores": {
              "Restrictive": 44.099999999999994,
              "Moderate": 60.89999999999999,
              "Permissive": 30.099999999999998
            },
            "keyword_matches": {
              "Restrictive": [
                [
                  "cannot",
                  1,
                  2.0
                ],
                [
                  "limited",
                  3,
                  4.5
                ],
                [
                  "penalty",
                  1,
                  2.0
                ],
                [
                  "penalties",
                  3,
                  6.0
                ],
                [
                  "disciplinary",
                  2,
                  4.0
                ],
                [
                  "sanctions",
                  1,
                  2.0
                ],
                [
                  "academic misconduct",
                  4,
                  8.0
                ],
                [
                  "required",
                  3,
                  3.9000000000000004
                ],
                [
                  "must",
                  4,
                  5.2
                ],
                [
                  "necessary",
                  3,
                  3.9000000000000004
                ],
                [
                  "enforce",
                  1,
                  1.3
                ],
                [
                  "compliance",
                  1,
                  1.3
                ]
              ],
              "Moderate": [
                [
                  "appropriate",
                  8,
                  12.0
                ],
                [
                  "considered",
                  1,
                  1.5
                ],
                [
                  "conditions",
                  1,
                  1.2
                ],
                [
                  "guidelines",
                  2,
                  2.4
                ],
                [
                  "framework",
                  1,
                  1.2
                ],
                [
                  "principles",
                  1,
                  1.2
                ],
                [
                  "standards",
                  2,
                  2.4
                ],
                [
                  "requirements",
                  2,
                  2.4
                ],
                [
                  "ethical",
                  2,
                  2.6
                ],
                [
                  "integrity",
                  8,
                  10.4
                ],
                [
                  "fairness",
                  1,
                  1.3
                ],
                [
                  "respect",
                  1,
                  1.3
                ],
                [
                  "context",
                  4,
                  5.6
                ],
                [
                  "circumstances",
                  1,
                  1.4
                ],
                [
                  "specific",
                  8,
                  11.2
                ],
                [
                  "particular",
                  2,
                  2.8
                ]
              ],
              "Permissive": [
                [
                  "encouraged",
                  1,
                  1.8
                ],
                [
                  "support",
                  3,
                  5.4
                ],
                [
                  "creative",
                  1,
                  1.5
                ],
                [
                  "improve",
                  1,
                  1.2
                ],
                [
                  "innovation",
                  1,
                  1.2
                ],
                [
                  "guidance",
                  13,
                  13.0
                ],
                [
                  "advice",
                  2,
                  2.0
                ],
                [
                  "support",
                  3,
                  3.0
                ],
                [
                  "resources",
                  1,
                  1.0
                ]
              ]
            },
            "method": "rule_based"
          },
          "ml_based": {
            "classification": "Restrictive",
            "confidence": 61,
            "scores": {
              "Moderate": 0.30459886057744584,
              "Permissive": 0.08118023395741143,
              "Restrictive": 0.6142209054651444
            },
            "method": "ml_based"
          },
          "agreement": false
        }
      },
      "summary": {
        "total_themes": 10,
        "top_theme": "Academic Integrity",
        "classification_type": "Moderate",
        "confidence": 42
      }
    }
  ],
  "recommendations": [
    {
      "_id": "rec_1",
      "user_id": 5,
      "analysis_id": "analysis_3",
      "recommendations": [
        {
          "title": "Strengthen Existing Governance with Formal Accountability Metrics",
          "description": "Enhance current governance structures by implementing quantitative accountability measures, establishing clear performance indicators for policy effectiveness, and creating systematic feedback loops from stakeholder communities.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "enhancement",
          "timeframe": "1-3 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Develop KPIs for current governance structure effectiveness",
            "Implement quarterly stakeholder satisfaction surveys",
            "Create public transparency reports on governance activities and outcomes",
            "Establish formal escalation procedures for unresolved policy concerns"
          ],
          "success_metrics": [
            "KPI dashboard operational",
            "Stakeholder satisfaction >75%",
            "Response time to concerns <48 hours"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist",
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Strengthen Existing Human Oversight with Systematic Review Processes",
          "description": "Enhance current human oversight practices by implementing systematic review processes, establishing clear decision-making hierarchies, and creating documentation requirements for AI-assisted decisions.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "enhancement",
          "timeframe": "1-2 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Audit existing oversight practices for completeness and effectiveness",
            "Implement standardised documentation requirements for AI-assisted decisions",
            "Create clear escalation procedures for complex or ambiguous cases",
            "Establish quarterly review meetings to assess oversight effectiveness"
          ],
          "success_metrics": [
            "Oversight audit completed",
            "Documentation compliance >95%",
            "Escalation procedures tested and functional"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Enhance Expand Existing Accessibility Measures with Cultural Competency",
          "description": "Enhance current accessibility practices by incorporating cultural competency considerations, addressing diverse learning styles and preferences, and ensuring AI implementations respect varied cultural and linguistic backgrounds.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "enhancement",
          "timeframe": "2-4 months",
          "impact_score": 66.57600000000001,
          "current_score": 3.8,
          "expected_improvement": "Significant improvement expected: 3.8% \u2192 57.7%",
          "implementation_steps": [
            "Review existing accessibility measures for cultural inclusivity gaps",
            "Engage diverse student focus groups to identify additional needs",
            "Develop cultural competency guidelines for AI tool selection and implementation",
            "Create ongoing assessment mechanism for inclusive practice effectiveness"
          ],
          "success_metrics": [
            "Cultural inclusivity review completed",
            "Focus group recommendations implemented",
            "Assessment mechanism operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Barnes & Hutson (2024) - Mitigating Bias in Educational AI",
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Enhance Expand Existing Disclosure Requirements with Methodological Detail",
          "description": "Enhance current disclosure practices by requiring detailed documentation of AI methodologies, decision-making processes, and outcome validation procedures. Focus on academic integrity and reproducibility standards.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "enhancement",
          "timeframe": "1-2 months",
          "impact_score": 64.704,
          "current_score": 7.7,
          "expected_improvement": "Significant improvement expected: 7.7% \u2192 60.4%",
          "implementation_steps": [
            "Review and strengthen existing disclosure language for comprehensiveness",
            "Add requirements for AI methodology documentation in research contexts",
            "Create detailed examples and case studies for common disclosure scenarios",
            "Implement compliance monitoring through existing academic integrity mechanisms"
          ],
          "success_metrics": [
            "Enhanced disclosure guidelines published",
            "Compliance monitoring system operational",
            "Faculty feedback rating >4/5"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning",
            "Williamson & Eynon (2020) - AI in Education Historical Analysis"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-06-18T19:31:56.841331",
      "total_recommendations": 4
    },
    {
      "_id": "rec_2",
      "user_id": 3,
      "analysis_id": "analysis_14",
      "recommendations": [
        {
          "title": "Implement Graduated Human Oversight Protocol for Research AI",
          "description": "Establish risk-based human oversight framework requiring different levels of human control based on AI application complexity, research sensitivity, and potential impact. Protocol should preserve human authority in critical research decisions while enabling efficient AI integration.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-5 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Develop risk assessment matrix categorising AI applications by oversight requirements",
            "Create standard operating procedures for each oversight level",
            "Train research supervisors on appropriate oversight implementation",
            "Establish periodic review cycle for oversight level adjustments"
          ],
          "success_metrics": [
            "Risk matrix approved and implemented",
            "All active research projects classified",
            "Supervisor training >90% completion"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Comprehensive AI Accessibility Standards",
          "description": "Implement institution-wide accessibility requirements for all AI tools and platforms, ensuring compliance with disability rights legislation and promoting equitable access for students and faculty with diverse needs and capabilities.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Conduct accessibility audit of current AI tools and platforms",
            "Develop procurement requirements including accessibility criteria for new AI tools",
            "Create accommodation procedures for students unable to use standard AI tools",
            "Implement regular accessibility testing and compliance monitoring"
          ],
          "success_metrics": [
            "Accessibility audit completed for all AI tools",
            "Procurement standards updated",
            "Accommodation procedures operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist",
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Barnes & Hutson (2024) - Mitigating Bias in Educational AI",
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Develop Comprehensive AI Research Disclosure Framework",
          "description": "Create detailed disclosure requirements for AI use in research publications, grant applications, and academic presentations. Framework should include methodology transparency, dataset acknowledgment, and limitations documentation to maintain research integrity.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 64.704,
          "current_score": 7.7,
          "expected_improvement": "Significant improvement expected: 7.7% \u2192 60.4%",
          "implementation_steps": [
            "Draft disclosure templates for different research publication types",
            "Integrate disclosure requirements into institutional publication guidelines",
            "Provide training for researchers on proper AI methodology documentation",
            "Create review checklist for research integrity office and journal submissions"
          ],
          "success_metrics": [
            "Disclosure templates adopted by all departments",
            "100% compliance in new publications",
            "Training completed by >80% research-active faculty"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Professional development specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning",
            "Williamson & Eynon (2020) - AI in Education Historical Analysis"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-06-26T17:18:41.813620",
      "total_recommendations": 4
    },
    {
      "_id": "rec_3",
      "user_id": 3,
      "analysis_id": "analysis_15",
      "recommendations": [
        {
          "title": "Implement Graduated Human Oversight Protocol for Research AI",
          "description": "Establish risk-based human oversight framework requiring different levels of human control based on AI application complexity, research sensitivity, and potential impact. Protocol should preserve human authority in critical research decisions while enabling efficient AI integration.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-5 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Develop risk assessment matrix categorising AI applications by oversight requirements",
            "Create standard operating procedures for each oversight level",
            "Train research supervisors on appropriate oversight implementation",
            "Establish periodic review cycle for oversight level adjustments"
          ],
          "success_metrics": [
            "Risk matrix approved and implemented",
            "All active research projects classified",
            "Supervisor training >90% completion"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Comprehensive AI Accessibility Standards",
          "description": "Implement institution-wide accessibility requirements for all AI tools and platforms, ensuring compliance with disability rights legislation and promoting equitable access for students and faculty with diverse needs and capabilities.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Conduct accessibility audit of current AI tools and platforms",
            "Develop procurement requirements including accessibility criteria for new AI tools",
            "Create accommodation procedures for students unable to use standard AI tools",
            "Implement regular accessibility testing and compliance monitoring"
          ],
          "success_metrics": [
            "Accessibility audit completed for all AI tools",
            "Procurement standards updated",
            "Accommodation procedures operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist",
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Barnes & Hutson (2024) - Mitigating Bias in Educational AI",
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Develop Comprehensive AI Research Disclosure Framework",
          "description": "Create detailed disclosure requirements for AI use in research publications, grant applications, and academic presentations. Framework should include methodology transparency, dataset acknowledgment, and limitations documentation to maintain research integrity.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 64.704,
          "current_score": 7.7,
          "expected_improvement": "Significant improvement expected: 7.7% \u2192 60.4%",
          "implementation_steps": [
            "Draft disclosure templates for different research publication types",
            "Integrate disclosure requirements into institutional publication guidelines",
            "Provide training for researchers on proper AI methodology documentation",
            "Create review checklist for research integrity office and journal submissions"
          ],
          "success_metrics": [
            "Disclosure templates adopted by all departments",
            "100% compliance in new publications",
            "Training completed by >80% research-active faculty"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Professional development specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning",
            "Williamson & Eynon (2020) - AI in Education Historical Analysis"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-06-26T17:36:12.427166",
      "total_recommendations": 4
    },
    {
      "_id": "rec_4",
      "user_id": 3,
      "analysis_id": "analysis_15",
      "recommendations": [
        {
          "title": "Implement Graduated Human Oversight Protocol for Research AI",
          "description": "Establish risk-based human oversight framework requiring different levels of human control based on AI application complexity, research sensitivity, and potential impact. Protocol should preserve human authority in critical research decisions while enabling efficient AI integration.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-5 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Develop risk assessment matrix categorising AI applications by oversight requirements",
            "Create standard operating procedures for each oversight level",
            "Train research supervisors on appropriate oversight implementation",
            "Establish periodic review cycle for oversight level adjustments"
          ],
          "success_metrics": [
            "Risk matrix approved and implemented",
            "All active research projects classified",
            "Supervisor training >90% completion"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Comprehensive AI Accessibility Standards",
          "description": "Implement institution-wide accessibility requirements for all AI tools and platforms, ensuring compliance with disability rights legislation and promoting equitable access for students and faculty with diverse needs and capabilities.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Conduct accessibility audit of current AI tools and platforms",
            "Develop procurement requirements including accessibility criteria for new AI tools",
            "Create accommodation procedures for students unable to use standard AI tools",
            "Implement regular accessibility testing and compliance monitoring"
          ],
          "success_metrics": [
            "Accessibility audit completed for all AI tools",
            "Procurement standards updated",
            "Accommodation procedures operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist",
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Barnes & Hutson (2024) - Mitigating Bias in Educational AI",
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Develop Comprehensive AI Research Disclosure Framework",
          "description": "Create detailed disclosure requirements for AI use in research publications, grant applications, and academic presentations. Framework should include methodology transparency, dataset acknowledgment, and limitations documentation to maintain research integrity.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 64.704,
          "current_score": 7.7,
          "expected_improvement": "Significant improvement expected: 7.7% \u2192 60.4%",
          "implementation_steps": [
            "Draft disclosure templates for different research publication types",
            "Integrate disclosure requirements into institutional publication guidelines",
            "Provide training for researchers on proper AI methodology documentation",
            "Create review checklist for research integrity office and journal submissions"
          ],
          "success_metrics": [
            "Disclosure templates adopted by all departments",
            "100% compliance in new publications",
            "Training completed by >80% research-active faculty"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Professional development specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning",
            "Williamson & Eynon (2020) - AI in Education Historical Analysis"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-06-26T17:46:52.438931",
      "total_recommendations": 4
    },
    {
      "_id": "rec_5",
      "user_id": 3,
      "analysis_id": "analysis_15",
      "recommendations": [
        {
          "title": "Implement Graduated Human Oversight Protocol for Research AI",
          "description": "Establish risk-based human oversight framework requiring different levels of human control based on AI application complexity, research sensitivity, and potential impact. Protocol should preserve human authority in critical research decisions while enabling efficient AI integration.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-5 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Develop risk assessment matrix categorising AI applications by oversight requirements",
            "Create standard operating procedures for each oversight level",
            "Train research supervisors on appropriate oversight implementation",
            "Establish periodic review cycle for oversight level adjustments"
          ],
          "success_metrics": [
            "Risk matrix approved and implemented",
            "All active research projects classified",
            "Supervisor training >90% completion"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Comprehensive AI Accessibility Standards",
          "description": "Implement institution-wide accessibility requirements for all AI tools and platforms, ensuring compliance with disability rights legislation and promoting equitable access for students and faculty with diverse needs and capabilities.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Conduct accessibility audit of current AI tools and platforms",
            "Develop procurement requirements including accessibility criteria for new AI tools",
            "Create accommodation procedures for students unable to use standard AI tools",
            "Implement regular accessibility testing and compliance monitoring"
          ],
          "success_metrics": [
            "Accessibility audit completed for all AI tools",
            "Procurement standards updated",
            "Accommodation procedures operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist",
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Barnes & Hutson (2024) - Mitigating Bias in Educational AI",
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Develop Comprehensive AI Research Disclosure Framework",
          "description": "Create detailed disclosure requirements for AI use in research publications, grant applications, and academic presentations. Framework should include methodology transparency, dataset acknowledgment, and limitations documentation to maintain research integrity.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 64.704,
          "current_score": 7.7,
          "expected_improvement": "Significant improvement expected: 7.7% \u2192 60.4%",
          "implementation_steps": [
            "Draft disclosure templates for different research publication types",
            "Integrate disclosure requirements into institutional publication guidelines",
            "Provide training for researchers on proper AI methodology documentation",
            "Create review checklist for research integrity office and journal submissions"
          ],
          "success_metrics": [
            "Disclosure templates adopted by all departments",
            "100% compliance in new publications",
            "Training completed by >80% research-active faculty"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Professional development specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning",
            "Williamson & Eynon (2020) - AI in Education Historical Analysis"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-06-26T17:46:53.730071",
      "total_recommendations": 4
    },
    {
      "_id": "rec_6",
      "user_id": 3,
      "analysis_id": "analysis_25",
      "recommendations": [
        {
          "title": "Implement Graduated Human Oversight Protocol for Research AI",
          "description": "Establish risk-based human oversight framework requiring different levels of human control based on AI application complexity, research sensitivity, and potential impact. Protocol should preserve human authority in critical research decisions while enabling efficient AI integration.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-5 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Develop risk assessment matrix categorising AI applications by oversight requirements",
            "Create standard operating procedures for each oversight level",
            "Train research supervisors on appropriate oversight implementation",
            "Establish periodic review cycle for oversight level adjustments"
          ],
          "success_metrics": [
            "Risk matrix approved and implemented",
            "All active research projects classified",
            "Supervisor training >90% completion"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Comprehensive AI Accessibility Standards",
          "description": "Implement institution-wide accessibility requirements for all AI tools and platforms, ensuring compliance with disability rights legislation and promoting equitable access for students and faculty with diverse needs and capabilities.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Conduct accessibility audit of current AI tools and platforms",
            "Develop procurement requirements including accessibility criteria for new AI tools",
            "Create accommodation procedures for students unable to use standard AI tools",
            "Implement regular accessibility testing and compliance monitoring"
          ],
          "success_metrics": [
            "Accessibility audit completed for all AI tools",
            "Procurement standards updated",
            "Accommodation procedures operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist",
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Barnes & Hutson (2024) - Mitigating Bias in Educational AI",
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Develop Comprehensive AI Research Disclosure Framework",
          "description": "Create detailed disclosure requirements for AI use in research publications, grant applications, and academic presentations. Framework should include methodology transparency, dataset acknowledgment, and limitations documentation to maintain research integrity.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 64.704,
          "current_score": 7.7,
          "expected_improvement": "Significant improvement expected: 7.7% \u2192 60.4%",
          "implementation_steps": [
            "Draft disclosure templates for different research publication types",
            "Integrate disclosure requirements into institutional publication guidelines",
            "Provide training for researchers on proper AI methodology documentation",
            "Create review checklist for research integrity office and journal submissions"
          ],
          "success_metrics": [
            "Disclosure templates adopted by all departments",
            "100% compliance in new publications",
            "Training completed by >80% research-active faculty"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Professional development specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning",
            "Williamson & Eynon (2020) - AI in Education Historical Analysis"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-06-27T10:18:47.760147",
      "total_recommendations": 4
    },
    {
      "_id": "rec_7",
      "user_id": 3,
      "analysis_id": "analysis_26",
      "recommendations": [
        {
          "title": "Implement Graduated Human Oversight Protocol for Research AI",
          "description": "Establish risk-based human oversight framework requiring different levels of human control based on AI application complexity, research sensitivity, and potential impact. Protocol should preserve human authority in critical research decisions while enabling efficient AI integration.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-5 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Develop risk assessment matrix categorising AI applications by oversight requirements",
            "Create standard operating procedures for each oversight level",
            "Train research supervisors on appropriate oversight implementation",
            "Establish periodic review cycle for oversight level adjustments"
          ],
          "success_metrics": [
            "Risk matrix approved and implemented",
            "All active research projects classified",
            "Supervisor training >90% completion"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Comprehensive AI Accessibility Standards",
          "description": "Implement institution-wide accessibility requirements for all AI tools and platforms, ensuring compliance with disability rights legislation and promoting equitable access for students and faculty with diverse needs and capabilities.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Conduct accessibility audit of current AI tools and platforms",
            "Develop procurement requirements including accessibility criteria for new AI tools",
            "Create accommodation procedures for students unable to use standard AI tools",
            "Implement regular accessibility testing and compliance monitoring"
          ],
          "success_metrics": [
            "Accessibility audit completed for all AI tools",
            "Procurement standards updated",
            "Accommodation procedures operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist",
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Barnes & Hutson (2024) - Mitigating Bias in Educational AI",
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Develop Comprehensive AI Research Disclosure Framework",
          "description": "Create detailed disclosure requirements for AI use in research publications, grant applications, and academic presentations. Framework should include methodology transparency, dataset acknowledgment, and limitations documentation to maintain research integrity.",
          "dimension": "transparency",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 66.19200000000001,
          "current_score": 4.6,
          "expected_improvement": "Significant improvement expected: 4.6% \u2192 58.2%",
          "implementation_steps": [
            "Draft disclosure templates for different research publication types",
            "Integrate disclosure requirements into institutional publication guidelines",
            "Provide training for researchers on proper AI methodology documentation",
            "Create review checklist for research integrity office and journal submissions"
          ],
          "success_metrics": [
            "Disclosure templates adopted by all departments",
            "100% compliance in new publications",
            "Training completed by >80% research-active faculty"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Professional development specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning",
            "Williamson & Eynon (2020) - AI in Education Historical Analysis"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-06-27T10:19:35.692621",
      "total_recommendations": 4
    },
    {
      "_id": "rec_8",
      "user_id": 3,
      "analysis_id": "analysis_42",
      "recommendations": [
        {
          "title": "Implement Graduated Human Oversight Protocol for Research AI",
          "description": "Establish risk-based human oversight framework requiring different levels of human control based on AI application complexity, research sensitivity, and potential impact. Protocol should preserve human authority in critical research decisions while enabling efficient AI integration.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-5 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Develop risk assessment matrix categorising AI applications by oversight requirements",
            "Create standard operating procedures for each oversight level",
            "Train research supervisors on appropriate oversight implementation",
            "Establish periodic review cycle for oversight level adjustments"
          ],
          "success_metrics": [
            "Risk matrix approved and implemented",
            "All active research projects classified",
            "Supervisor training >90% completion"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Comprehensive AI Accessibility Standards",
          "description": "Implement institution-wide accessibility requirements for all AI tools and platforms, ensuring compliance with disability rights legislation and promoting equitable access for students and faculty with diverse needs and capabilities.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Conduct accessibility audit of current AI tools and platforms",
            "Develop procurement requirements including accessibility criteria for new AI tools",
            "Create accommodation procedures for students unable to use standard AI tools",
            "Implement regular accessibility testing and compliance monitoring"
          ],
          "success_metrics": [
            "Accessibility audit completed for all AI tools",
            "Procurement standards updated",
            "Accommodation procedures operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist",
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Barnes & Hutson (2024) - Mitigating Bias in Educational AI",
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Develop Comprehensive AI Research Disclosure Framework",
          "description": "Create detailed disclosure requirements for AI use in research publications, grant applications, and academic presentations. Framework should include methodology transparency, dataset acknowledgment, and limitations documentation to maintain research integrity.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 64.704,
          "current_score": 7.7,
          "expected_improvement": "Significant improvement expected: 7.7% \u2192 60.4%",
          "implementation_steps": [
            "Draft disclosure templates for different research publication types",
            "Integrate disclosure requirements into institutional publication guidelines",
            "Provide training for researchers on proper AI methodology documentation",
            "Create review checklist for research integrity office and journal submissions"
          ],
          "success_metrics": [
            "Disclosure templates adopted by all departments",
            "100% compliance in new publications",
            "Training completed by >80% research-active faculty"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Professional development specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning",
            "Williamson & Eynon (2020) - AI in Education Historical Analysis"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-06-27T11:57:55.898674",
      "total_recommendations": 4
    },
    {
      "_id": "rec_9",
      "user_id": 3,
      "analysis_id": "analysis_42",
      "recommendations": [
        {
          "title": "Implement Graduated Human Oversight Protocol for Research AI",
          "description": "Establish risk-based human oversight framework requiring different levels of human control based on AI application complexity, research sensitivity, and potential impact. Protocol should preserve human authority in critical research decisions while enabling efficient AI integration.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-5 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Develop risk assessment matrix categorising AI applications by oversight requirements",
            "Create standard operating procedures for each oversight level",
            "Train research supervisors on appropriate oversight implementation",
            "Establish periodic review cycle for oversight level adjustments"
          ],
          "success_metrics": [
            "Risk matrix approved and implemented",
            "All active research projects classified",
            "Supervisor training >90% completion"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Comprehensive AI Accessibility Standards",
          "description": "Implement institution-wide accessibility requirements for all AI tools and platforms, ensuring compliance with disability rights legislation and promoting equitable access for students and faculty with diverse needs and capabilities.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Conduct accessibility audit of current AI tools and platforms",
            "Develop procurement requirements including accessibility criteria for new AI tools",
            "Create accommodation procedures for students unable to use standard AI tools",
            "Implement regular accessibility testing and compliance monitoring"
          ],
          "success_metrics": [
            "Accessibility audit completed for all AI tools",
            "Procurement standards updated",
            "Accommodation procedures operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist",
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Barnes & Hutson (2024) - Mitigating Bias in Educational AI",
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Develop Comprehensive AI Research Disclosure Framework",
          "description": "Create detailed disclosure requirements for AI use in research publications, grant applications, and academic presentations. Framework should include methodology transparency, dataset acknowledgment, and limitations documentation to maintain research integrity.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 64.704,
          "current_score": 7.7,
          "expected_improvement": "Significant improvement expected: 7.7% \u2192 60.4%",
          "implementation_steps": [
            "Draft disclosure templates for different research publication types",
            "Integrate disclosure requirements into institutional publication guidelines",
            "Provide training for researchers on proper AI methodology documentation",
            "Create review checklist for research integrity office and journal submissions"
          ],
          "success_metrics": [
            "Disclosure templates adopted by all departments",
            "100% compliance in new publications",
            "Training completed by >80% research-active faculty"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Professional development specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning",
            "Williamson & Eynon (2020) - AI in Education Historical Analysis"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-06-27T11:59:50.798124",
      "total_recommendations": 4
    },
    {
      "_id": "rec_10",
      "user_id": 3,
      "analysis_id": "analysis_52",
      "recommendations": [
        {
          "title": "Implement Graduated Human Oversight Protocol for Research AI",
          "description": "Establish risk-based human oversight framework requiring different levels of human control based on AI application complexity, research sensitivity, and potential impact. Protocol should preserve human authority in critical research decisions while enabling efficient AI integration.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-5 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Develop risk assessment matrix categorising AI applications by oversight requirements",
            "Create standard operating procedures for each oversight level",
            "Train research supervisors on appropriate oversight implementation",
            "Establish periodic review cycle for oversight level adjustments"
          ],
          "success_metrics": [
            "Risk matrix approved and implemented",
            "All active research projects classified",
            "Supervisor training >90% completion"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Comprehensive AI Accessibility Standards",
          "description": "Implement institution-wide accessibility requirements for all AI tools and platforms, ensuring compliance with disability rights legislation and promoting equitable access for students and faculty with diverse needs and capabilities.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Conduct accessibility audit of current AI tools and platforms",
            "Develop procurement requirements including accessibility criteria for new AI tools",
            "Create accommodation procedures for students unable to use standard AI tools",
            "Implement regular accessibility testing and compliance monitoring"
          ],
          "success_metrics": [
            "Accessibility audit completed for all AI tools",
            "Procurement standards updated",
            "Accommodation procedures operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist",
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Barnes & Hutson (2024) - Mitigating Bias in Educational AI",
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Develop Comprehensive AI Research Disclosure Framework",
          "description": "Create detailed disclosure requirements for AI use in research publications, grant applications, and academic presentations. Framework should include methodology transparency, dataset acknowledgment, and limitations documentation to maintain research integrity.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 64.704,
          "current_score": 7.7,
          "expected_improvement": "Significant improvement expected: 7.7% \u2192 60.4%",
          "implementation_steps": [
            "Draft disclosure templates for different research publication types",
            "Integrate disclosure requirements into institutional publication guidelines",
            "Provide training for researchers on proper AI methodology documentation",
            "Create review checklist for research integrity office and journal submissions"
          ],
          "success_metrics": [
            "Disclosure templates adopted by all departments",
            "100% compliance in new publications",
            "Training completed by >80% research-active faculty"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Professional development specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning",
            "Williamson & Eynon (2020) - AI in Education Historical Analysis"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-06-27T12:53:51.225303",
      "total_recommendations": 4
    },
    {
      "_id": "rec_11",
      "user_id": 3,
      "analysis_id": "analysis_54",
      "recommendations": [
        {
          "title": "Implement Graduated Human Oversight Protocol for Research AI",
          "description": "Establish risk-based human oversight framework requiring different levels of human control based on AI application complexity, research sensitivity, and potential impact. Protocol should preserve human authority in critical research decisions while enabling efficient AI integration.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-5 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Develop risk assessment matrix categorising AI applications by oversight requirements",
            "Create standard operating procedures for each oversight level",
            "Train research supervisors on appropriate oversight implementation",
            "Establish periodic review cycle for oversight level adjustments"
          ],
          "success_metrics": [
            "Risk matrix approved and implemented",
            "All active research projects classified",
            "Supervisor training >90% completion"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Comprehensive AI Accessibility Standards",
          "description": "Implement institution-wide accessibility requirements for all AI tools and platforms, ensuring compliance with disability rights legislation and promoting equitable access for students and faculty with diverse needs and capabilities.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Conduct accessibility audit of current AI tools and platforms",
            "Develop procurement requirements including accessibility criteria for new AI tools",
            "Create accommodation procedures for students unable to use standard AI tools",
            "Implement regular accessibility testing and compliance monitoring"
          ],
          "success_metrics": [
            "Accessibility audit completed for all AI tools",
            "Procurement standards updated",
            "Accommodation procedures operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist",
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Barnes & Hutson (2024) - Mitigating Bias in Educational AI",
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Develop Comprehensive AI Research Disclosure Framework",
          "description": "Create detailed disclosure requirements for AI use in research publications, grant applications, and academic presentations. Framework should include methodology transparency, dataset acknowledgment, and limitations documentation to maintain research integrity.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "new_implementation",
          "timeframe": "3-6 months",
          "impact_score": 64.704,
          "current_score": 7.7,
          "expected_improvement": "Significant improvement expected: 7.7% \u2192 60.4%",
          "implementation_steps": [
            "Draft disclosure templates for different research publication types",
            "Integrate disclosure requirements into institutional publication guidelines",
            "Provide training for researchers on proper AI methodology documentation",
            "Create review checklist for research integrity office and journal submissions"
          ],
          "success_metrics": [
            "Disclosure templates adopted by all departments",
            "100% compliance in new publications",
            "Training completed by >80% research-active faculty"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Professional development specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning",
            "Williamson & Eynon (2020) - AI in Education Historical Analysis"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Strong - Research-based",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-06-27T13:18:51.304011",
      "total_recommendations": 4
    },
    {
      "_id": "rec_12",
      "user_id": 7,
      "analysis_id": "analysis_97",
      "recommendations": [
        {
          "title": "Strengthen Existing Human Oversight with Systematic Review Processes",
          "description": "Enhance current human oversight practices by implementing systematic review processes, establishing clear decision-making hierarchies, and creating documentation requirements for AI-assisted decisions.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "enhancement",
          "timeframe": "1-2 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Audit existing oversight practices for completeness and effectiveness",
            "Implement standardised documentation requirements for AI-assisted decisions",
            "Create clear escalation procedures for complex or ambiguous cases",
            "Establish quarterly review meetings to assess oversight effectiveness"
          ],
          "success_metrics": [
            "Oversight audit completed",
            "Documentation compliance >95%",
            "Escalation procedures tested and functional"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Enhance Expand Existing Accessibility Measures with Cultural Competency",
          "description": "Enhance current accessibility practices by incorporating cultural competency considerations, addressing diverse learning styles and preferences, and ensuring AI implementations respect varied cultural and linguistic backgrounds.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "enhancement",
          "timeframe": "2-4 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Review existing accessibility measures for cultural inclusivity gaps",
            "Engage diverse student focus groups to identify additional needs",
            "Develop cultural competency guidelines for AI tool selection and implementation",
            "Create ongoing assessment mechanism for inclusive practice effectiveness"
          ],
          "success_metrics": [
            "Cultural inclusivity review completed",
            "Focus group recommendations implemented",
            "Assessment mechanism operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Enhance Expand Existing Disclosure Requirements with Methodological Detail",
          "description": "Enhance current disclosure practices by requiring detailed documentation of AI methodologies, decision-making processes, and outcome validation procedures. Focus on academic integrity and reproducibility standards.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "enhancement",
          "timeframe": "1-2 months",
          "impact_score": 62.495999999999995,
          "current_score": 12.3,
          "expected_improvement": "Significant improvement expected: 12.3% \u2192 63.6%",
          "implementation_steps": [
            "Review and strengthen existing disclosure language for comprehensiveness",
            "Add requirements for AI methodology documentation in research contexts",
            "Create detailed examples and case studies for common disclosure scenarios",
            "Implement compliance monitoring through existing academic integrity mechanisms"
          ],
          "success_metrics": [
            "Enhanced disclosure guidelines published",
            "Compliance monitoring system operational",
            "Faculty feedback rating >4/5"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-07-08T15:35:36.495427+00:00",
      "total_recommendations": 4
    },
    {
      "_id": "rec_13",
      "user_id": 7,
      "analysis_id": "analysis_100",
      "recommendations": [
        {
          "title": "Strengthen Existing Human Oversight with Systematic Review Processes",
          "description": "Enhance current human oversight practices by implementing systematic review processes, establishing clear decision-making hierarchies, and creating documentation requirements for AI-assisted decisions.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "enhancement",
          "timeframe": "1-2 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Audit existing oversight practices for completeness and effectiveness",
            "Implement standardised documentation requirements for AI-assisted decisions",
            "Create clear escalation procedures for complex or ambiguous cases",
            "Establish quarterly review meetings to assess oversight effectiveness"
          ],
          "success_metrics": [
            "Oversight audit completed",
            "Documentation compliance >95%",
            "Escalation procedures tested and functional"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Enhance Expand Existing Accessibility Measures with Cultural Competency",
          "description": "Enhance current accessibility practices by incorporating cultural competency considerations, addressing diverse learning styles and preferences, and ensuring AI implementations respect varied cultural and linguistic backgrounds.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "enhancement",
          "timeframe": "2-4 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Review existing accessibility measures for cultural inclusivity gaps",
            "Engage diverse student focus groups to identify additional needs",
            "Develop cultural competency guidelines for AI tool selection and implementation",
            "Create ongoing assessment mechanism for inclusive practice effectiveness"
          ],
          "success_metrics": [
            "Cultural inclusivity review completed",
            "Focus group recommendations implemented",
            "Assessment mechanism operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Enhance Expand Existing Disclosure Requirements with Methodological Detail",
          "description": "Enhance current disclosure practices by requiring detailed documentation of AI methodologies, decision-making processes, and outcome validation procedures. Focus on academic integrity and reproducibility standards.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "enhancement",
          "timeframe": "1-2 months",
          "impact_score": 62.495999999999995,
          "current_score": 12.3,
          "expected_improvement": "Significant improvement expected: 12.3% \u2192 63.6%",
          "implementation_steps": [
            "Review and strengthen existing disclosure language for comprehensiveness",
            "Add requirements for AI methodology documentation in research contexts",
            "Create detailed examples and case studies for common disclosure scenarios",
            "Implement compliance monitoring through existing academic integrity mechanisms"
          ],
          "success_metrics": [
            "Enhanced disclosure guidelines published",
            "Compliance monitoring system operational",
            "Faculty feedback rating >4/5"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-07-08T16:48:49.134370+00:00",
      "total_recommendations": 4
    },
    {
      "_id": "rec_14",
      "user_id": 7,
      "analysis_id": "analysis_101",
      "recommendations": [
        {
          "title": "Strengthen Existing Human Oversight with Systematic Review Processes",
          "description": "Enhance current human oversight practices by implementing systematic review processes, establishing clear decision-making hierarchies, and creating documentation requirements for AI-assisted decisions.",
          "dimension": "human_agency",
          "priority": "critical",
          "implementation_type": "enhancement",
          "timeframe": "1-2 months",
          "impact_score": 68.39999999999999,
          "current_score": 0.0,
          "expected_improvement": "Significant improvement expected: 0.0% \u2192 55.0%",
          "implementation_steps": [
            "Audit existing oversight practices for completeness and effectiveness",
            "Implement standardised documentation requirements for AI-assisted decisions",
            "Create clear escalation procedures for complex or ambiguous cases",
            "Establish quarterly review meetings to assess oversight effectiveness"
          ],
          "success_metrics": [
            "Oversight audit completed",
            "Documentation compliance >95%",
            "Escalation procedures tested and functional"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Technical/IT specialist"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Preserves human authority and oversight in educational processes, ensuring AI augments rather than replaces human judgment in critical academic decisions. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "BERA (2018) - Ethical Guidelines for Educational Research",
            "Chan & Hu (2023) - Student Perspectives on Generative AI",
            "UNESCO (2023) - Human-Centric AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Academic Affairs",
            "Faculty Senate",
            "Student Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Enhance Expand Existing Accessibility Measures with Cultural Competency",
          "description": "Enhance current accessibility practices by incorporating cultural competency considerations, addressing diverse learning styles and preferences, and ensuring AI implementations respect varied cultural and linguistic backgrounds.",
          "dimension": "inclusiveness",
          "priority": "critical",
          "implementation_type": "enhancement",
          "timeframe": "2-4 months",
          "impact_score": 67.488,
          "current_score": 1.9,
          "expected_improvement": "Significant improvement expected: 1.9% \u2192 56.3%",
          "implementation_steps": [
            "Review existing accessibility measures for cultural inclusivity gaps",
            "Engage diverse student focus groups to identify additional needs",
            "Develop cultural competency guidelines for AI tool selection and implementation",
            "Create ongoing assessment mechanism for inclusive practice effectiveness"
          ],
          "success_metrics": [
            "Cultural inclusivity review completed",
            "Focus group recommendations implemented",
            "Assessment mechanism operational"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Assessment design expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Addresses equity and accessibility requirements to ensure AI implementation does not create or exacerbate educational inequalities or exclude diverse student populations. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "JISC (2023) - Inclusive AI Implementation",
            "Bond et al. (2024) - Equity Considerations in AI Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Disability Services",
            "Diversity & Inclusion Office",
            "Student Support",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Establish Multi-Stakeholder AI Governance Committee",
          "description": "Create institution-wide committee comprising faculty representatives, student body delegates, IT specialists, ethics experts, and senior administrators. Committee should meet monthly to review AI policy implementation, assess emerging risks, and update guidelines based on practical experience.",
          "dimension": "accountability",
          "priority": "critical",
          "implementation_type": "new_implementation",
          "timeframe": "2-4 months",
          "impact_score": 66.624,
          "current_score": 3.7,
          "expected_improvement": "Significant improvement expected: 3.7% \u2192 57.6%",
          "implementation_steps": [
            "Identify and recruit diverse committee members with relevant expertise",
            "Develop committee charter defining roles, responsibilities, and decision-making authority",
            "Establish regular meeting schedule and reporting mechanisms to senior leadership",
            "Create standardised incident reporting and policy violation review processes"
          ],
          "success_metrics": [
            "Committee established within 2 months",
            "Monthly meeting attendance >80%",
            "Quarterly policy reviews completed"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "General project management"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Establishes clear governance structures essential for responsible AI integration in research_university contexts, addressing institutional risk management and stakeholder trust requirements. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "Dabis & Cs\u00e1ki (2024) - AI Ethics in Higher Education Policy",
            "UNESCO (2023) - ChatGPT and AI in Higher Education",
            "Bond et al. (2024) - Meta-systematic Review of AI in Education"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; New implementation required - establish foundation before building advanced features",
          "stakeholder_considerations": [
            "Senior Leadership",
            "IT Services",
            "Legal/Compliance",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        },
        {
          "title": "Enhance Expand Existing Disclosure Requirements with Methodological Detail",
          "description": "Enhance current disclosure practices by requiring detailed documentation of AI methodologies, decision-making processes, and outcome validation procedures. Focus on academic integrity and reproducibility standards.",
          "dimension": "transparency",
          "priority": "high",
          "implementation_type": "enhancement",
          "timeframe": "1-2 months",
          "impact_score": 62.495999999999995,
          "current_score": 12.3,
          "expected_improvement": "Significant improvement expected: 12.3% \u2192 63.6%",
          "implementation_steps": [
            "Review and strengthen existing disclosure language for comprehensiveness",
            "Add requirements for AI methodology documentation in research contexts",
            "Create detailed examples and case studies for common disclosure scenarios",
            "Implement compliance monitoring through existing academic integrity mechanisms"
          ],
          "success_metrics": [
            "Enhanced disclosure guidelines published",
            "Compliance monitoring system operational",
            "Faculty feedback rating >4/5"
          ],
          "estimated_resources": {
            "staff_time": "Substantial (2-3 FTE months)",
            "budget_requirement": "High (\u00a310,000-\u00a325,000)",
            "specialist_expertise": [
              "Legal/Compliance expert"
            ],
            "external_support": "Recommended - Complex implementation benefits from external expertise"
          },
          "source": "PolicyCraft Enhanced Framework",
          "academic_rationale": "Implements disclosure and communication frameworks critical for maintaining academic integrity and enabling informed decision-making by all institutional stakeholders. Particularly important for research integrity and scholarly communication standards.",
          "related_literature": [
            "UNESCO (2023) - AI Transparency Guidelines",
            "JISC (2023) - Generative AI in Teaching and Learning"
          ],
          "institution_specific_notes": "Consider integration with existing research ethics and integrity frameworks; Build on established academic governance structures; Builds on existing policy foundation - focus on strengthening and expanding current provisions",
          "stakeholder_considerations": [
            "Communications Team",
            "Student Services",
            "Faculty Representatives",
            "Research Office",
            "Graduate School",
            "Ethics Board"
          ],
          "potential_challenges": [
            "Complex implementation requiring significant coordination",
            "Faculty autonomy considerations in research contexts",
            "Integration with existing research governance structures",
            "Resource allocation and budget constraints",
            "Timeline coordination with academic calendar",
            "Change management and stakeholder buy-in"
          ],
          "mitigation_strategies": [
            "Establish clear project governance with defined roles and responsibilities",
            "Implement phased rollout approach to manage complexity and risk",
            "Provide comprehensive training and support for all stakeholders",
            "Create feedback mechanisms for continuous improvement during implementation",
            "Engage faculty governance bodies early in planning process",
            "Align implementation with research ethics review cycles"
          ],
          "recommendation_confidence": "High",
          "evidence_strength": "Good - Measurable outcomes defined",
          "implementation_complexity": "High"
        }
      ],
      "created_date": "2025-07-09T10:20:53.083495+00:00",
      "total_recommendations": 4
    }
  ]
}